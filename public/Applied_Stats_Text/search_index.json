[["index.html", "Applied Statistics for the Ascendant Social Science Researcher: A Course Focused on Developing Skills, Building Confidence, and Not Being So Absolutely Confusing and Miserable Chapter 1 Intro", " Applied Statistics for the Ascendant Social Science Researcher: A Course Focused on Developing Skills, Building Confidence, and Not Being So Absolutely Confusing and Miserable Charles Marks 2022-02-18 Chapter 1 Intro (Nothing to see here - work in progress ;) "],["welcome.html", "Chapter 2 Welcome to R 2.1 In This Chapter 2.2 Downloading R and RStudio 2.3 So, WaitWhat is R? 2.4 What can we tell the computer to do with R? 2.5 Saving Values 2.6 Types of Data 2.7 A Quick Check-In 2.8 Storing Larger Quantities of Data 2.9 Nice Job Everyone", " Chapter 2 Welcome to R 2.1 In This Chapter In this chapter, we are going to go over some of the basics of the R programming language. R is a very powerful language for undertaking statistical and computational tasks, but it certainly can be intimidating trying to learn how to use it. And that is totally ok! There may be moments in this class where you are confused, unsure, stressed, incredibly frustrated with your teacher, or a combination of all of these - not to worry! R is a language and learning a new language can be challenging. This text was designed to expose you to the language and to provide you lots of opportunities to practice using it as well! In fact, dont worry too much about memorizing what you see here, you can always open this chapter back up and find the informationwhen you run your analyses for your research, no one is going to ask you if you were able to do it all from memory. 2.2 Downloading R and RStudio Before getting started, you will need to have R and RStudio downloaded on your computer. R is a language we can use to communicate with the computer and RStudio is software that helps us use R. This text is designed to help you learn how to use R via RStudio. You will first need to download R and then download RStudio. You can find detailed instructions for downloading R and RStudio by clicking here. 2.3 So, WaitWhat is R? To put it simply, R is a language that you can use to communicate with your computer. By writing sentences in R, you can tell the computer to undertake specific tasks. So, for example, you might want the computer to add or multiply or divide two numbers together. It turns out math expressions can be written in R quite similar to how we are accustomed to. Here you can see code run in R (in this book, the code is displayed in gray boxes and the computers responses follow): 121 + 27 ## [1] 148 121*27 ## [1] 3267 121/27 ## [1] 4.481481 It turns out computers are really good and really fast at doing math! However, it is important to know that computers cannot do everythingfor example, I cannot tell a computer to Do my homework! or to Clean my room! Unfortunately, those are things you are still going to have to do yourself. So, as we are learning to code in R, really, we are actually learning how to communicate with our computer. Computers interpret you very literally, so if the computer isnt doing what you want it means that something is wrong with what youve told the computer So, the question is 2.4 What can we tell the computer to do with R? 2.4.1 Arithmetic As shown above, arithmetic (addition, subtraction, multiplication, etc) is one of the primary things that R can do. We have some important operators, which you are probably already familiar with: \\(+\\), for addition \\(-\\), for subtraction \\(*\\), for multiplication \\(/\\), for division ^, for exponents \\(()\\), to place an expression within parentheses There are a couple of other arithmetic operators (such as modulo division), but it is something we can add when the time is right. R follows the PEMDAS order of operations, so it is important that we use parentheses to make sure we are telling the computer exactly what we want. PEMDAS tells us the order of operations when executing mathematical expressions, executing terms in the following order: P is for parentheses, we start by executing arithmetic within each set of parentheses E is for exponents M is for multiplication D is for division A is for addition S is for subtraction So, we could understand that \\(2 + 3/5\\) is different from \\((2 + 3)/5\\). In the former, we only have division and addition. We do the division first to get \\(2 + 3/5 = 2 + 0.6\\) and then we do the addition to get \\(2 + 0.6 = 2.6\\). In the latter, we must execute the term inside the parentheses first, such that \\((2 + 3)/5 = 5/5\\). Now the only operator remaining is division and we calculate that \\(5/5 = 1\\). We can see this in action in R: 2 + 3/5 ## [1] 2.6 is not the same as (2 + 3)/5 ## [1] 1 Perhaps we want to know what 25 raised to the power of 2 + 3 is. We might (wrongly) try: 25^2+3 ## [1] 628 However, we know it should be equal to 25 raised to the power of 5 (because 2 + 3 = 5), which equals: 25^5 ## [1] 9765625 So, it looks like parentheses are quite important. The computer cannot guess what it thinks we meant, it just reads what we told it to do. To get it right, we can use parentheses and tell the computer: 25^(2+3) ## [1] 9765625 2.5 Saving Values When we run an operation, it can be very useful to save that information so that we can use it again later. With R, you can ask the computer to save information by assigning it a name. You do this by typing the name of the variable, write an arrow (&lt;-) and then the value you want to save. For example, maybe I want to record my grade on each test in my class. We had three tests and I got a 92 on the first one, a 61 on the second, and 96 on the third. I could save them like so: test1 &lt;- 92 test2 &lt;- 61 test3 &lt;- 96 What is very useful now is that I can type the variable names now instead of the numbers. For example, if I want to calculate my average score I could add together all three test scores and then divide by three like so: average_score &lt;- (test1 + test2 + test3)/3 I saved the average score in the variable that I decided to name average_score. Now if I want to view the average score, I can simply type and the computer will tell me the value stored in that variable: average_score ## [1] 83 2.6 Types of Data Here, we see that computers are really good at working with numbers, and R provides a nice language for asking a computer to work with numbers. However, there are several important basic data types to know in R: numeric (i.e., numbers) character (i.e., text) logical (i.e., TRUE/FALSE) and, factor (more on this later) 2.6.1 Numeric Data Numeric data is any data that is stored such that computer recognizes it as a number. As we have displayed above, numeric data is created by typing out numbers. Numeric data can be integers or have decimal points, they can be negative or positive. Here I will create some numeric data to display: n1 &lt;- 2 n2 &lt;- 12.75 n3 &lt;- -22.1 n4 &lt;- 1 + 1 n5 &lt;- 92/(3^4) 2.6.2 Character Data Character data is essentially just text. Perhaps we want to store the name of participants in a study, this can be useful if we have to follow-up with them  we dont want to forget their names, that would be rude! We indicate that we are providing the computer character data by surrounding it in \"\" quotation marks, like so: participant1 &lt;- &quot;Gabriella&quot; participant2 &lt;- &quot;David&quot; participant3 &lt;- &quot;Remi&quot; participant4 &lt;- &quot;Surya&quot; participant5 &lt;- &quot;Mr. Kitty&quot; Interestingly, we can save numbers as characters, for example, I could do something like: var1 &lt;- &quot;33&quot; var2 &lt;- &quot;10&quot; If I do this, the computer will recognize var1 and var2 as characters, not as numbers. If I tried to add them together, the computer would tell me that you cant add together character data. 2.6.3 Logical Data Next, we have logical data. We use logical data when something can either be True or False. As such, logical values can either be equal to TRUE (or T) and FALSE (or F). R recognizes logical data like so: log1 &lt;- TRUE log2 &lt;- FALSE log3 &lt;- T log4 &lt;- F The computer recognizes TRUE and T to be the same thing and FALSE and F to be the same thing. You can use either one. Logical data is different from character data because its not just the word TRUE or the word FALSEour computer actually understands the concepts of TRUE and FALSE in a very similar way that we do. 2.6.3.1 Logical Operators It is important that we learn a few important logical operators. You are probably familiar with some of these from math class: \\(&gt;\\), greater than \\(&gt;=\\), greater than or equal to \\(&lt;\\), less than \\(&lt;=\\), less than or equal to \\(==\\), equal to \\(!=\\), not equal to \\(|\\), OR \\(\\&amp;\\), AND When we use these operators, we are asking the computer a question. For example, if we type 5 &gt; 3, we are asking the computer, Is 5 greater than 3? and the computer can either tell us TRUE or FALSE. We can see what the computer says: 5 &gt; 3 ## [1] TRUE If we invert it, and ask the computer if 3 is greater than 5 we see that the computer is able to tell us that this is FALSE: 3 &gt; 5 ## [1] FALSE Some of these operators will make more sense once we are working with a bit more data. Here you can see a few examples of these operators and the computers response. You can look at these to try to get a better sense of how these operators work: 5 &lt; 10 ## [1] TRUE 10 &lt; 10 ## [1] FALSE 10 &lt;= 10 ## [1] TRUE 10 == 10 ## [1] TRUE 10 != 10 ## [1] FALSE 11 != 10 ## [1] TRUE The AND and OR operators are specifically intersting cases, but they are quite important, especially later when we are cleaning our datasets. AND is supplied two logicals, if they are both TRUE, then the computer will respond TRUE. OR is similar, it is supplied two logicals and if one of them is TRUE then the computer will respond TRUE. For example: TRUE &amp; TRUE ## [1] TRUE TRUE &amp; FALSE ## [1] FALSE FALSE | FALSE ## [1] FALSE FALSE | TRUE ## [1] TRUE These might not seem useful this way. Perhaps I want to know if I passed both tests for a class. On Test 1 I got a 75 and on Test 2 I got a 61. My teacher told me I need at least a 60 on each test to pass. The following code checks if I passed: test1 &lt;- 75 test2 &lt;- 61 (test1 &gt;= 60) &amp; (test2 &gt;= 60) ## [1] TRUE Here we can see that we strung together several logical operators. Within the parentheses I ask the computer to check if the score of test 1 was greater than or equal to 60 and if the score of test 2 was greater than or equal to 60. With the \\(\\&amp;\\) I then asked the computer to check if both were true (i.e., if it was true that I passed both tests). Turns out I did, which is quite a relief!! 2.7 A Quick Check-In This might have been a lot of information already! If your brain is feeling a bit frazzled, dont worry, that is pretty understandable. So far, weve gone over some of the things we can tell the computer to do using Rbut we are learning how to use R so we can do statistics and usually we dont just save a couple of numbers and add them together. Usually our advisor or PI or whoever gives us a dataset and then asks us to run some t-tests or regressions or something far scarier. So the next thing we need to learn is how to store larger quantities of data in R  hint, this is what makes R super cool!!! 2.8 Storing Larger Quantities of Data Most of us are really used to working with Excel spreadsheets. A lot of times, when we get a dataset from a study, it is in a spreadsheet format. Spreadsheets have rows and columnsusually, each row represents a participant in our study and each column represents a piece of information about that participant. In R, we have two types of objects that mimic a spreadsheet. 2.8.1 Vectors The first is called a vector. It is equivalent to either a single row or a single column in a spreadsheet. For example, we can make a vector that represents a row. Maybe I did a study where I asked participants for their name, their age, if they drink alcohol (TRUE/FALSE), and how many drinks they have each week. I could create a vector which represents this person in R. I use the c() syntax to create a vector like so: c(&quot;Charlie&quot;,29,TRUE,4) ## [1] &quot;Charlie&quot; &quot;29&quot; &quot;TRUE&quot; &quot;4&quot; Just like with our other data types, we can save a vector like so: vector_1 &lt;- c(&quot;Charlie&quot;,29,TRUE,4) As you can see, this vector has four pieces of data in it. Youll notice that it contains different types of data: the first is a character, the second and fourth are numeric, and the third is logical. Now, you can access different elements in the vector by using bracket notation. For example, if I want to pull out the first element I can write: vector_1[1] ## [1] &quot;Charlie&quot; Similarly I could do the same with the other elements by writing: vector_1[2] ## [1] &quot;29&quot; vector_1[3] ## [1] &quot;TRUE&quot; vector_1[4] ## [1] &quot;4&quot; Now this next example might seem a bit silly, but I could even put a numeric value into a variable and use that to pull out an element, for example: num &lt;- 3 vector_1[num] ## [1] &quot;TRUE&quot; As you can see, because the variable num was assigned the value three, when I ran vector_1[num] it pulled out the third element of the vector. We can also use vectors as if it is a column of a spreadsheet. For example, lets say I am a teacher and my 5 students had three exams. I stored their grades in a vector for each test, like so: test_1 &lt;- c(99, 82, 33, 100, 68) test_2 &lt;- c(44, 54, 65, 78, 99) test_3 &lt;- c(82, 89, 91, 90, 87) If we think of these as columns, we can see that student 1 got a 99 on the first test, a 44 on the second test, and an 82 on the third test. Then student 2 got an 82 on the first, a 54 on the second, and an 89 on the third. So, earlier, we calculated the average grade for three tests for one student. But now we have five students. We could do each student by hand, but imagine a teacher with 100 students! It would take a lot of time to calculate the average score of each test by hand. Well, this is something that R is really good at. You can actually do math on vectors. For example, I can use R to tell the computer to calculate the average for each student by doing the following: average_scores &lt;- (test_1 + test_2 + test_3)/3 average_scores ## [1] 75.00000 75.00000 63.00000 89.33333 84.66667 So, why did that work??? When you tell the computer to add together two vectors, it adds together the first element of each vector to make a new first element. Then it adds together the second element of each vector to make a new second element. Lets look at a couple examples to understand this better: c(1,2,3) + c(1,2,3) ## [1] 2 4 6 c(1,2,3) * c(1,2,3) ## [1] 1 4 9 c(1,2,3) - c(1,2,3) ## [1] 0 0 0 This is incredibly useful when we are building a dataset. Perhaps we have an index where there are 10 questions and the final score is the sum of all the answers. You can just add together the vectors representing each question and get the final score!! Dont worry, there will be examples of that later on! Also notice how in that example we divided the vector by 3 to get the average score. You can also do vector arithmetic with a single number. That number is just applied to every element in the vector. For example: c(1,2,3) + 1 ## [1] 2 3 4 c(1,2,3) * 2 ## [1] 2 4 6 c(1,2,3) / 3 ## [1] 0.3333333 0.6666667 1.0000000 At this point, it is totally understandable if your brain is just like ENOUGH! Like I said earlier, dont worry about memorizing this. There are practice exercises you can do for homework where you can play with this and get more familiar. Coding is kind of like riding a bicycle - a lot of times it only makes sense after you do it for yourself and practice until it feels natural! 2.8.2 Data Frames The second object is called the data frame. This is the most important object that you will use as a statistician. It is pretty much just an Excel spreadsheet. Just like a spreadsheet it has columns and rows. Each column has a name that tells what is in that column. For example, you can have a data frame where the first column contains participants age (you might call this column Age), the second column their gender, the third and so on and so on. Just like with a spreadsheet, each row represents an obseervation of our data. Often times, we will just load a data frame object from a file, like a .csv or an Excel worksheet (or even from a SAS, SPSS, or Stata file). There are ways to create your own data frame too. Here we are going to create our own data frame, but, for the most part you will just be loading a file with your data already in it (you will get to do this a lot in future chapters). 2.8.2.1 Create a Data Frame Here I am going to create a data frame representing a class of 5 students who each took 3 tests. Each test was scored out of 100. So first, I am going to create a vector for each column in our data frame. students &lt;- c(&quot;Amy&quot;,&quot;Bart&quot;,&quot;Cathy&quot;,&quot;Delilah&quot;,&quot;Edwina&quot;) test1 &lt;- c(82,77,100,78,58) test2 &lt;- c(99,100,98,64,61) test3 &lt;- c(80,91,99,81,59) Next, we are going to create our data frame by using the data.frame() syntax. This is similar to the c() syntax for creating a vector. Technically, these are called functions, which we will cover in more detail in the following chapter. We create the data frame like so: data.frame(students, test1, test2, test3) ## students test1 test2 test3 ## 1 Amy 82 99 80 ## 2 Bart 77 100 91 ## 3 Cathy 100 98 99 ## 4 Delilah 78 64 81 ## 5 Edwina 58 61 59 As you can see, this looks really similar to a spreadsheet in Excel! We can see that each row represents an observation (in this case, a student), and each column represents a piece of information about each participant (their name, test scores). Lets play around with this data frame to see how data frames work. First, we are going to save the data frame like so: df &lt;- data.frame(students, test1, test2, test3) 2.8.2.2 Working With a Data Frame So, now that we have saved the data frame, we first need to learn how to access information in the data frame. One way to do this is very similar to how we can access information in a vector. In a vector, we can access the 5th element of the vector by typing vector_name[5]. We use the brackets and the 5 to tell the computer to get the 5th element out of the vector. However, data frames have two-dimensions, rows and columns. So we can use the bracket syntax to access elements of a data frame, but we have to supply two pieces of information: the row number and the column number. If we want to access the element in the 1st row and 2nd column (82) then we would type the following: df[1,2] ## [1] 82 Similarly, we can access an entire row (lets say we want the third row), by only supplying the row number. To do this, we would type the following: df[3,] ## students test1 test2 test3 ## 3 Cathy 100 98 99 Now notice that we still included the comma. If you leave out the comma and just type df[3], the computer will throw an error because it wont understand what you are asking for. Similarly, you can access a column (lets say the 4th column) by typing: df[,4] ## [1] 80 91 99 81 59 You can also access mutliple columns or multiple rows by providing a vector with the row/column numbers you want. Lets say you want the three columns representing the test scores (2 - 4). Well you can tell the computer the following: df[,c(2,3,4)] ## test1 test2 test3 ## 1 82 99 80 ## 2 77 100 91 ## 3 100 98 99 ## 4 78 64 81 ## 5 58 61 59 As you can see, typing it in this way, the computer returns the 2nd, 3rd, and 4th columns back to you. This may come in handy later on when we are cleaning up our data sets  I have worked with some datasets that have thousands of columns and sometimes we only need like 15. It can be nice to subset our data frame to only have those columns. If say we wanted to de-identify our class roster and just have test scores, we could do the following: deidentified_df &lt;- df[,c(2,3,4)] By doing this, we now have a data frame without student names. Sometimes our datasets have sensitive information and it can be good to know how to remove it. There is another useful way to do this as well. You can also tell the computer to remove rows or columns. In the previous example, we told the computer to give us our data frame df, but only with the 2nd, 3rd, and 4th columns. We could also do this via subtraction by telling the computer to give us df with the 1st column removed, like so: df[,-c(1)] ## test1 test2 test3 ## 1 82 99 80 ## 2 77 100 91 ## 3 100 98 99 ## 4 78 64 81 ## 5 58 61 59 In this case, df[,-c(1)] and df[,c(2,3,4)] actually do the same exact thing. Something really important to know is that, often times, there are many many many different ways to tell the computer to do the same thing. What is important is that you do one of the right ways  if someone else does something different than you, that doesnt mean one of you is right and one of you is wrong. The important thing is that the computer does what you want it to do. 2.8.2.3 Using the $ Operator Ok, so this whole [row,column] syntax seems a bit much, right? Like why do the columns all have names if we are just going to be typing in the numbers inside the brackets??? Well, good news! R makes it really easy to communicate with the computer about which columns you want to work with. For example, as we can see, in our data frame df we have four columns whose names are students, test1, test2, and test3. We can access these columns by using the $ operator. We simply type df$ followed by the name of our variable. What is really cool, since we are using RStudio, is that RStudio will usually show you a dropdown with all your variable names! As you type more letters, the dropdown will narrow. So, for example, we can access each column in our data frame as follows: df$students ## [1] Amy Bart Cathy Delilah Edwina ## Levels: Amy Bart Cathy Delilah Edwina df$test1 ## [1] 82 77 100 78 58 df$test2 ## [1] 99 100 98 64 61 df$test3 ## [1] 80 91 99 81 59 What is really cool is that each column can be treated just like a vector! So, since I am a teacher, I need to calculate the average score for each student in the class. We have done this a few times already! So, just like in our vectors example, we can get the average for each student as follows: (df$test1 + df$test2 + df$test3)/3 ## [1] 87.00000 89.33333 99.00000 74.33333 59.33333 But here is where working with R to talk to your computer is pretty cool. Instead of just calculating the average scores, I can actually add them to my data frame by creating a new column! All I have to do is: df$final_grade &lt;- (df$test1 + df$test2 + df$test3)/3 df ## students test1 test2 test3 final_grade ## 1 Amy 82 99 80 87.00000 ## 2 Bart 77 100 91 89.33333 ## 3 Cathy 100 98 99 99.00000 ## 4 Delilah 78 64 81 74.33333 ## 5 Edwina 58 61 59 59.33333 Here, we have created a new column called final_grade and we have assigned the average score of the three tests as its value! Thats pretty darn useful! 2.8.2.4 Creating Categorical Data (AKA Factors) However, as a teacher, I need to assign every student with a letter grade (A,B,C,D,F). I could do this by hand, but again, we want a really easy way to do this  imagine if our dataset had 1,000 students in it!!! It would be awful to do that by hand. In R, we refer to categorical data as factors. And we use the factor() syntax in order do this. These next few snippets of code might be confusing, dont worry at all! The best way to get familiar is to see it and then apply it! In fact, lets go over a little trick that can help with reading and writing code in R. We can actually write comments into our code. Comments are just plain text that the computer does not try to read  its just there to help you understand what is happening in the code. That is because sometimes code isnt easy to translate back into English. In order to write a comment, we simply write a # and then whatever comes after it on the same line is a comment. We can do that like so: ## This is a comment. # So is this ########################### # Here is a fancy comment # ########################### # Don&#39;t worry about the different colors ## R sometimes will try to color code things # Sometimes it helps a lot ## Sometimes it does not /\\(o0)/\\ Previously, we made a new column called final_grade. It is good practice to use comments to tell whoever is reading the code why you wrote that line of code. For example, we can update the code to read: ## Create a new variable called final_grade ## It is the average score of each of the three tests df$final_grade &lt;- (df$test1 + df$test2 + df$test3)/3 Ok ok! Now that we learned how to do that, lets get back to our goal of making a new column with the letter grade for each student. There are multiple steps - dont worry about memorizing them, you can always just come back to this code if you cannot remember. The first step is that we are going to create a new column. We just want to assign it a name (final_letter_grade) but we do not want to put any data in it yet. In R, when an object does not have data assigned to it, the value is called NA. NA pretty much means there is nothing here. So, we can create the new column like so: ## Create new variable called final_letter_grade df$final_letter_grade &lt;- NA ## Print out the df object so we can view our new variable df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 NA ## 2 Bart 77 100 91 89.33333 NA ## 3 Cathy 100 98 99 99.00000 NA ## 4 Delilah 78 64 81 74.33333 NA ## 5 Edwina 58 61 59 59.33333 NA So, as we can see, we have created a new column and it is empty. R uses NA to display that the entries are empty. Next, we are going to introduce some new syntax. We will use the comments to help understand what it is saying. It is totally okay if it is confusing at first. Essentially, we want to tell the computer that if the final grade is greater than or equal to 90, that the final grade should be an A. However, we also want to maintain that the grades are ordered (i.e., an A is higher than a B is higher than a C is). This is why factors are useful. We are first going to start by assigning final_letter_grade a number. In fact, the way I am going to do it, to maintain order, is to assign F to the number 0, D = 1, C = 2, B = 3, A = 4. This will help us tell the computer that not only do the grades have character names (A,B,), but that these letters have a numerical relationship with each other. So, let us just start with the people who got 90 or higher. We want to assign any student with a final_grade greater than or equal to 90 and final_letter_grade numerical value of 4 (because thats the value we are going to match with A later on). We can do this like so: ## Here we assign all students with a final grade &gt;= 90 ## With a final_letter_grade of 4 (which corresponds with an A) df$final_letter_grade[df$final_grade &gt;= 90] &lt;- 4 ## Print the df to see the result df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 NA ## 2 Bart 77 100 91 89.33333 NA ## 3 Cathy 100 98 99 99.00000 4 ## 4 Delilah 78 64 81 74.33333 NA ## 5 Edwina 58 61 59 59.33333 NA Ok ok okwhat is that all about? We can see from the table that that line of code only affected Cathy. We see that she was the only student whose final_grade was greater than or equal to 90. As such, we correctly assigned her a final_letter_grade of 4! So, how exactly did that weird line of code make that happen? The above line of code has three parts. The first is df$final_letter_grade. Here we are telling the computer that we want to assign a value to the final_letter_grade variable. The second part is [df$final_grade &gt;= 90]. Here we are telling the computer that we only want to change final_letter_grade for the students whose final_grade was 90% or higher. The final part is &lt;- 4 which is just telling the computer that for the students who had a final grade greater than or equal to 90, assign their final_letter_grade value to be 4. As we can see, since Cathy was the only student with a final_grade in the 90s, she was the only student assigned the value 4. We can then repeat this for each grade value. ## Here we assign all students with a final grade in the 80s ## With a final_letter_grade of 3 (which corresponds with a B) ## We use the &amp; operator to provide two restrictions ## The lower bound that the grade must be greater than or equal to 80 ## Adn the upper bound that the grade must be less than 90 df$final_letter_grade[df$final_grade &gt;= 80 &amp; df$final_grade &lt; 90] &lt;- 3 ## Here we assign all students with a final grade in the 70s ## With a final_letter_grade of 2 (which corresponds with a C) df$final_letter_grade[df$final_grade &gt;= 70 &amp; df$final_grade &lt; 80] &lt;- 2 ## Here we assign all students with a final grade in the 60s ## With a final_letter_grade of 1 (which corresponds with a D) df$final_letter_grade[df$final_grade &gt;= 60 &amp; df$final_grade &lt; 70] &lt;- 1 ## Here we assign all students with a final grade &lt; 60 ## With a final_letter_grade of 0 (which corresponds with an F) df$final_letter_grade[df$final_grade &lt; 60] &lt;- 0 ## Then lets print out the data frame to see if it worked df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 3 ## 2 Bart 77 100 91 89.33333 3 ## 3 Cathy 100 98 99 99.00000 4 ## 4 Delilah 78 64 81 74.33333 2 ## 5 Edwina 58 61 59 59.33333 0 As you can see we were able to correctly assign the right numerical value to each student! We used our \\(\\&amp;\\) logical operator to tell the computer we wanted to capture grades within two values. For example, we could tell the computer to find final_grades corresponding to a B by looking for final_grades greater than or equal to 80 AND less than 90. If we just said greater than 80 without including less than 90, we would have accidentally changed Cathys grade. Now there is one final step! We want to assign the numerical values (0,1,2,3,4) to the correct letter grades such that (F = 0, D = 1, C = 2, B = 3, D = 4). To do this we are going to use the factor() function. As we will discuss in more detail in the following chapter, a function takes arguments. An argument is just a piece of data that the function uses to complete its given task. Arguments are separated by commons and sometimes they have names that we have to type. In this case, the factor function receives 3 arguments: 1) a vector of data (in this case, the column of our data frame); 2) the numeric levels of the data (i.e., 0,1,2,3,4); and 3) the labels for each level (i.e., F, D, C, B, A). With this information, the factor function will tell the computer to convert our final_letter_grade column into an ordered, categorical variable. We do this like so: ## We will convert the final_letter_grade variable into a categorical variable ## By applying the factor function to it df$final_letter_grade &lt;- factor(df$final_letter_grade, levels = c(0,1,2,3,4), ## Here we include a vector with the numerical values labels = c(&quot;F&quot;,&quot;D&quot;,&quot;C&quot;,&quot;B&quot;,&quot;A&quot;)) ## Here we include a vector with the names of each level ## We can then print the data frame to confirm the code worked df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 B ## 2 Bart 77 100 91 89.33333 B ## 3 Cathy 100 98 99 99.00000 A ## 4 Delilah 78 64 81 74.33333 C ## 5 Edwina 58 61 59 59.33333 F Tada! We have just made an ordered categorical variable! That is super cool! Dont worry if you didnt quite follow or if your head hurts - we are going to get lots of practice throughout this class working with categorical data! 2.8.2.5 Creating a Logical Variable So, we are just about done with this introduction. I want to show you a couple additional things we can now do with our factor. We might want to create a simple logical (TRUE/FALSE) variable indicating whether students passed or failed the class. We are going to call this variable passed, where a TRUE value means they passed and FALSE value means they did not. We can do this like so: ## First we create the variable df$passed &lt;- NA ## Next we assign anyone with an &quot;F&quot; grade a passed value of FALSE ## Be careful, you need the quotation marks around the F, otherwise the computer will think you mean FALSE df$passed[df$final_letter_grade == &quot;F&quot;] &lt;- FALSE ## Finally we assign anyone who did not get an &quot;F&quot; grade a passed value of TRUE df$passed[df$final_letter_grade != &quot;F&quot;] &lt;- TRUE ## We can look at our data frame to confirm that we got it correct df ## students test1 test2 test3 final_grade final_letter_grade passed ## 1 Amy 82 99 80 87.00000 B TRUE ## 2 Bart 77 100 91 89.33333 B TRUE ## 3 Cathy 100 98 99 99.00000 A TRUE ## 4 Delilah 78 64 81 74.33333 C TRUE ## 5 Edwina 58 61 59 59.33333 F FALSE As you can see, we now have a single variable which indicates if students passed or did not pass the class. We could certainly just use the final_letter_grade variable to figure out who passed and who didnt, but sometimes it is useful to take a categorical variable with many values and condense it down to a dichotomous variable. This may come up later when we learn to implement something like logistic regression! 2.8.2.6 Using the summary() function Before we wrap up, I just want to show you the summary() function. In R, you can pretty much put anything in the summary() function and the computer will tell you some information about that object. Sometimes this information is useful, sometimes it is not. For example, we can run summary() on our data frame like so: summary(df) ## students test1 test2 test3 final_grade ## Amy :1 Min. : 58 Min. : 61.0 Min. :59 Min. :59.33 ## Bart :1 1st Qu.: 77 1st Qu.: 64.0 1st Qu.:80 1st Qu.:74.33 ## Cathy :1 Median : 78 Median : 98.0 Median :81 Median :87.00 ## Delilah:1 Mean : 79 Mean : 84.4 Mean :82 Mean :81.80 ## Edwina :1 3rd Qu.: 82 3rd Qu.: 99.0 3rd Qu.:91 3rd Qu.:89.33 ## Max. :100 Max. :100.0 Max. :99 Max. :99.00 ## final_letter_grade passed ## F:1 Mode :logical ## D:0 FALSE:1 ## C:1 TRUE :4 ## B:2 ## A:1 ## As we can see, this gives us some useful information about each column in the data frame. We can see that the average (mean) score of the first test was 79, we can see that 2 students got a B in the class, and we can see that 4 students passed. Sometimes when data frames have a lot of columns, it is really messy to just run the summary function on the whole data frame. lets say you just want to see the final letter grades for the class. You can use the summary function just on this column by telling the computer: summary(df$final_letter_grade) ## F D C B A ## 1 0 1 2 1 As you can see, this tells us that 1 student got an F, 1 got a C, 2 got Bs, and 1 got an A. This can be particularly useful when you have a lot of observations and you want to summarize your data. For example, you might do a study and want to share the descriptive statistics about your sample. Maybe you want to know how many people in your study identified as Male, how many identified as Female, and how many identified as Gender Non-Conforming. The summary function can provide an easy way to access this information! 2.9 Nice Job Everyone Ok!!! That was quite literally so much information. Great job! :) If you are feeling overwhelmed or confused, do not worry at all. Part of learning to code is practicing. And that is why this book is also intended to give you lots of opportunities to practice! "],["functions.html", "Chapter 3 Functions and Libraries 3.1 Okay, waitWhats a function? 3.2 Breather Time 3.3 Looking Under the Hood: Writing Our Own Function 3.4 So Whats A Library? 3.5 Quick word of caution 3.6 In Conclusion", " Chapter 3 Functions and Libraries In the first chapter, we got our first introduction to R. We learned how we can use R to communicate with our computer and ask it to do things that we would rather not do ourselves. Some these instructions are pretty intuitive to us - for example, telling the computer to add a few numbers together is pretty straightforward. However, sometimes we want the computer to do some pretty complicated tasks that require many steps. Many statistical techniques are quite involved and require many computational steps for the computer to undertake - and, yes, someone has to write out these computational steps (HINT: dont worry, that someone is usually not you!). Even just making a summary table of all your variables of interest can involve many steps. If a variable is continuous you need to take the mean and standard deviation, if it is categorical you need to calculate frequencies and proportions. And maybe, while we are at it, we want to stratify our descriptive statistics based on exposure group (control versus exposed), so we also would like to run the appropriate statistical tests (t-tests, ANOVA, chi-squared) and report the p-values. The task I have just described is quite important in public health research. When we write a paper, it is very important that the reader knows who our study sample is as well as key differences between groups of interest within our sample. This information is so important, in fact, that it is typically the first table (TABLE ONE!) of human subjects research papers. Even though descriptive statistics and t-tests arent the most complicated thing to do (if you so desired, you could run them by hand), the computer requires many many instructions to complete this task. For each variable you want descriptives for, the computer has to identify the type of variable (continuous, categorical, etc), then has to calculate means &amp; standard deviations or frequencies &amp; proportions, and, then, if applicable, run appropriate statistical tests. Then after the computer does all of this, it needs to format the information in a way that we can easily read and work with it. If youre thinking to yourself, that sounds like it takes a lot of code to do!!! I would tell you that you are quite correct! The good news is that R has a feature that makes communicating complicated tasks to the computer quite simple and that many people have dedicated a lot of time to writing the code to do these complicated tasks for you already! The important thing though is to learn how this works and how to use them effectively for yourself! In this chapter we will talk about functions, packages, and your library. A function is simply a collection of lines of code that are stored so that, instead of needing to rewrite all of the code every time you want to use it, you can just call the function with a single line of code and the computer will actually run all of the lines of code! A library is a collection of functionsin a way its a cookbook that you can load into R and then use the functions to complete given tasks. Dont worry if this is confusing, it will make sense soon! 3.1 Okay, waitWhats a function? Youve already seen a few functions from the first chapter. Whether it was running \\(c(1,2,3)\\) or \\(summary(vector)\\) or \\(data.frame(vector1,vector2,vector3)\\), each of these were functions. You can usually recognize functions in R because they have two main parts: the first is a name (such as c or summary) and the second part is a set of parentheses containing arguments. The name is actually associated with some code that isnt visible to you (unless you do some digging). When you run \\(summary(vector)\\), the computer doesnt just read summary and think to itself, well, I think my user wants the mean and median of the numbers in this vector. In fact, there are many lines of code associated with the summary function that the computer runs in the background. 3.1.1 A Real World Example of a Function: Buying a Cake To display what a function is, lets look at a real world example. It is my partners birthday in a few days and I am not a very good baker. I go to this bakery that I hear has the very best cakes. In this analogy, I am the coder and the bakery is the computer. So I walk into the bakery and I say to the baker: Please make me a cake, esteemed and talented baker! This would sort of like if there was a \\(bake\\_cake()\\) function in R and I told the computer-baker: bake_cake() And the baker responds: Can do! It will be ready in a few hours. You see, all I had to do was tell the baker to make the cake. It turns out the baker already has the recipe for making cake. That recipe is sort of like R code - it would be pretty frustrating to have to explain to the baker how to make the cake just like it would be frustrating to have to write every line of code for a complicated task. So, I go to the cafe next door and work on some other projects while the cake is being compiledI mean while the cake is being baked! When I return to the bakery, the baker hands me a beautiful, albeit incredibly plain cake - just a rectangular cake with white frosting. Now I look at the cake and my first instinct is to be frustrated because, well, this is a birthday cake and a plain white cake isnt going to cut it. But I realize this is my fault, I didnt tell the baker what kind of cake I wanted (remember, if the computer doesnt do what you wanted, then it means you didnt tell it the right thing!) So, I say to the baker: Wow that is a beautiful cake! Turns out I actually need another cake. This time could you make it a circle, with light pink frosting, dark pink roses around the rim, and in dark pink could you please write Happy Birthday, Angel! Now, in R I could communicate this to the computer-baker in R by saying: bake_cake(shape = &quot;Circle&quot;, frosting_color = &quot;Light Pink&quot;, roses = TRUE, rose_color = &quot;Dark Pink&quot;, message = &quot;Happy Birthday, Angel!&quot;, message_color = &quot;Dark Pink&quot;) In this example, we used arguments to provide more information to the computer to complete the function for us. Arguments are built into a function - so when someone makes a function, they decide what arguments a function can take in. Whoever made the \\(bake\\_cake\\) function made sure to include arguments (i.e., shape, roses, rose_color, etc) that would allow us to specify what kind of cake we want. So I go back to the cafe after making my order and when I return, lo and behold, before me is the cake that I wanted! First, I apologize if this example made it seem like you could use R to tell your computer to make a cake  that was misleading of me and I apologize. Your computer cannot make a cake. More importantly, though, this interaction displays the power of functions in R. Applying statistical concepts often involves running very complicated sequences of tasks - while it is important that we understand how they work, it is far too complicated (and also not a good use of time) to try to write code out for every single step of these procedures. As such, we can use specific functions to undertake complicated tasks with easy to write code. Each function has a unique set of arguments that we can also apply in order to provide information to the computer to complete the task as we desire it! 3.1.2 Picking Functions in R: The read.csv() function 3.1.2.1 Finding Functions When we want to complete a task using R, there are a couple of important questions to ask. The first is: Is there a function already built that can do this task? If you have completed this task before, then you just need to find the function from last time you used it. If not, then its time to use Dr. Google (alternatives include flipping through the index of an R coding book or asking your statistics teacher!). For example, maybe your PI gave you a .csv file containing the dataset you want to work with and you want to know if you could load the dataset using R so you can work with it! You type into Google: How do I load a csv file in R? Using this search you find many, many articles with tutorials on how to do so. After clicking a few, you discover that the \\(read.csv()\\) function appears to be the perfect function for undertaking this task. 3.1.2.2 Figuring Out How the Function Works So, a really cool thing about R is that every function has a documentation file. This is a file which tells you all about the function. It tells you what the function is for, it tells you what arguments the functions can take, it tells you which arguments are required and which are optional, and it tells you what information the computer will return to you at the end. You usually can find the documentation file by typing something like read.csv() documentation into Google. However! We are using RStudio so you can find this information without going online. In the bottom right corner of RStudio (unless you changed your settings), there is a pane that has 5 tabs: Files, Plots, Packages, Help, and Viewer. You can view documentation files in the Help tab. So, click on the Help button  there is a little icon of a house, click on that too!. The view in this pane should look like this: In the top right, there is a text entry with a magnifying glass. You can type in the name of the function you want to learn about here. In this case, read.csv. RStudio may try to create a drop down list based on what you type. Either click on the option for read.csv or finish typing read.csv and hit enter. The Help pane will take you the documentation file, which looks like: Hmm, this page has multiple functions on it, but we can see that the one we wanted is the second in the list. Lets just take a look at the function we want and try to decipher what it is saying: This provides us an overview of the arguments that the read.csv function can accept. First, we can see that there is only one REQUIRED argument, the one called file. How do we know that it is required? Because it does not have an \\(=\\) after it. The function is basically saying, I need you to tell me what file you want me to read, because I cant even guess what you want! All of the other arguments are optional. You can tell because the function assigns a default value for each one. For example, if you ran the function without specifying a value for the header argument, then the computer will assume that header = TRUE because that is how the function was written. We can see that all of the arguments besides file are optional. But, it isnt totally clear what these arguments actually mean! So, how do we find out? We just scroll down a little bit and see that we have descriptions of every single argument! How nice! For this function, there are three really important ones to know, pictured here: We can see the following important information: the file argument refers to the name of the file which the data are to be read from the header argument indicates to the computer whether or not the first row of the CSV has column names or not the computer will assume that the first row has column names if you do not specify because the default is header = TRUE the sep argument indicates to the computer what character separates data within the file a CSV file stands for Comma-Separated Values, so it makes sense that the sep argument defaults to a comma You might be seeing that really long description of the file argument and wondering what on earth all of that is about. It is trying to tell you that you need to make sure the file you want to read is somewhere that the computer can find. So, earlier I had you download a CSV file and save it in a place that makes sense. Your computer has a default location it assumes when you start working in R. You can find it by typing the following code: getwd() ## [1] &quot;C:/Users/Charl/OneDrive/Documents/Intro to Applied Stats with R&quot; Basically, your computer is full of folders and different locations where files can be saved. You can think of each folder as a room  and your computer can only be in one room at a time (this sort of sounds like computers are consciousmaybe they are?). When we type in \\(getwd()\\), we are basically just finding out what room (i.e., folder) our computer is currently in. We call this room the Working Directory - its not super important to remember the name, but thats what the wd in getwd stands for. So now, we want to set the working directory of the computer to correspond to the location where you saved the csv file I had you download. In order to find the correct folder, we are going to look in the pane in the bottom right again. This time, instead of clicking the Help tab, we are going to click the Files tab, which will give you a display that looks like this: Now we are going to navigate by clicking on the appropriate folders to the folder (aka the room) where the CSV is. Just because our display is showing the folder we want does not mean that the computer knows this is the room it is supposed to be in. To tell the computer that this is the right place, click on the More button with the drop down arrow and select the option Set As Working Directory, like so: Now that we are in the correct directory, we can actually load the dataset by using the \\(read.csv()\\), like so: df &lt;- read.csv(&quot;dataset.csv&quot;) df We can take a look and we can see that we now have a data frame object called df and it contains the data from the CSV file! We did not modify any of the optional arguments because it was not necessary to do so! 3.2 Breather Time Ok. That was a lot of info. If youre just reading through, this is definitely a good spot to take a little breather, drink some water, stretch it out. We covered a bunch of stuff: what a function is, how to use functions, how to navigate the help pane in RStudio, and how to navigate folders and set the correct working directory! Especially if this is new for you, this might be pretty overwhelming. The good thing is that, when we use R for statistics work, loading data files is almost always the very first step so you will get lots of practice with this. Now, not every data file is going to be in CSV format. You might have an .xlsx file or a SAS file or an SPSS file or something else altogether. A quick Google search will usually tell you how to load these files, but, for simplicity, in the copy pasta file at the end of this chapter, I have included functions for loading common types of data files with R! 3.3 Looking Under the Hood: Writing Our Own Function Now that we have a grasp on what a function is and how it works, lets actually write our own function. Dont worry! Writing your own functions isnt common, but, you might just find as you get better at coding in R that writing your own functions will come in handy! As discussed before, functions allow us to tell the computer to do a series of complicated tasks with only one line of code. For example, the \\(read.csv()\\) function has a lot to it - the computer has to open the file, read each piece of data, convert it into a data frame, and then save it. All we had to do, though, was write just one line of code. Thats because somebody else wrote all those other lines of code  theyre just invisible to us. So, to understand better how this works, we are going to write our own function and then use it. We are going to write a function that converts Fahrenheit to Celsius. There is a simple equation for converting a Fahrenheit temperature into celsius: \\(Celsius = (Fahrenheit - 32)*(5/9)\\). Im going to call my temperature converter fahrenheit_celsius_converter. I can create the function like so: ## We will name our new function fahrenheit_celsius_converter ## here we are saying our function will have one required argument called temp ## temp is required because we did not assign it a value with the = sign fahrenheit_celsius_converter &lt;- function(temp){ ## we generate our new temperature value using the formula to convert fahrenheit to celsius new_temp &lt;- (temp - 32)*(5/9) ## We then &quot;return&quot; the new temperature we have calculated ## We use the return function to tell the computer this is the value we want it to spit out, so to speak return(new_temp) } So, now we can see from the above code that we have made a function and it converts a fahrenheit temperature to a celsius temperature. I have used lots of comments to help make sure that a reader can understand what the function does. Lets quickly test out the function to make sure it works. Some well known values are that freezing in Fahrenheit is 32 and freezing in Celsius is 0 and boiling in Fahrenheit is 212 and in Celsius is 100: fahrenheit_celsius_converter(32) ## [1] 0 fahrenheit_celsius_converter(212) ## [1] 100 Super cool! Now we no longer need to have that formula memorized (thank goodenss, right?!)! We can just use our function and the computer will go fetch the correct code! Now lets add a little bit more complexity to our code! Just for fun (isnt this fun :)?) I am even going to throw in a few functions that you havent seen before. Dont worry a bit! This is just to help get you a little more exposure to some cool tools at your disposal. I want to create an argument for this function that, instead of printing out a numeric value, will print out a sentence stating X degrees Fahrenheit is the same a Y degrees Celsius. Heres what that function looks like (there is more than one way to do this): ## Here we define our function ## We have two arguments ## The first, temp, is required and is the temperature (in Fahrenheit) being fed into the function ## The second, text, is optional. Its default value is false. ## When text is set equal to TRUE, the function will print out the text version of our conversion fahrenheit_celsius_converter &lt;- function(temp, text = FALSE){ ## we generate our new temperature value using the formula to convert fahrenheit to celsius new_temp &lt;- (temp - 32)*(5/9) ## Introducing the if statement ## We can use the if statement to tell the computer to do different things based on different parameters if(text == FALSE){ ## Here we are saying that if the text argument is FALSE, just return the converted value ## This is the same as our original function return(new_temp) } ## What is cool about an if statement is that you can follow it with an ELSE IF statement ## This just tells the computer that if the first if statement wasn&#39;t true, to try to do this one instead else if(text == TRUE){ ## So here, we want to print &quot;X degrees Fahrenheit is the same a Y degrees Celsius&quot; ## Where X = temp and Y = new_temp ## First, temp and new_temp are numeric, but we want them to be character types (because we are printing text!) ## We can use the as.character() function to make this conversion like so temp &lt;- as.character(temp) new_temp &lt;- as.character(new_temp) ## So, now I will introduce you to the paste function ## The paste function lets you stick together character objects to make one longer object output &lt;- paste(temp, &quot;degrees Fahrenheit is the same as&quot;, new_temp,&quot;degrees Celsius&quot;, sep = &quot; &quot;) ## notice the term sep = &quot; &quot; ## I am telling the computer to stick together these four pieces of text and to put a space (&quot; &quot;) in between each one return(output) } ## now, there is always the possibility that someone decided to do something silly like set text = 42 ## there are some other ways to catch this but one way is to use one more ELSE statement ## in this case we don&#39;t need to use an IF because there isn&#39;t anything specific that matters, logically ## if we have gotten to this else statement it means something is wrong else{ return(&quot;The text argument needs to equal TRUE or FALSE -- please try again!&quot;) } } Now, dont worry one bit if some of that just went straight over your head. I showed you this so you can see that even really simple tasks can get a bit complicated really easily. Youll also notice that I wrote a lot of comments alongside the code. I encourage you to always comment your code thoroughly. Its not just because someone else might read your code, but let me tell you, if you dont look at your code for a few weeks, when you come back it might look like gibberish. Commenting your code is a great way to make sure you dont forget what on earth you did! And isnt it nice that now we dont have to type all of that out ourselves. Check out how easy it is to use this function with just one line of code: fahrenheit_celsius_converter(32) ## [1] 0 fahrenheit_celsius_converter(32, text = TRUE) ## [1] &quot;32 degrees Fahrenheit is the same as 0 degrees Celsius&quot; fahrenheit_celsius_converter(32, text = 12) ## [1] &quot;The text argument needs to equal TRUE or FALSE -- please try again!&quot; Alright alright alright, thanks for bearing with me! I promise that my intentions are good and that knowing how to make a temperature converting function will help you as a statistician!! The main takeaway is that functions provide us with an easy way to apply very complicated tasks. In later chapters we will learn how to use various functions that allow us to run different methods like regression - you dont have to know how all the machinery on the inside of the function works, but it is important to understand how to use functions and to understand how they function so you can apply them correctly! 3.4 So Whats A Library? So far, we have already used some cool functions like \\(c()\\), \\(summary()\\), and \\(data.frame()\\). These functions are built in to R, so to speak. Our library is the set of functions that we have loaded into our R session! When you open R, certain functions are already pre-loaded into our library. These are functions that are generally considered so integral to using R that it made sense to have them ready to go when you start working. 3.4.1 How do I see what is in my library? Great question! In order to see what you have loaded in your library, you can start by running the \\(sessionInfo()\\) function like so: sessionInfo() ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.21 digest_0.6.27 R6_2.5.1 jsonlite_1.7.2 ## [5] magrittr_2.0.1 evaluate_0.14 stringi_1.5.3 rlang_0.4.12 ## [9] rstudioapi_0.13 jquerylib_0.1.4 bslib_0.3.1 rmarkdown_2.11 ## [13] tools_3.6.1 stringr_1.4.0 xfun_0.26 yaml_2.2.1 ## [17] fastmap_1.1.0 compiler_3.6.1 htmltools_0.5.2 knitr_1.30 ## [21] sass_0.4.0 We have a few headers of interest: attached base package and other attached packages. We see here this word package. A package is simply a set of functions that can be loaded into your library. Packages are designed and written by people  each package usually has some set of specific, unifying functionality that can help you in some way. For example, the \\(stats\\) package contains functions that allow you to easily run regression models and the \\(ggplot2\\) package contains functions which can help you plot data. An easier way to see which packages you have in your R session is to click on the Packages tab in the bottom right pane like so: Here we see a list of A LOT of packages. These are all of the packages currently downloaded onto my computer. If the checkbox next to a package is marked, that means the package has been loaded into my library. If it isnt marked it means that the package is not loaded. In order to use a package, we must load it into our R session. We dont load all of the packages we have at once because that would overload our computer. I wont get into how R works, but the computer doesnt like it when we load lots of information in R (dont worry, for most of our statistics projects, this is not a concern!). There are two ways to load a package into our library. The most common is to use the \\(library()\\) function. We simply add the name of the package we want to load and it will load the package into our library. Lets say I want to load the \\(tidyverse\\) package. I would do that like so: library(tidyverse) Now, you see the computer gave us a bunch of warnings. That can look a little bit concerning, but I would like to point out two different kinds of message the computer will give you. A warning is a message that the computer provides that basically says Hey, I did what you asked me to do, but here is something you should be aware of! An error message is when the computer fails to do what youve asked - when the computer throws an error, that means it tried to do something and it crashed. Anyway! The other way to load a package into your library is to just scroll down to it in the packages pane and just click on the checkbox! 3.4.1.1 What if I want a new package that isnt on my computer? When you first start using R, you wont have very many packages on your computer. You have to download them. Packages are basically just expansion packs for R. Sometimes it can feel overwhelming figuring out what packages to use and, honestly, there are many many ways to complete the same task. Googling and asking other statisticians which packages to download is an important way to find new packages. Now, usually packages correspond with a task you are interested in. For example, if you want to run a mixed effects regression model, the \\(lme4\\) package was designed exactly for that purpose! If you want to run a mediation analysis, one option is the \\(mediation\\) package. How do I know this - a bit of Googling, a bit of asking around, reading academic manuscripts methods sections, reading the supplemental code attached to a manuscript, reading statistics textbooks, watching YouTube statistics tutorials, and, perhaps most importantly, by reading Q&amp;As on Stack Overflow. Stack Overflow is a forum where people can ask questions and the public can respond. You will likely find your answers to your R questions on Stack Overflow (or Reddit, actually), unless you are doing some real niche stuff! I actually would like to share with you one of my absolute favorite packages: \\(tableone\\). We are going to be using this package a lot! This package is for building descriptive tables of your study sample. It is called tableone because Table One in a human subjects research paper is typically the table describing the study sample. I would say this is the most important table in your entire study! If you dont know who is in your study, then, umm, how do you know what youre studying??? So, it turns out that the tabelone package isnt automatically built in to R. We have to download it. We have two options. The first is the click and point way. In the Packages pane, click on the Install button, like so: You can then start to type in the name of the package we want, tableone, in the appropriate box, like so. Notice how it will provide you a dropdown of available packages. These are packages that have been registered so that the computer knows to find them: Type in the full name of the package you want or click on it when you see it. Then you will click the Install button like so: Et voila! You have downloaded the package. You can also install the package by running the \\(install.packages()\\) function like so: install.packages(&quot;tableone&quot;) Usually, when you install, you will get a lot of messages. At the end, you want to see the message The downloaded source packages are in. If you get an ERROR message, that means your computer failed to download the package. Now that we have downloaded the package, we can simply add it to our library by typing in: library(tableone) So, thats pretty darn cool! The cool thing about R is that all the expansion packs are free!!! 3.5 Quick word of caution R is awesome, in part because it is free. It is an open source software which means that anyone can write packages for R and register them. That doesnt mean that they are inherently untrustworthy, but it does mean we need to develop a spidey-sense for when a package might be suspect. You know how on Wikipedia, sometimes there is a message that says something like This article doesnt appear to have sources - well, it is important to be able to always ask if a package is a good one for what you want to do. The nice thing about R is that there is a huge community of users online that discuss these things. Some packages are so commonly applied that the statistics community has come to trust in them. Code was written by people and people make mistakesremember, the computer just does what you tell it to do, if you tell it the wrong thing then itll do the wrong thing. The takeaway of me telling you this is that when you use a new package, do a little vetting. Ask your stats professor, ask the internet, read up about it! One hint I will give you (and let me tell you this is not a rule by any means) is to look at the documentation files for the package you are considering to use! The more useful this file is, the more likely this is a good package. If you open a documentation file and it barely tells you anything  run! 3.6 In Conclusion Functions are an important part of coding in R. Nearly all statistical algorithms will be run by calling functions. Understanding what a function is and how they operate will help you better understand what is happening when you use them. While you wont necessarily ever need to write your own functions, being able to do so will also empower you to take on complicated quantitative approaches as you become a more advanced statistician! "],["working-with-datasets-in-r.html", "Chapter 4 Working with Datasets in R 4.1 Loading a Dataset into R 4.2 Exploring the Data", " Chapter 4 Working with Datasets in R R provides us a powerful interface through which we can manipulate and work with our data. While it is important that we understand different statistical principles so that we may undertake rigorous quantitative work, a large portion of time will be spent loading data, cleaning data, and preparing data for analyses. In fact, it is not uncommon for 90% or more of the code you write for an analysis to be dedicated to just getting the data ready for the analysis itself. As such, to be effective applied statisticians, it is important we be comfortable working with and manipulating datasets. 4.1 Loading a Dataset into R One of the most common formats for a dataset to be in is a .csv file. CSV stands for comma-separated values. You are likely familiar with CSVs, as they are a standard formatting option for spreadsheet data (i.e., Microsoft Excel, Google Sheets). We can use the \\(read.csv()\\) function in R to read a .csv file, like so: data &lt;- read.csv(&quot;filename.csv&quot;, header = T) In the above line of code, we read the a file called filename.csv and saved it into the object weve called data. We use the argument \\(header = T\\) to tell R that the first row of the csv contains the names of each of the columns (i.e., the names of our variables), as opposed to containing data. In order to read the file, you must have the Current Working Directory set to the folder containing the file. You see, R treats each folder on your computer like a room - R hangs out in the room that you tell it to. So, if you want R to read in filename.csv, you need to make sure that R is in the correct room (folder). You can check what your working directory is using the \\(getwd()\\) function: getwd() ## [1] &quot;C:/Users/Charl/OneDrive/Documents/Intro to Applied Stats with R&quot; Now, if R is in the wrong folder, you need to tell it to go to the correct folder. You could use the \\(setwd(dir = ...)\\) function, but RStudio has a nice way to do this without typing code. On the bottom right hand corner of your RStudio window is the File Explorer. You can click your way to the correct folder such that you can see your file, like so: Once, here, you then click the More dropdown and select Set as Working Directory, like so: Now you are in the correct directory and can load your file in. 4.1.1 Loading Other Formats of Data Now, .csvs are not the only type of file format your data may take. There are .RDS (R specific objects), .xlsx (Excel), .sas7bdat (SAS), .sav (SPSS) files, among many others. While I cannot provide an exhaustive list, I wanted to provide some libraries and functions that may be useful to you depending on where your file came from. The \\(foreign\\) and \\(readxl\\) libraries are particularly useful. Here are some examples for how you can read in various file types: ## Reading an RDS file dataRDS &lt;- readRDS(&quot;filename.rds&quot;) ## Reading an Excel file ## You will need the &quot;readxl&quot; library library(readxl) dataXL &lt;- read_excel(&quot;filename.xlsx&quot;) ## Reading an SPSS file ## You will need the &quot;foreign&quot; library library(foreign) dataSPSS &lt;- read.spss(&quot;filename.sav&quot;) ## Reading SAS file ## You will need the &quot;haven&quot; library library(haven) dataSAS &lt;- read_sas(&quot;filename.sas7bdat&quot;) If you look at the documentation, you will see that both the \\(foreign\\) and \\(haven\\) libraries provide many functions for loading various data types. If you ever have a file type that you can not figure out how to load into R, you should search something like Load [insert file type] into R. There is very likely a Stack Overflow forum post that will explain to you how to do it. While R does not have the same type of official documentation as proprietary software like SPSS or SAS, R does have a broad community of users who share and discuss how to use R effectively. Google is your friend :) 4.2 Exploring the Data Once the dataset is loaded into R, R treats it as a data.frame object. It is important that we know how to use R to look at the data. Often, one of the hardest parts of running a statistical analysis is making sure that our data is in the correct format. Usually, when we receive a dataset it is not perfectly formatted for our intended purposes. We are going to use data from the BRFSS survey from the state of New York to display how we can use R to work with data. You can download the dataset here if youd like to follow along: data &lt;- read.csv(&quot;Data/ny_2016_brfss.csv&quot;, header = T) 4.2.1 Keep Your Codebook Handy Before we look at the data, it is crucial that you have the codebook for your data on hand. The codebook is a form of documentation that accompanies data which explains what each variable in the dataset is and how it was calculated. Typically, if you download a dataset from a public source or receive a dataset as part of a study, there is a codebook which accompanies it. However, in some cases (particularly on smaller research projects), there is no codebook - in such cases, getting a copy of the survey questionaire is necessary so that you can match up variables in the dataset with the questions that were asked on the survey. A codebook tells you what is in your dataset. You need to have it handy whenever you work with a given dataset. The codebook for the 2016 BRFSS study is available here 4.2.2 The Environment pane The first thing we will notice when we load the dataset is that the environment pane in the top right corner of RStudio will update to include it, seeing something like: From this information, we can see that our dataset is made up of 34,190 observations (rows) and 375 variables. If you click the right arrow next to the name of the data.frame, then it will expand and show the first several variables and some values. We can also see in this example the letters int before many of the variables. This is RStudio tellings you that each of theses variables is an integer type of data. You might also see num for numeric, char for character (like written text), or factor for categorical data. Finally, if you click on the name of the data.frame directly, it will execute the \\(View()\\) function and open the data.frame in a spreadsheet style view, which looks like: There are buttons and filters you can use to explore the data, much like how you would be able to if you opened the file in Excel. This view can be useful for looking at the data, but we typically do not do any of our analytic work via this View. Sometimes, it can be useful to open this View to confirm that some operation executed correctly or to try to identify specific instances of missing data. 4.2.3 Functions for Exploring and Cleaning Data When you first load a dataset, there are many functions which can be useful to know. The first is the \\(head()\\) function. This function tells R to print out the first several rows of the data.frame, like so: head(data) ## X X_STATE X_GEOSTR PRECALL SECSCRFL REPNUM REPDEPTH FMONTH IDATE IMONTH ## 1 1 36 201 1 NA 120006 3 12 12052016 12 ## 2 2 36 206 1 NA 30046 13 3 3142016 3 ## 3 3 36 201 1 NA 10025 15 1 2172016 2 ## 4 4 36 201 1 NA 50098 12 5 5152016 5 ## 5 5 36 212 1 NA 100019 6 10 11092016 11 ## 6 6 36 201 1 NA 80006 30 8 8052016 8 ## IDAY IYEAR DISPCODE SEQNO X_PSU NATTMPTS NRECSEL NRECSTR PVTRESD1 ## 1 5 2016 1200 2016000012 2016000012 1 58621 19463347 NA ## 2 14 2016 1200 2016000013 2016000013 3 15440 1202936 NA ## 3 17 2016 1200 2016000014 2016000014 7 58621 19463347 NA ## 4 15 2016 1200 2016000015 2016000015 2 58621 19463347 NA ## 5 9 2016 1200 2016000016 2016000016 5 135028 1660029 NA ## 6 5 2016 1200 2016000017 2016000017 1 58621 19463347 NA ## COLGHOUS STATERES LADULT NUMADULT CADULT PVTRESD3 CCLGHOUS CSTATE1 RSPSTAT1 ## 1 NA NA NA NA 1 1 NA 2 36 ## 2 NA NA NA NA 2 1 NA 2 36 ## 3 NA NA NA NA 1 1 NA 2 36 ## 4 NA NA NA NA 1 1 NA 2 36 ## 5 NA NA NA NA 2 1 NA 2 36 ## 6 NA NA NA NA 1 1 NA 2 36 ## LANDLINE HHADULT GENHLTH PHYSHLTH MENTHLTH POORHLTH HLTHPLN1 PERSDOC2 MEDCOST ## 1 2 1 1 88 88 NA 1 3 2 ## 2 2 5 4 5 20 2 1 2 2 ## 3 1 3 3 3 30 10 1 1 1 ## 4 2 1 4 30 12 30 2 1 1 ## 5 2 2 2 88 88 NA 1 2 2 ## 6 2 3 2 1 20 88 1 3 1 ## CHECKUP1 EXERANY2 SLEPTIM1 CVDINFR4 CVDCRHD4 CVDSTRK3 ASTHMA3 ASTHNOW ## 1 2 1 6 2 2 2 2 NA ## 2 3 2 6 2 2 2 2 NA ## 3 1 1 5 2 2 2 2 NA ## 4 2 1 6 2 2 2 2 NA ## 5 2 1 7 2 2 2 2 NA ## 6 2 1 7 2 2 2 1 2 ## CHCSCNCR CHCOCNCR CHCCOPD1 HAVARTH3 ADDEPEV2 CHCKIDNY DIABETE3 DIABAGE2 ## 1 2 2 2 2 2 2 3 NA ## 2 2 2 2 2 1 2 3 NA ## 3 2 2 2 2 1 2 3 NA ## 4 2 2 2 2 1 2 3 NA ## 5 2 2 2 2 2 2 3 NA ## 6 2 2 2 2 1 2 3 NA ## LASTDEN3 RMVTETH3 SEX AGE HISPANC3 MRACE1 ORACE3 MARITAL EDUCA RENTHOM1 ## 1 1 8 1 33 5 10 NA 5 6 2 ## 2 1 8 2 20 5 1088 NA 5 5 2 ## 3 4 8 1 47 5 1088 NA 5 6 3 ## 4 1 8 1 22 5 1088 NA 5 4 2 ## 5 1 8 2 36 5 1088 NA 1 5 2 ## 6 2 8 1 26 5 1088 NA 6 6 2 ## CTYCODE1 ZIPCODE1 NUMHHOL2 NUMPHON2 CPDEMO1 VETERAN3 EMPLOY1 CHILDREN INCOME2 ## 1 NA DSU NA NA NA 2 1 88 8 ## 2 NA DSU NA NA NA 2 6 88 77 ## 3 NA DSU NA NA NA 1 1 88 8 ## 4 NA DSU NA NA NA 2 4 88 3 ## 5 NA DSU NA NA NA 2 1 3 6 ## 6 NA DSU NA NA NA 2 1 88 77 ## INTERNET WEIGHT2 HEIGHT3 PREGNANT DEAF BLIND DECIDE DIFFWALK DIFFDRES ## 1 1 NA NA NA 2 2 2 2 2 ## 2 1 NA NA 2 2 2 2 2 2 ## 3 1 NA NA NA 2 2 1 2 2 ## 4 1 NA NA NA 2 2 2 2 2 ## 5 1 NA NA 2 2 2 2 2 2 ## 6 1 NA NA NA 2 2 7 2 2 ## DIFFALON SMOKE100 SMOKDAY2 STOPSMK2 LASTSMK2 USENOW3 ECIGARET ECIGNOW ALCDAY5 ## 1 2 1 1 2 NA 3 1 3 103 ## 2 1 2 NA NA NA 3 1 3 203 ## 3 2 2 NA NA NA 3 2 NA 102 ## 4 2 1 3 NA 5 3 1 2 888 ## 5 2 1 3 NA 5 3 1 1 888 ## 6 2 1 3 NA 4 3 1 3 215 ## AVEDRNK2 DRNK3GE5 MAXDRNKS FLUSHOT6 FLSHTMY2 PNEUVAC3 TETANUS FALL12MN ## 1 7 12 10 2 NA 2 3 NA ## 2 2 88 3 2 NA 1 3 NA ## 3 2 88 3 1 102015 2 2 2 ## 4 NA NA NA 1 22016 1 4 NA ## 5 NA NA NA 2 NA 7 3 NA ## 6 1 4 7 1 22016 7 7 NA ## FALLINJ2 SEATBELT DRNKDRI2 HADMAM HOWLONG HADPAP2 LASTPAP2 HPVTEST HPLSTTST ## 1 NA 4 88 NA NA NA NA NA NA ## 2 NA 1 88 2 NA 2 NA 2 NA ## 3 88 1 88 NA NA NA NA NA NA ## 4 NA 1 NA NA NA NA NA NA NA ## 5 NA 1 NA 1 1 1 1 1 1 ## 6 NA 2 88 NA NA NA NA NA NA ## HADHYST2 PCPSAAD2 PCPSADI1 PCPSARE1 PSATEST1 PSATIME PCPSARS1 BLDSTOOL ## 1 NA NA NA NA NA NA NA NA ## 2 2 NA NA NA NA NA NA NA ## 3 NA 1 1 1 1 1 5 NA ## 4 NA NA NA NA NA NA NA NA ## 5 2 NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## LSTBLDS3 HADSIGM3 HADSGCO1 LASTSIG3 HIVTST6 HIVTSTD3 HIVRISK4 PDIABTST ## 1 NA NA NA NA 1 62016 1 NA ## 2 NA NA NA NA 2 NA 2 NA ## 3 NA NA NA NA 1 772015 NA NA ## 4 NA NA NA NA 2 NA 2 NA ## 5 NA NA NA NA 1 12008 2 NA ## 6 NA NA NA NA 7 NA 1 NA ## PREDIAB1 CAREGIV1 CRGVREL1 CRGVLNG1 CRGVHRS1 CRGVPRB2 CRGVPERS CRGVHOUS ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## CRGVMST2 CRGVEXPT CIMEMLOS CDHOUSE CDASSIST CDHELP CDSOCIAL CDDISCUS SSBSUGR2 ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA ## SSBFRUT2 TYPEWORK TYPEINDS SXORIENT TRNSGNDR RCSBIRTH RCSGENDR RCHISLA1 ## 1 NA DSU DSU NA NA DSU NA DSU ## 2 NA DSU DSU NA NA DSU NA DSU ## 3 NA DSU DSU NA NA DSU NA DSU ## 4 NA DSU DSU NA NA DSU NA DSU ## 5 NA DSU DSU NA NA DSU NA DSU ## 6 NA DSU DSU NA NA DSU NA DSU ## RCSRACE1 RCSBRAC2 RCSRLTN2 CASTHDX2 CASTHNO2 QLACTLM2 USEEQUIP QSTVER QSTLANG ## 1 DSU NA NA NA NA NA NA 20 1 ## 2 DSU NA NA NA NA NA NA 20 1 ## 3 DSU NA NA NA NA NA NA 20 1 ## 4 DSU NA NA NA NA NA NA 20 1 ## 5 DSU NA NA NA NA NA NA 20 1 ## 6 DSU NA NA NA NA NA NA 20 1 ## X_MSACODE MSCODE X_STSTR X_STRWT X_RAW X_WT2 X_RAWRAKE X_WT2RAKE ## 1 DSU NA 362019 332.02004 NA 332.02004 1 332.02004 ## 2 DSU NA 362069 77.91039 NA 77.91039 1 77.91039 ## 3 DSU NA 362019 332.02004 NA 332.02004 1 332.02004 ## 4 DSU NA 362019 332.02004 NA 332.02004 1 332.02004 ## 5 DSU NA 362129 12.29396 NA 12.29396 1 12.29396 ## 6 DSU NA 362019 332.02004 NA 332.02004 1 332.02004 ## X_REGION X_IMPSEX X_IMPAGE X_IMPRACE X_IMPNPH X_IMPCTY X_IMPEDUC X_IMPMRTL ## 1 NA 1 33 1 NA NA 6 5 ## 2 NA 2 20 1 NA NA 5 5 ## 3 NA 1 47 1 NA NA 6 5 ## 4 NA 1 22 1 NA NA 4 5 ## 5 NA 2 36 1 NA NA 5 1 ## 6 NA 1 26 1 NA NA 6 6 ## X_IMPHOME O_STATE X_CHISPNC X_CRACE1 X_CPRACE X_IMPCAGE X_IMPCRAC X_IMPCSEX ## 1 2 1 NA NA NA NA NA NA ## 2 2 2 NA NA NA NA NA NA ## 3 3 4 NA NA NA NA NA NA ## 4 2 4 NA NA NA NA NA NA ## 5 2 4 NA NA NA NA NA NA ## 6 2 5 NA NA NA NA NA NA ## X_RAWCH X_WT2CH X_CLCM1V2 X_CLCM2V2 X_CLCM3V2 X_CLCM4V2 X_CLCM5V2 X_CLCWTV2 ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## X_DUALUSE X_DUALCOR X_LLCPWT2 X_LLCPM01 X_LLCPM02 X_LLCPM03 X_LLCPM04 ## 1 9 NA 1503.22419 2 1 4 2 ## 2 9 NA 437.50684 8 1 3 2 ## 3 2 0.3749592 1797.50648 4 1 4 2 ## 4 9 NA 1941.69415 1 1 2 2 ## 5 9 NA 72.34833 10 1 3 1 ## 6 9 NA 1941.69415 2 1 4 2 ## X_LLCPM05 X_LLCPM06 X_LLCPM07 X_LLCPM08 X_LLCPM09 X_LLCPM10 X_LLCPM11 ## 1 2 1 1 1 5 26 9 ## 2 2 6 1 1 17 100 34 ## 3 2 1 5 3 4 21 7 ## 4 2 1 1 1 4 18 7 ## 5 2 6 5 1 22 135 44 ## 6 2 1 1 1 4 19 7 ## X_LLCPM12 X_LLCPM13 X_LLCPM14 X_LLCPM15 X_LLCPM16 X_LLCPWT X_LCM01V1 ## 1 15 14 30 82 27 1544.12591 NA ## 2 50 29 68 173 58 778.14863 NA ## 3 11 11 19 64 21 1097.60766 NA ## 4 11 11 19 61 21 1770.86414 NA ## 5 57 26 61 160 52 40.91907 NA ## 6 11 11 19 62 21 1105.47228 NA ## X_LCM02V1 X_LCM03V1 X_LCM04V1 X_LCM05V1 X_LCM06V1 X_LCM07V1 X_LCM08V1 ## 1 NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA ## X_LCPWTV1 X_LCM01V2 X_LCM02V2 X_LCM03V2 X_LCM04V2 X_LCM05V2 X_LCM06V2 ## 1 NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA ## X_LCM07V2 X_LCM08V2 X_LCPWTV2 X_LCM01V3 X_LCM02V3 X_LCM03V3 X_LCM04V3 ## 1 NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA ## X_LCM05V3 X_LCM06V3 X_LCM07V3 X_LCM08V3 X_LCPWTV3 X_RFHLTH X_PHYS14D ## 1 NA NA NA NA NA 1 1 ## 2 NA NA NA NA NA 2 2 ## 3 NA NA NA NA NA 1 2 ## 4 NA NA NA NA NA 2 3 ## 5 NA NA NA NA NA 1 1 ## 6 NA NA NA NA NA 1 2 ## X_MENT14D X_HCVU651 X_TOTINDA X_MICHD X_LTASTH1 X_CASTHM1 X_ASTHMS1 X_DRDXAR1 ## 1 1 1 1 2 1 1 3 2 ## 2 3 1 2 2 1 1 3 2 ## 3 3 1 1 2 1 1 3 2 ## 4 2 2 1 2 1 1 3 2 ## 5 1 1 1 2 1 1 3 2 ## 6 3 1 1 2 2 1 2 2 ## X_EXTETH2 X_ALTETH2 X_DENVST2 X_MRACE1 X_M_RACE X_HISPANC X_RACE X_RACEG21 ## 1 1 NA 1 1 10 2 1 1 ## 2 1 NA 1 1 10 2 1 1 ## 3 1 NA 2 1 10 2 1 1 ## 4 1 NA 1 1 10 2 1 1 ## 5 1 NA 1 1 10 2 1 1 ## 6 1 NA 2 1 10 2 1 1 ## X_RACEGR3 X_RACE_G1 X_AGEG5YR X_AGE65YR X_AGE80 X_AGE_G HTIN4 HTM4 WTKG3 ## 1 1 1 3 1 33 2 NA NA NA ## 2 1 1 1 1 20 1 NA NA NA ## 3 1 1 6 1 47 4 NA NA NA ## 4 1 1 1 1 22 1 NA NA NA ## 5 1 1 4 1 36 3 NA NA NA ## 6 1 1 2 1 26 2 NA NA NA ## X_BMI5 X_BMI5CAT X_RFBMI5 X_CHLDCNT X_EDUCAG X_INCOMG X_SMOKER3 X_RFSMOK3 ## 1 NA 2 1 1 4 5 1 2 ## 2 NA 2 1 1 3 9 4 1 ## 3 NA 2 1 1 4 5 4 1 ## 4 NA 3 2 1 2 2 3 1 ## 5 NA 2 1 4 3 4 3 1 ## 6 NA 2 1 1 4 9 3 1 ## X_ECIGSTS X_CURECIG DRNKANY5 DROCDY3_ X_RFBING5 X_DRNKWEK X_RFDRHV5 X_FLSHOT6 ## 1 3 1 1 43 2 2100 2 NA ## 2 3 1 1 10 1 140 1 NA ## 3 4 1 1 29 1 400 1 NA ## 4 2 2 2 0 1 0 1 NA ## 5 1 2 2 0 1 0 1 NA ## 6 3 1 1 50 2 350 1 NA ## X_PNEUMO2 X_RFSEAT2 X_RFSEAT3 X_DRNKDRV X_RFMAM2Y X_MAM5021 X_RFPAP33 ## 1 NA 2 2 2 NA NA NA ## 2 NA 1 1 2 NA NA NA ## 3 NA 1 1 2 NA NA NA ## 4 NA 1 1 9 NA NA NA ## 5 NA 1 1 9 NA NA 1 ## 6 NA 1 2 2 NA NA NA ## X_RFPSA21 X_RFBLDS3 X_COL10YR X_HFOB3YR X_FS5YR X_FOBTFS X_CRCREC X_AIDTST3 ## 1 NA NA NA NA NA NA NA 1 ## 2 NA NA NA NA NA NA NA 2 ## 3 1 NA NA NA NA NA NA 1 ## 4 NA NA NA NA NA NA NA 2 ## 5 NA NA NA NA NA NA NA 1 ## 6 NA NA NA NA NA NA NA 9 ## MEDICARE HLTHCVR1 STRSRENT STRSMEAL BPHIGH4 BPMEDS FALOLDRE FALREDUC EVERWALK ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA ## EVERBIKE HEALTHCL1 HLTHPREG PREGEVER BRTHCNTL3 TYPCNTRL2 CLRCTREC EATFRUIT ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## EATVEGET SEXHIST HSCNDMS HCVHEAR HCVTEST HCVLASTT HCVINPTR HCVINPTO HCVINPTA ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA ## HCVPRIMR HCVPRIMO HCVPRIMA HCVTESTB ACEDEPRS ACEDRINK ACEDRUGS ACEPRISN ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## ACEDIVRC ACEPUNCH ACEHURT ACESWEAR ACETOUCH ACETTHEM ACEHVSEX STRSYMP1 ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## STRSYMP2 STRSYMP3 STRSYMP5 STRSYMP6 FIRSTAID2 CMGENER CMDETECT CMBATTER ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## CAREGIVB CIMEMLSB CDDISCSB REGION DSRIPREG PPS_1 PPS_3 PPS_8 PPS_9 PPS_14 ## 1 NA NA NA 2 102 NA NA NA NA NA ## 2 NA NA NA 1 104 NA NA NA NA NA ## 3 NA NA NA 2 102 NA NA NA NA NA ## 4 NA NA NA 2 102 NA NA NA NA NA ## 5 NA NA NA 1 106 NA NA NA NA NA ## 6 NA NA NA 2 102 NA NA NA NA NA ## PPS_16 PPS_19 PPS_20 PPS_21 PPS_22 PPS_23 PPS_25 PPS_27 PPS_32 PPS_33 PPS_34 ## 1 NA NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA NA NA ## PPS_36 PPS_39 PPS_40 PPS_43 PPS_44 PPS_45 PPS_46 PPS_48 PPS_52 X_CTYWGT ## 1 NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA NA Youll notice that we are currently working with a lot of data. 375 variables is way more than we need for our analysis, so, I want to narrow the dataset down to just the variables we need (this is where having the codebook nearby is useful!). For my study I need the following data: age, sex, educational attainment, lifetime cigarette smoking, and lifetime diagnosis of heart disease. In the BRFSS it turns out they have two separate variables for heart disease, one related to myocardial infarction (CVDINFR4) and angina (CVDCRHD4), so I want both of those. I can narrow down my dataset like so: ## First, I look in the codebook and I find the names of the variables I want ## I am going to save the variable names into a vector like so variables &lt;- c(&quot;SEX&quot;,&quot;AGE&quot;,&quot;EDUCA&quot;,&quot;SMOKE100&quot;,&quot;CVDINFR4&quot;,&quot;CVDCRHD4&quot;) ## Next, we simply subset our dataset using these variable names ## The following line of code tells R to only include the columns of the dataset that contain the specified variable names data &lt;- data[,variables] ## Now we can run the head() function to confirm that it work head(data) ## SEX AGE EDUCA SMOKE100 CVDINFR4 CVDCRHD4 ## 1 1 33 6 1 2 2 ## 2 2 20 5 2 2 2 ## 3 1 47 6 2 2 2 ## 4 1 22 4 1 2 2 ## 5 2 36 5 1 2 2 ## 6 1 26 6 1 2 2 Cool! 6 variables is much easier to work with than 375. However, all of the values for the variables are numbers!!! What on earth does 1 mean for SMOKE100? For that matter, what is SMOKE100??? So, this is where the codebook is truly necessary. It appears that many of these variables are categorical, but the dataset only contains the numeric values for the categories. So, now we need to use the codebook and assign that actual category levels. For example, for the sex variable we see the values 1 and 2it seems intuitive that these may correspond to Male and Female, but without the codebook we have no way of knowing which! Lets start by opening the codebook up and looking at the EDUCA variable. Now, to find it I am just going to use the Ctrl+F command and type in EDUCA to find it. Here is a photo of the entry: First we can see that the specific question is What is the highest grade or year of school you completed? Then we can see what each numeric value maps onto. A value of 1 means Never attended school or only kindergarten, a value of 2 means Grades 1 through 8 (Elementary) and so on. Now, we are going to use the \\(factor()\\) function like we learned in the last class to turn the numeric value into a categorical one, like so: ## Factor Educational Attainment data$EDUCA &lt;- factor(data$EDUCA, levels = c(1,2,3,4,5,6,9), labels = c(&quot;Never attended school or only kindergarten&quot;, &quot;Grades 1 through 8&quot;, &quot;Grades 9 through 11&quot;, &quot;Grade 12 or GED&quot;, &quot;College 1 year to 3 years&quot;, &quot;College 4 years or more&quot;, &quot;Refused to Answer&quot;)) Now, we can repeat this process with each of the categorical variables like so: ## Factor SEX data$SEX &lt;- factor(data$SEX, levels = c(1,2,9), labels = c(&quot;Male&quot;,&quot;Female&quot;,&quot;Refused&quot;)) ## Factor Smoke 100 - Have you smoked at least 100 cigarettes in your entire life? ## I decided to write out the full question here, because the variable name does not make it obvious data$SMOKE100 &lt;- factor(data$SMOKE100, levels = c(1,2,7,9), labels = c(&quot;Yes&quot;, &quot;No&quot;, &quot;Don&#39;t Know&quot;, &quot;Refused&quot;)) ## Factor CVDINF4 - &quot;Has a doctor, nurse, or other health professional ever told you ## that you had a heart attack, also called a myocardial infarction?&quot; data$CVDINFR4 &lt;- factor(data$CVDINFR4, levels = c(1,2,7,9), labels = c(&quot;Yes&quot;, &quot;No&quot;, &quot;Don&#39;t Know/Not Sure&quot;, &quot;Refused&quot;)) ## Factor CVDCRHD4 - &quot;Has a doctor, nurse, or other health professional ever told you ## that you had angina or coronary heart disease? ## NOTE: This next line of code is almost identical to the one above ## Copy-pasting will save you lots of time BUT, you need to make sure you change the parts that are different data$CVDCRHD4 &lt;- factor(data$CVDCRHD4, levels = c(1,2,7,9), labels = c(&quot;Yes&quot;, &quot;No&quot;, &quot;Don&#39;t Know/Not Sure&quot;, &quot;Refused&quot;)) Whats nice with R is that now that you wrote these functions you dont have to write them again. Every time you load your dataset, you can also run these lines of codes to prepare your dataset for analysis. This is also good because you never have to edit your raw data file. Never edit the raw datafile you got for analysis. R lets you modify the data after youve loaded it, so that you can work with your dataset without editing the raw file. So, lets try and run the \\(head()\\) function again and see how our data looks now: head(data) ## SEX AGE EDUCA SMOKE100 CVDINFR4 CVDCRHD4 ## 1 Male 33 College 4 years or more Yes No No ## 2 Female 20 College 1 year to 3 years No No No ## 3 Male 47 College 4 years or more No No No ## 4 Male 22 Grade 12 or GED Yes No No ## 5 Female 36 College 1 year to 3 years Yes No No ## 6 Male 26 College 4 years or more Yes No No Thats much better! However, we have a lot of NOs over there in the CVDINFR4 column. It would be weird if they were all NOs, so we want a way that lets us see the variable breakdown (i.e., How many YES responses and how many NO responses). This is where we can use the \\(summary()\\) function, like so: summary(data$CVDINFR4) ## Yes No Don&#39;t Know/Not Sure Refused ## 2021 32020 130 19 summary(data$CVDCRHD4) ## Yes No Don&#39;t Know/Not Sure Refused ## 2127 31853 186 24 Nice! So now we can see how many Yes, No responses to each question, as well as how many people didnt know or refused to answer. We can also run the \\(summary()\\) function on the entire dataset and it will show us a similar overview for each variable. Notice how the computer seems to recognize that age is a numeric variable and, instead of providing frequencies, it instead provides us with Mean, Median and the Interquartile Range. summary(data) ## SEX AGE ## Male :15057 Min. : 7.00 ## Female :19131 1st Qu.:41.00 ## Refused: 2 Median :57.00 ## Mean :54.52 ## 3rd Qu.:68.00 ## Max. :99.00 ## ## EDUCA SMOKE100 ## Never attended school or only kindergarten: 40 Yes :15177 ## Grades 1 through 8 : 923 No :17126 ## Grades 9 through 11 : 1989 Don&#39;t Know: 80 ## Grade 12 or GED : 9952 Refused : 28 ## College 1 year to 3 years : 8815 NA&#39;s : 1779 ## College 4 years or more :12303 ## Refused to Answer : 168 ## CVDINFR4 CVDCRHD4 ## Yes : 2021 Yes : 2127 ## No :32020 No :31853 ## Don&#39;t Know/Not Sure: 130 Don&#39;t Know/Not Sure: 186 ## Refused : 19 Refused : 24 ## ## ## So, for my research, I want to know the association between educational attainment and heart disease. But, it appears that many people did not respond to the education and heart disease questions. While we will learn other ways to impute missing data, for right now I want to show you how to simply remove them from your data set. This is why it is important to not ever edit your raw data file. It would be bad if you actually deleted an entry from your dataset. In R, we can always go back to the top of the file and reload the dataset if we think we made a mistake. So, lets start by removing all the people in the dataset who refused to answer the educational attainment question. There are many ways to approach removing a variable, but Ill show you one I like to use. With coding, there are usually multiple ways to accomplish a given task. ## Let&#39;s remove all rows in the data set where EDUCA == &quot;Refused to Answer&quot; ## We do this with conditional brackets by telling the computer ## to only include rows in data where EDUCA does not equal (!=) &quot;Refused to Answer&quot; data &lt;- data[data$EDUCA != &quot;Refused to Answer&quot;,] ## Notice how in the brackets we put a comma ## That is because data is a data.frame, meaning it is two-dimensional ## So, we must provide both a row number and a column number ## Here, we have left the column number blank, letting it know we want ALL the columns ## For row, we have told it to only include rows where data$EDUCA does not equal &quot;Refused to Answer&quot; ## NOTE: If you are getting an error, check to make sure that you have (or don&#39;t have) the comma ## That is a really common error to get ## Now let&#39;s look at our data! summary(data$EDUCA) ## Never attended school or only kindergarten ## 40 ## Grades 1 through 8 ## 923 ## Grades 9 through 11 ## 1989 ## Grade 12 or GED ## 9952 ## College 1 year to 3 years ## 8815 ## College 4 years or more ## 12303 ## Refused to Answer ## 0 ## As we can see, we got rid of all the observations we didn&#39;t want BUT ## We also do not want the level &quot;Refused to Answer&quot; to appear in our dataset anymore ## That could mess things up when we eventually run some analyses ## So, we will use the droplevels() function like so data$EDUCA &lt;- droplevels(data$EDUCA) ## The function tells the computer to get rid of levels if they don&#39;t appear in the vector ## Now let&#39;s look at our data again summary(data$EDUCA) ## Never attended school or only kindergarten ## 40 ## Grades 1 through 8 ## 923 ## Grades 9 through 11 ## 1989 ## Grade 12 or GED ## 9952 ## College 1 year to 3 years ## 8815 ## College 4 years or more ## 12303 So now we can repeat the process for our two heart disease variables. Notice that there are actually two levels we want to get rid of Dont Know/Not Sure and Refused, so it is slightly different than above. ## I will show you two similar but slightly different ways to remove these two levels ## First, we can write two separate lines of code, one for each level like so data &lt;- data[data$CVDINFR4 != &quot;Don&#39;t Know/Not Sure&quot;,] data &lt;- data[data$CVDINFR4 != &quot;Refused&quot;,] ## Print out summary to confirm summary(data$CVDINFR4) ## Yes No Don&#39;t Know/Not Sure Refused ## 2010 31869 0 0 ## Perfect, now let&#39;s just drop those levels data$CVDINFR4 &lt;- droplevels(data$CVDINFR4) ## We can also do this in a single line using the &amp; operator like so data &lt;- data[data$CVDCRHD4 != &quot;Don&#39;t Know/Not Sure&quot; &amp; data$CVDCRHD4 != &quot;Refused&quot;,] ## With the &amp; we are telling the computer to include rows ## only if CVDCRHD4 doesn&#39;t equal &quot;Don&#39;t Know/NOt Sure&quot; AND doesn&#39;t equal &quot;Refused&quot; ## Print out summary to confirm summary(data$CVDCRHD4) ## Yes No Don&#39;t Know/Not Sure Refused ## 2107 31589 0 0 ## And then drop those levels data$CVDCRHD4 &lt;- droplevels(data$CVDCRHD4) Amazing, we have cleaned up some of our data! The last thing I want to do before we run our analysis is to create a new heart disease variable. I want my new variable, which I will call any_heart_disease, to be YES if either of the two heart disease variables are YES and be NO if both are NO. We are going to use a for-loop and if-else statements to do this. Like I said before, there are often many ways to do this. Understanding for-loops and if-else statements will be really useful for you as you continue your statistical coding journey! ## First, I am going to create the new column in our dataset ## I am going to assign NA as the value for this column ## We will fill in the values with our for-loop data$any_heart_disease &lt;- NA ## Let&#39;s look at our dataset real quick to confirm head(data) ## SEX AGE EDUCA SMOKE100 CVDINFR4 CVDCRHD4 ## 1 Male 33 College 4 years or more Yes No No ## 2 Female 20 College 1 year to 3 years No No No ## 3 Male 47 College 4 years or more No No No ## 4 Male 22 Grade 12 or GED Yes No No ## 5 Female 36 College 1 year to 3 years Yes No No ## 6 Male 26 College 4 years or more Yes No No ## any_heart_disease ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA ## Now, I am going to use a for-loop to fill in each value ## We start by calling the for function like so for(row_number in 1:nrow(data)){ ## let&#39;s break this down ## this line can be translated as ## &quot;For each number, saved as row_number, inbetween 1 and the number of rows in our data.frame, do the following...&quot; ## Inside the brackets we then write the code we want to repeat ## This allows us to tell the computer to repeat an operation for each row ## so, we can treat row_number as a counter that keeps track of what row in the data frame we are on ## So, we want to check if either of our two heart disease variables are &quot;Yes&quot; ## Let&#39;s grab them like so: infarction &lt;- data$CVDINFR4[row_number] ## By writing [row_number] we are telling the computer to get the value at the current row we are working with ## We can do this with the other variable as well angina &lt;- data$CVDCRHD4[row_number] ## Perfect, we have our two values. ## Now, IF either value is equal to &quot;Yes&quot; we want to assign &quot;Yes&quot; to any_heart_disease ## ELSE we will assign &quot;No&quot; ## Which is exactly how an IF-ELSE statement works! if(infarction == &quot;Yes&quot; | angina == &quot;Yes&quot;){ ## The above line can be translated: ## &quot;If infarctiohn equals Yes or angina equals Yes, do the following: ## Now we want to assign &quot;Yes&quot; to any_heart_disease ## However, we are going to just give it a number for now ## It is nice to have 1 = Yes and 0 = No ## So, we will do the following: data$any_heart_disease[row_number] &lt;- 1 ## Don&#39;t forget the brackets with &quot;row_number&quot; ## If you did data$any_heart_disease &lt;- 1, then you would assign that value to every single row!!! ## Next we write our else statement ## The computer will only run the ELSE if the IF statement was false ## In this case it will only run ELSE if neither of the variables equal &quot;Yes&quot; } else{ ## So we want to assign 0 to the final value data$any_heart_disease[row_number] &lt;- 0 } } ## And then the last to do is make it categorical with the factor variable data$any_heart_disease &lt;- factor(data$any_heart_disease, levels = c(0,1), labels = c(&quot;No&quot;,&quot;Yes&quot;)) ## Let&#39;s run the summary function to make sure everything looks good! summary(data) ## SEX AGE ## Male :14806 Min. : 7.00 ## Female :18888 1st Qu.:41.00 ## Refused: 2 Median :57.00 ## Mean :54.49 ## 3rd Qu.:68.00 ## Max. :99.00 ## EDUCA SMOKE100 ## Never attended school or only kindergarten: 37 Yes :14947 ## Grades 1 through 8 : 893 No :16926 ## Grades 9 through 11 : 1945 Don&#39;t Know: 75 ## Grade 12 or GED : 9837 Refused : 26 ## College 1 year to 3 years : 8741 NA&#39;s : 1722 ## College 4 years or more :12243 ## CVDINFR4 CVDCRHD4 any_heart_disease ## Yes: 1942 Yes: 2107 No :30707 ## No :31754 No :31589 Yes: 2989 ## ## ## ## Whenever you create a new variable, it is good to check to make sure everything looks good. I just ran the summary function and I noticed that 1,942 people reported infarction and 2,107 reported angina/coronary heart disease. As such, no more than 1,942 + 2,107 = 4,049 people could possible have had any heart disease, but at least 2,107 must have. Our new variable shows that 2,989 people had heart disease, which is in the range we would expect. This is a good sign that we created our variable correctly. Now we are ready to run our analyses! But, that is for another day! "],["mathematical-notation-probability-distributions.html", "Chapter 5 Mathematical Notation, Probability, &amp; Distributions 5.1 Statistics: Identifying the Signal From the Noise 5.2 Intro to Probability and the Normal Distribution 5.3 Conclusion", " Chapter 5 Mathematical Notation, Probability, &amp; Distributions Before we dive into learning and applying statistical tools, we need to learn about some important concepts. This week, we will discuss how statistics is all about assessing probability. Further, we will formally introduce some mathematical language we can use to discuss probability and we will introduce theoretical distributions and how we can use them to think about probability. In this text, we will specifically focus on introducing and understanding the normal distribution, one of the most important distributions in the field of statistics. 5.1 Statistics: Identifying the Signal From the Noise One of the primary goals in this class is to help get you comfortable with inferential statistics techniques. Inferential statistics are methods where we take data from a sample of \\(n\\) people and then try to learn something about the broader population of people we think they represent. When we talk about an arbitrary number of people, we will often use the the letter \\(n\\) when we dont know what that number actually is (or when the exact amount doesnt matter). A population represents a broad category of people we want to learn about and a sample represents a subset of that population that we recruited for a study. The reason inferential statistics are so important is because usually you cannot survey every person in a population of interest. Perhaps you want to do a study focusing on people who vape in the United States - well, that is quite literally millions of people. Surveying millions of people is impractical or impossible. However, it is more practical to recruit \\(n = 1,000\\) people who vape or \\(n = 10,000\\) people who vape. With inferential statistics methods, we can take information about our sample of \\(n\\) people who vape and try to make conclusions about the broader population of people who vape. This is our main mission when employing inferential statistics techniques - take information from a sample to learn about a corresponding population. These inferential conclusions are essentially probabalistic guesses! These methods allow us to ask, How probable do we think it is that the signal we have observed in our sample is representative of the broader population? Here, signal just refers to any effect or difference that we may observe. For example, in our (hypothetical) study of people who vape, we might find that twice as many men in the sample report using their vape while at work compared to women in the sample. This difference between men and women in the sample represents a signal - the objective of inferential statistical methods is to help us determine how probable we think it is that the signal within the sample represents a meaningful, non-random pattern within the overall population. In the case of our example, how probable do we think it is that men who vape are actually more likley than women who vape to vape while they are at work? One of the hardest parts about trying to identify these signals is wading through the noise in our data. We can think of noise as variability within our data that is simply the result of randomness. Even if there exists a signal in our overall population, when we run a study we intend to recruit a random sample - a sample is considered random when every person in the population has the same probability of being chosen. This randomness introduces variability into the data that may obscure signals we might observe. For example, lets say we want to know how likely it is that if we flip a coin we get heads. Intuitively, we imagine it is 50-50, that 50% of all coin flips should be heads and 50% should be tails. So, we flip a coin once and get heads. We flip it again and getheads?!?! In fact we flip 5 heads in a row before we get a tails! We flip the coin 100 hundred times and we end up flipping heads 72 times. Now, I promise you, the reader, that there is a 50% chance of getting heads (assuming this isnt some trick coin or some quantum thought experiment), however, because every coin flip is independent (i.e., not dependent on any other coin flip) and random, the reality is that we have flipped heads 72% of the time! This represents our signal. Because we have taken a random sample of coin flips, there is natural variation in the results we have observed - this variation is the noise. Noise, natural variation in our sample data as a result of randomness, obscures the signals that can tell us about our population. So, our goal in inferential statistics is to take a sample and try to learn something about the population we believe they represent. In order to do so, we must wade through noisy data in search of meaningful, probable signals. While by no means a rigorous equation, our ability to make conclusions about the population is dependent on the ratio of signal to noise, or: \\(\\frac{signal}{noise}\\). Often we assume that no signal exists at the population-level and then try to assess how probable our observed data is under this assumption. With a fraction, a big numerator (i.e., the top) means the overall number is bigger (i.e., \\(\\frac{4}{3} &gt; \\frac{2}{3}\\)). A bigger denominator (i.e., the bottom) means the overall number is smaller (i.e., \\(\\frac{2}{5} &lt; \\frac{2}{3}\\)). So, we can understand that the bigger (or stronger) our signal in our sample, the less probable we think it is that no signal exists at the population-level (or, the more probable we think it is that a signal does exist). Noise can be understood to obscure signals we may observe in our data (i.e., weaken signals) 5.1.1 Statistics and Falsification So, statistics represents a broad set of methods through which we can take data from a sample, search for signals amidst the random noise, and assess how probable we think it is that the signal we are observing is representative of the broader population! Importantly, statistics isnt about figuring out what is true or false. Statistical methods were developed because we really cannot know what is true  instead we can reflect on what is probable. If something is not probable, we could assume some alternate reality is true, a sort of logic of contradiction. We refer to the act of determining something is improbable as falsification. For example, we could never prove that men vape at work more than women do, but we could observe data and feel confident that it is probable that men and women dont vape the same amount at work - we can seek to falsify that possibility by examing our data. In statistics we do this by assuming a signal does not exist and then asking how probable our observed data is under this assumption - a confirmation by contradiction, of sorts. For example, we could assume that men and women who vape, vape the same amount at work. Then we could collect data and ask, if we assume that men and women vape the same amount at work, how likely is the data we have observed? Lets say we collect data about vaping at work and find that men vape at work 2% more than women - if we assume that men and women, generally, vape the same amount of work, does this signal in our sample seem probable? Sure! - 2% doesnt seem like a very big difference. Maybe we just happened to interview a couple men who vape at work a lot (e.g., noise). Or, what if in our sample, men smoke at work 300% more than women? This observation seems way more unlikely if we assume no true signal and we feel more confident rejecting our assumption that men and women vape the same amount at work! This is the logic we will be employing when we use inferential statistics techniques - we will assume that a signal doesnt exist and then observe data and assess how probable our observed data is under our assumption of no signal. This represents a logic of contradiction. We make an assumption and then ask how likely our observation are under that assumption. If the observations are unlikely, this provides evidence that our assumption may be wrong and then we reject our assumption. Almost every inferential statistical test involves three primary steps: the first is to assume that a signal does not exist in the overall population (e.g., assume that men do not vape at work more than women do); the second is then to measure and quantify the signal and the noise within the sample; and the final step is to ask how probable it is that we could observe the patterns in our data assuming that no such signal exists. To apply this to our coin flipping scenario: first, we assume that there is a 50% chance of flipping heads any flip; then we flip a coin \\(n\\) times and calculate how often we got heads; and, finally, based on the data, we ask how probable it is that we could have observed the sample (i.e., the coin flips) assuming that the chances of flipping a heads was 50-50. So, if we flip a coin 100 times and get heads 72 times, do we still feel confident that there is a 50% chance of getting heads? 5.1.2 Statistics: The Art of Making Educated Guesses Perhaps this was all a long-winded way of saying that statistics is the art of making educated guesses based on the data we have available to us. This is, in fact, a very human activity! We do it all the time! Every day we make probabalistic decisions based on information available to us. A classic example is which way should I drive home to avoid traffic? You usually cant know the best way, but from experience (your sample), you decide the route (usually dependent on the time of day and which routes are available). You choose the route you think has the highest probablity of being fastest. Now, statistics can feel scary because it is often presented as a bunch of mathematical equations and weird distributions and there are lots of Greek letters and tables. I definitely dont whip out a calculator and do some mathematical calculations to decide which way to drive home (even Google Maps cannot predict a car crash before it happens). In this chapter, I want to go over some of this scary math stuff because its all just ways of presenting probability in formal and testable terms. So, as we dive in, I want to assure you that you are familiar with the logic behind probability - understanding how we represent probablity and probabilistic decision-making in statistics will actually make understanding the statistical methods way way way easier. 5.2 Intro to Probability and the Normal Distribution Statistics is all about assessing the probability of our observed data given some assumption. As such, we need ways to formally think through the concepts of probablity. 5.2.1 Defining Probablity Mathematically We need a way to express the following question mathematically: What is the probability that [insert phenomenon] will occur? For example, we might wish to ask, what is the probability that the result of our next coin flip will be heads? If we let \\(A\\) represent our next coin flip, we then want to ask, What is the probability that \\(A = heads\\)? We can use \\(P(x)\\) notation to achieve this statement mathematically. \\(P(x)\\) can simply be translated as The probablity of \\(x\\). So, if we were to write \\(P(A = heads)\\), we would read that as saying that The probability that our next coin flip will be heads is. Intuitively, we know that \\(P(A = heads) = .5 = 50\\%\\). We would read this as saying The probability that our next coin flip is heads equals 50%. We can also chain together multiple phenomena and ask how likely the combination of outcomes is. For example, we could ask \\(P(A = heads\\) \\(OR\\) \\(A = tails)\\). Here we are just asking what the probability is that the coin flip will be either heads or tails - we can see that since those are the only possibilities for a normal coin that \\(P(A = heads\\) \\(OR\\) \\(A = tails) = 1 = 100\\%\\). 5.2.1.1 Conditional Probablity At the heart of inferential statistics is the concept of conditional probablity. Conditional probability comes in handy when we want to ask, Assuming that \\(A\\) is true, what is the probablity of \\(B\\) occurring? Here \\(A\\) and \\(B\\) simply represent phenomenon or circumstances. We can write this mathematically, like so: \\(P(B|A)\\). We would read this as The probability of \\(B\\), given that \\(A\\) is true. Now, \\(A\\) does not actually have to be true, it can be entirely hypothetical. For example, someone could ask you if the freeway is the fastest route to get to your house - let \\(B\\) represent the freeway route to your home. Now, (hypothetically) you know that the freeway is the fastest way except during rush hour. During rush hour, you have found that the freeway is fastest only 1/3 of the time. So, let \\(A\\) represent whether or not it is rush hour. We could ask \\(P(B|A = not\\) \\(rush\\) \\(hour)\\). Well, from experience we have found that the freeway is always fastest when it is not rush hour so, \\(P(B|A = not\\) \\(rush\\) \\(hour) = 1 = 100\\%\\). Likewise we could ask \\(P(B|A = rush\\) \\(hour)\\). From experience, we have found that \\(P(B|A = rush\\) \\(hour) = 1/3 = 33.\\bar{3}\\%\\). Why is conditional probability so important in inferential statistics? In a quantitative study, we will observe some data (i.e., our study sample). We will also assume that a signal does not exist (e.g., men and women who vape, vape the same amount at work). So, we will try to ask \\(P(data|no\\) \\(signal)\\) - or, what is the probablity that we observed our data assuming that no signal exists? That is the foundation of every single inferential statistical method that we will employ, for example: If we want to know if smoking cigarettes leads to lung cancer, we would 1) assume that smoking cigarettes and lung cancer are not related, 2) observe data from a sample (perhaps ask people if they smoked and if they had lung cancer), and 3) then assess how probable our data is given our assumption that smoking cigarettes and lung cancer are not related. The probablity of data seems like a funny concept. Let us say we assume that men and women who vape do so the same amount at work. We recurit a sample and ask how often they vape at work and then we compare responses of men and women. If we assume that men and women vape the same amount at work, then we imagine it is quite probable that men and women report vaping at work at similar rates. However, its almost certain that men and women in our sample wont have identical vaping patterns at work - thus, it is important that we be able to capture probabilties of discrepancies between our observed data and our assumption. For example, if we think that men and women vape the same amount at work, it seems that it would be quite probable that in our sample we find that men, on average, vape at work 0.2 times more per workday than women - perhaps we randomly sampled a couple of men who vape more than others. Whereas, it might be quite improbable, assuming men and women vape the same amount, if we found that men vape 10 times more per workday than women - such an improbable finding might force us to question if our initial assumption was correct. So, we need a way to assess the probability of our data given some underlying assumption. 5.2.1.2 Introducing Theoretical Probability Distributions We do so by employing theoretical probability distributions. A probability distribution is a mathematical function that identifies the probability of a given outcome occurring. While distributions are sometimes a primary point of confusion in statistics, the reality is that distributions are just a way of capturing the way we think about probabilities - they first come from our intuition about the world, the math is just a way of making it rigorous. Essentially, a probability distribution is a tool by which we can make assumptions about the behavior of a given variable. For example, lets imagine we are playing a game where we are guessing the height of the next person to walk in the room - we dont have any information prior to making our guess. Well, our best guess is probably the average height of all peoplelets say we are pretty sure the average person is 5 foot 8 inches tall. We are also pretty sure that there are just as many people shorter than 58\" as there are taller than 58\" (i.e., the distribution of height is symmetrical around the mean). Further, we are quite positive that most peoples heights are around 58\" - we feel confident that most people are between 52\" and 62\". It is quite rare for someone to be shorter than or taller than that range. So, we have constructed a theory of the distribution of height in order to play this game. We think that if someone walks into the room (i.e., a random observation), that they are most likely to be of average height (58) or close to that height (whether taller or shorter). Further, we think that heights far away (way shorter or way taller) than 58 are the least likely to be observed. We can actually capture this probability by plotting it as a mathematical function like so. We will have the x-axis be height (in inches) and the y-axis will represent the hypothetical probability of observing that height if someone walked in the door: ## Let&#39;s create our x-axis, ranging from 4&#39;4&quot; (52 inches) to 7&#39;0&quot; (84 inches) x &lt;- seq(52, 84, by = .1) ## We will define probabilities using the dnorm function ## The dnorm function generates the points that correspond to a normal distribution ## We will set the average height to 5&#39;8&quot; (68 inches) with a standard deviation of 4 inches y &lt;- dnorm(x, mean = 68, sd = 4) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) As we can see, this mathematical function represents our idea of the distribution of height among a random sample (i.e., the people walking in the door). The height of the curve represents how likely we believe it is that that value will be observed. We can see that 68 inches is the most probable height to observe (it is the tallest point of the curve), that heights nearer to 58\" are relatively likely, and that heights further away from 58\" are rarer. Further, we dont have any reason to think there are more short people than tall, or vice versa, so the distribution is symmetrical around the mean. This curve is referred to as the normal distribution and it is the first and most important distribution we will encounter in our statistics work. We will often assume that an outcome follows the normal distribution. So, as we can see, this normal distribution sort of captures our intuition around how we believe height is distributed amongst the population. 5.2.1.3 Assessing Probabilities Using Theoretical Distributions One of the really cool parts of theoretical probability distributions is that we can ask how probable a range of outcomes are, assuming that the distribution is correct. We may wish to know how likely it is that someone 66\" inches or taller walks in the door! Importantly, we can agree that if someone walks through the door, there is a 100% that they will have a height! That might be silly to say, but since the height of the distribution curve represents the probability of observing that given value, that means that if you sum together the height of the curve at every point, that the sum will add up to exactly 1 (which corresponds to 100%). We can depict this using the concept of area under the curve. We can depict the area under the curve like so: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) polygon(x,y,col = &quot;slateblue1&quot;) The sum of the shaded region of the plot in this case is exactly 1. We can actually calculate this using the auc function in the MESS library. ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(x,y) ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 1 Now, this isnt super interesting. Of course there is a 100% chance that a person has a height. But, now we can start asking more interesting questions. If \\(X\\) is the height of the next person to walk in to the room, we could ask \\(P(X &gt;= 5&#39;8&quot; | height\\) \\(is\\) \\(normally\\) \\(distributed)\\) (or what is the probability someone is 58\" or taller, assuming that height is normally distributed around 58). We can use the same principle as above to make this calculation. We start by shading in the area under the curve corresponding to 58 and taller, like so: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(68,x[x&gt;=68]) index_val &lt;- which(x == 68) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) Intuitively, since the normal distribution is symmetrical, it appears that there is a 50% chance that the height of the next person to walk in will be average (58\") or greater. We can check with the auc function: ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.5 Perfect! Now lets do one moreat the beginning of the example, I asked how likely it is that someone 66\" or taller walks through the door next (or \\(P(X &gt;= 6&#39;6&quot;\\))). We can do that like so: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(78,x[x&gt;=78]) index_val &lt;- which(x == 78) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,4) ## [1] 0.0061 As we can see, we think it is quite unlikely that the next person to walk through the door will be 66\" or taller! In fact, rounding to 4 decimals, our distribution is suggesting to us that there is only a 0.61% chance that the next person to walk in will be 66\" or taller. I would say that matches up pretty well with how often I bump into folks that are that tall! 5.2.1.4 Cannot Prove That A Variable Follows a Distribution So, this is one of the tricky parts of statistics. We cannot prove that a variable \\(X\\) truly follows a given distribution. Generally, we assume that it does until evidence is presented to us that we should not. For example, we have displayed above why it makes sense to assume that height is normally distributed. The way we understand how height is distributed throughout the human population matches up with the structure of the normal distribution quite well. However, we cannot truly prove that height is normally distributed - we can only assume that it is. Now, if 100 people walked in the door and half were less than 50\" tall and half were over 70\", I might be a lot less confident that height follows a normal distribution. In other words, we can assume that a variable follows a given distribution and then we can look at data to reflect on this assumption. This is a central activity in undertaking inferential statistics. 5.2.1.5 Formally Defining the Normal Distribution This has been a rather descriptive introduction to the normal distribution. Given how important it is, lets go over how we define it and what its properties are. Remember, our job as statisticians is to identify the signal over the noise. Distributions provide us an ability to rigorously navigate the ratio between the two. There are two parameters which define a normal curve: the mean value (denoted \\(\\mu\\), the Greek letter mew) and the standard deviation (denoted \\(\\sigma\\), the Greek letter sigma). We can think of the mean as our signal and the standard deviation as a measure of how noisy the data is. The larger the standard deviation, the wider our normal curve is (i.e., the more spread out from the mean value we will expect values to be observed). We can visualize this by plotting multiple normal curves with different standard deviations: y1 &lt;- dnorm(x, mean = 68, sd = 1) y3 &lt;- dnorm(x, mean = 68, sd = 3) y6 &lt;- dnorm(x, mean = 68, sd = 6) y10 &lt;- dnorm(x, mean = 68, sd = 10) plot(x,y1,type=&quot;l&quot;,xlab = &quot;X&quot;, ylab = &quot;Density&quot;) lines(x,y3, col = &quot;blue&quot;) lines(x,y6, col = &quot;green&quot;) lines(x,y10, col = &quot;red&quot;) All of these curves are normal distributions with the same mean. They just have different standard deviations. We can understand that a distribution with a larger standard deviation is anticipated to represent noisier data than that with a smaller standard deviation. This is because, the larger the standard deviation, the more likley it is that we will observe values farther and farther away from the mean value. As we discussed in our sample, when we assume a variable is normally distributed, we understand that most of the values we will observe will fall close to the mean value. More extreme observations are understood the be rarer. We can quantify this more specifically - if we assume a variable is normally distributed, then we may understand that just over 2/3 (or around 68.2%) of all observations are expected to fall within 1 standard deviation of the mean. Further, that 95% of all observations will fall within 2 standard deviations of the mean. This can be depicted like so: In our example from before with height, we had defined a normal distribution with a mean height of 58\" and a standard deviation of 4. By definition, we would anticipate then that there is a 68.2% probability that the next person to walk through the door will be within 4 inches (1 standard deviation) of 58 (or, 54\" through 60) and that there is a 95% probability that the next person will be within 8 inches (2 standard deviations) of 58 (or 50\" through 64\"). We can confirm this by looking at the area under the curve again: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(64,x[x&gt;=64 &amp; x&lt;=72],72) index_low &lt;- which(x == 64) index_high &lt;- which(x == 72) poly_y &lt;- c(0,y[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.68 Here, we have filled in the area within 1 standard deviation of the mean and we calculated that this area under the curve sums to 0.68, corresponding to a 68% probability of observing a value within this range! We can do the same for within 2 standard deviations: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(60,x[x&gt;=60 &amp; x&lt;=76],76) index_low &lt;- which(x == 60) index_high &lt;- which(x == 76) poly_y &lt;- c(0,y[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.95 Voila! We can see that 95% of observations fall between 2 standard deviations of the mean! You might be thinking that 95% (or 5%) is an important threshold in the statistics youve read or done before - we will get into that more in the next chapter! 5.2.1.6 The Standard Normal Distribution One of the most important distributions is the standard normal distribution, sometimes also called the \\(z\\)-distribution. It is a normal distribution whose mean value is 0 and whose standard deviation is 1. We can plot it like so: x&lt;- seq(-5,5,by=.1) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;) You can actually take any value from any normal distribution and standardize it so that it maps onto the z-distribution! This is really useful because it allows us to standardize many of our statistical tests. Standardization is simply the process of dividing the observed signal by the observed noise. Standardization was REALLY important before computers came along. I have been running the auc() function to calculate the area under the curve, but before computers this was actually an incredibly complicated task! Standardzing a curve means that instead of dealing with an infinite possible number of curves, we can just think about one. Old statistics books had tables you could use to check the area under the curve given a specific value on the \\(z\\)-distribution, because calculating the area under the curve is too challenging by hand. A value that is standardized to the \\(z\\)-distribution is called a \\(z\\)-score. A \\(z\\)-score is calculated by the following formula: \\(z = \\frac{x - \\mu}{\\sigma}\\), where \\(x\\) is a value observed from a normally distributed variable. In this case, the signal is the difference between the observed value \\(x\\) and the mean value \\(\\mu\\) and the noise is the standard deviation \\(\\sigma\\). For example, let us say that someone walked into the room and their height was 70 inches. Well, we know that the mean height \\(\\mu = 68\\) and that the standard deviation \\(\\sigma = 4\\). Therefore, the \\(z\\)-score is \\(\\frac{70 - 68}{4} = \\frac{2}{4} = 0.5\\). While, most of the time standardization is going to happen behind the scenes, it is important to reflect on how standardization doesnt change the expected results - it is just a way of making the data easier to work with. Our process indicates to us that the value of 70 in a normal distribution with mean \\(\\mu = 68\\) and standard deviation \\(\\sigma = 4\\) is equivalent to \\(z = 0.5\\). This would mean that the probability of someone being 70 inches or taller \\(P(X &gt;= 70|\\mu = 68, \\sigma = 4)\\) should have equivalent value to \\(P(z &gt;= 0.5| \\mu = 0, \\sigma = 1)\\). Lets check real quick. First, we can look at \\(P(X &gt;= 70|\\mu = 68, \\sigma = 4)\\) by shading in the region of the normal curve corresponding to heights greater than or equal to 70 inches and then we can calculate the area under the curve: x &lt;- seq(52, 84, by = .01) y &lt;- dnorm(x, mean = 68, sd = 4) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(70,x[x&gt;=70]) index_val &lt;- which(x == 70) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values round(AUC,3) ## [1] 0.308 We get a value of 0.308. We can do the same process to check the value of \\(P(z &gt;= 0.5| \\mu = 0, \\sigma = 1)\\). We will plot the \\(z\\)-distribution and shade in all values greater than or equal to 0.5 and then take the area under the curve: x &lt;- seq(-5, 5, by = .01) y &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(.5,x[x&gt;=.5]) index_val &lt;- which(x == .5) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values round(AUC,3) ## [1] 0.308 As we can see, we get the same exact value! I know that this process of standardizing values may seem a bit odd to do, but it is the foundation of the first inferential technique we will learn about in a few chapters - the \\(t\\)-test. In fact, when we get there, we will actually see that a \\(t\\)-test is actually just a slightly variation of standardizing a normally distributed variable! 5.2.1.7 Approximating a Normal Distribution From Sample Data We have covered a whole lot of information in this chapter! The last thing I want to discuss is how we approximate a normal distribution from available data. This is important because usually we do not know what the actual mean and standard deviation of a variable are. So, lets say I sat at my desk all day and I counted that \\(n\\) people walked through the door. I wrote down the height of every single person who came in, like so \\(\\{x_1,x_2,x_3,\\cdots,x_n\\}\\), where \\(x_i\\) represents the height of the \\(i\\)th person to walk through the door. In order to approximate the theoretical normal distribution we assume to describe height, we must attempt to estimate the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of the height of each person. It can be helpful to use \\(\\Sigma\\) (sigma) notation to make these calculations. \\(\\Sigma\\) is used in mathematics to say, take the sum of or add together. So, if I were to write: \\[\\sum_{i = 1}^{n}{x_i}\\] , what I am saying is, take the sum of every observation \\(x_i\\). The \\(i = 1\\) is saying that you should start with \\(x_1\\) and then keep adding the values together until you get to the value at the top, which is \\(n\\) and corresponds to \\(x_n\\). The mean value of a sampled variable with \\(n\\) observations is the sum of all the observations divided by the number of observations, \\(n\\). Or, this can be written as: \\[\\bar{x} = \\frac{\\sum_{i = 1}^{n}{x_i}}{n}\\] We denote this mean with \\(\\bar{x}\\) instead of \\(\\mu\\) because we have calculated this mean from the sample, not the entire population. We want to make this distinction because there are many applications where we will compare the sample mean \\(\\bar{x}\\) to an assumed population mean \\(\\mu\\). Next, we need to generate an estimation for the standard deviation \\(\\sigma\\). We will use the symbol \\(s\\) to denote the standard deviation when it is calculated from a sample. Our goal in measuring the standard deviation is to attempt to capture how far, on average, each observation \\(x_i\\) is from the mean value \\(\\bar{x}\\). The further observations are from the mean value, the larger the standard deviation is. Ideally, then we would just take the average of the distance from all the observed values from the mean, but, unfortunately, this actually always equals 0. For example, lets say our values of \\(x_i\\) were \\(\\{3,10,19,2,1\\}\\). Here the mean value is 7 (i.e., \\(\\frac{3 + 10 + 19 + 2 + 1}{5} = \\frac{35}{5} = 7\\)). We can take the distance of each observation from the mean \\(\\{3 -7,10-7,19-7,2-7,1-7\\} = \\{-4,3,12,-5,-6\\}\\) but the sum of all these distances actually equals 0: \\((-4) + 3 + 12 + (-5) + (-6) = 0\\). So, we are forced to do an intermediary step and calculate the variance, which we denote as \\(s^2\\). For the variance, we square the difference between each value and the mean before we sum them. By squaring the value first, we always end up with a positive number. But, we have also squared the values we actually wanted, hence why we consider standard deviation \\(s\\) the square root of variance \\(s^2\\). So, to calculate variance we use the following equation: \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}\\] We can then get the standard deviation because \\(s = \\sqrt{s^2}\\). All we need to do is take the square root of the variance and we have the standard deviation. At this point we have calculated a sample mean \\(\\bar{x}\\) and a standard deviation \\(s\\), which we can consider estimations of both the population-level values \\(\\mu\\) and \\(\\sigma\\). We can use our values of \\(\\bar{x}\\) and \\(s\\) to either generate the normal distribution or to standardize the values of our variable onto the \\(z\\)-distribution! 5.3 Conclusion In this text, we have introduced a new language of probability. We have briefly discussed how statistics is an art of falsification - we assume that something is true, typically by assuming that an outcome follows a specific theoretical distribution and then we try to assess how probable our sample data is given our assumption. If the data we observe seems improbable under our assumption, then we might need to question the validity of our assumption. "],["scientific-research-questions-and-null-hypothesis-significance-testing-framework.html", "Chapter 6 Scientific Research Questions and Null Hypothesis Significance Testing Framework 6.1 Introduction 6.2 Significance 6.3 An Important Distinction: Scientific Versus Statistical Hypotheses 6.4 A Look At History To Understand NHST 6.5 Falsifying the Null Hypothesis: Say Hello to the P-Value! 6.6 But, There Must Be an Alternative 6.7 Putting It All Together-ish: Null Hypothesis Significance Testing 6.8 In Conclusion", " Chapter 6 Scientific Research Questions and Null Hypothesis Significance Testing Framework 6.1 Introduction In the prior chapter, we discussed how statistics is an art of making probablistic guesses about the nature of phenomena that we observe. At its heart, inferential statistics techniques make an assumption that a signal does not exist and then ask how probable this assumption is once we observe some sample data. Before diving into learning these statistics techniques, it will be important that we discuss the null hypothesis significance testing (NHST) framework. While null hypothesis significance testing is not the only way to approach inferential statistics, it is the dominant framework and it is important that you understand how it operates. The NHST framework is not without criticism, including publicly by myself. A big part of this chapter is to introduce null hypothesis testing to you as a necessity, but also to make clear that you should not be basing the quality nor validity of your quantitative research on \\(p\\)-values alone. To approach this, in this chapter we will discuss some of the historical developments that have lead to NHST - specifically the works of Ronald Fisher versus that of Jerzy Neyman and Egon Pearson. Modern NHST is a sort of an amalgamation of the works of these two teams and, unfortunately, it is often applied in ways that have resulted in studies of poor scientific quality. You will not need to know the history of statistics in order be a successful applied statistician, BUT, understanding the principles of Fishers approach versus Neyman-Pearsons will provide you context into what NHST is and what some of its flaws are. 6.2 Significance Many of you (all of you?) are likely to recognize the importance of the word significance. Significance takes on a sacred quality - many of us have been taught to think of the term significant as indicating that your study has found something truly important worth sharing! Results that do not meet this standard are then considered inconsequential and thrown out, never to be looked at again. I really want to challenge you to not care too much about this word. As we will discuss, a result being significant does not inherently mean that it is meaningful, useful, or insightful. By the same token, a result being not significant does not mean the result is not meaningful, useful, or insightful. Part of the goal in this chapter is to understand what significance means and how to think about it in your own studies. 6.3 An Important Distinction: Scientific Versus Statistical Hypotheses Prior to discussing the null hypothesis framework, I first want to draw the distinction between scientific research questions and hypotheses and statistical hypotheses. While inter-related, we must understand the distinction, else we fail to adequately answer our primary research goals when employing statistical techniques. 6.3.1 Scientific Research Questions &amp; Scientific Hypotheses When we undertake a research study, we generally have a research question we are hoping to answer. The purpose of a scientific study is to answer your research question as best as possible. Research questions are presented in plain language, often at the end of the Introduction section of a manuscript. It is common to read a statement in research paper that reads something like: The primary objective of this research study is to address the following research question: among people who smoke cigarettes, is income level associated with likelihood of quitting cigarette use? Often times, a research question is followed by a hypothesis statement. To continue this example, the next sentence may read: We hypothesize that higher income levels will be associated with an elevated likelihood of reporting quitting cigarette use at 6-month follow-up. Assuming we are undertaking a quantitative study, we then take the data available to us and use statistical methods in an effort to best answer our research question. In this case, we need to identify a set of statistical methods that can be applied to answer our research question. The important thing in a research study is to answer our scientific research question and the results of our statistical tests must be understood as being in service to the mission to answer this question. 6.3.2 Statistical Hypotheses: The Null and the Alternate We choose our statistical methods in order to answer our scientific research question. However, each inferential statistical method has its own set of hypotheses. The hypotheses that correspond to a statistical method always follow the same form, regardless of the scientific research question being asked. Further, making a decision about your statistical hypotheses is not the same as making a decision about your scientific hypotheses. Within the NHST framework, an inferential statistical method has one inherent hypothesis (the null) and a second that can also be specified (the alternate). We will discuss the purpose and origins of these hypotheses later on in the chapter: \\(H_0\\): The null hypothesis, which states that any signal observed within a sample is the result of random chance (i.e., the signal does not exist at the population-level) \\(H_A\\): The alternate hypothesis, which states that the observed signal is not the result of random chance and that the signal does exist at the population-level. Within the NHST framework, this is often presented as the negation of the null hypothesis. Remember, when we take a random sample of a population, there is going to be variance (or noise) within the data. Much of the variation in our sample is simply the result of random chance - it wouldnt be weird to see variations in our sample data from what we may expect. So, statistical tests are designed to assess how confident we are in the signal we have observed, despite the noise within the data. The larger the signal and the smaller the noise, the more confident we are that we can reject the null hypothesis. 6.3.3 Answering Our Scientific Research Question with Our Statistical Results Often, when we write a quantitative research report, we string together multiple statistical tests, each with their own set of statistical hypotheses. We take the results of this set of statistical tests and we, as the authors, make an argument about how they answer our scientific research questions and reflect on the plausability of our initial scientific hypotheses. This was a rather long-winded way of saying that if you run a statistical test and find a significant result, your job as a researcher is not over. You must always seek to answer your primary research question and significance alone is not enough to do so. 6.4 A Look At History To Understand NHST Okay! Now that that is out of the way, we need to actually discuss the purpose of the null and alternate hypotheses and how we go about evaluating them. NHST is actually a weird mixture of the methods for statistical inference developed by Ronald Fisher and that developed by Jerzy Neyman and Egon Pearson. In fact, their approaches are not compatible and which approach is better has been a topic of debate for nearly a century now! Unfortunately, hypothesis testing is often taught as a series of steps and little time is spent reflecting on what NHST is and what its flaws are. In the following sections, we will discuss the system of statistical inference developed by Fisher and that by Neyman-Pearson. This will help us understand what NHST is and how to use it effectively. Unfortunately, as a social science researcher, you will often be expected to approach science in the normal and customary ways. For decades, NHST has been misapplied to the point that the normal use of NHST is quite often not scientifically rigorous. This has been argued to have resulted in the replication crisis in the social sciences. So, first we will discuss the system developed by Fisher, which focuses solely on falsification of the null hypothesis. Then we will discuss the system developed by Neyman-Pearson, which focuses on choosing between a main and alternate hypothesis. Finally, we will discuss how NHST has attempted to marry these two approaches and how this has resulted in a mishmash of ideas that are often misunderstood and misapplied. In a sense, we must know how to navigate NHST so that we can engage with and author quantitative research, but, I want to encourage you to be critical of these practices. 6.5 Falsifying the Null Hypothesis: Say Hello to the P-Value! Ronald Fisher (1890 - 1962) was a statistician who formally developed the modern idea of falsifying the null hypothesis. Fisher and other statisticians were faced with a challenging philosophical question: how do you prove that something is true (or, how do you prove your scientific hypothesis is correct)? Well, it turned out that proving something is true, especially about human behavior, is basically impossible. There are too many people and too many sources of variability to ever really establish that some fact is true about large populations of people (nor is it clear that all people are ruled by some unifying set of constructs that explain humanness). Fishers approach was an elegant response to this conundrum. Instead of trying to prove something is true or false, we can actually assume that something is true and then make observations about the world through this assumption. Does what we are observing make sense with our assumption - or, how likely are our observations given our assumption? This is where the idea of the null hypothesis was born. If we want to try to show that some signal exists, Fishers method suggests that we assume that (\\(H_0\\)) a signal does not exist. Then we observe data and ask how likely the observed data if we assume no signal exists, or: \\[P(data|H_0)\\] In other words, what is the probability of observing our sample data, assuming that the null hypothesis is true that no signal exists. If this value approaches 0, we are suggesting that there is a very low probability we could observe our data if the null hypothesis were true. The closer to 0 this value becomes, the more confident we can feel that our assumption of \\(H_0\\) is wrong. In otherwords, the closer to 0 this value becomes, the more confident we can feel that a signal actually exists at the population level. What does probability of observing our sample data mean, though? While we may assume that no signal exists at the population-level (\\(H_0\\)), we know it is quite unlikely that we wont observe any signal at all in our sample data. It turns out that when we assume \\(H_0\\), we are also assuming that our observations will follow specific patterns, otherwise known as theoretical probability distributions. For example, if I want to run a statistical test to compare anxiety between undergraduate students (\\(\\mu_{under}\\)) and PhD students (\\(\\mu_{PhD}\\)), I would start by assuming that \\(H_0: \\mu_{under} = \\mu_{PhD}\\). Now if I recruit 100 undergrads and 100 PhD students and calculate mean anxiety scores \\(\\bar{x}_{under}\\) and \\(\\bar{x}_{PhD}\\), it seems fairly likely that these values will be slightly differentwhat we want to know is how likely is what we observe assuming \\(H_0\\). So, we calculate our signal, in this case we can consider the difference between anxiety scores for the two groups is our signal (i.e., \\(\\bar{x}_{under} - \\bar{x}_{PhD}\\)). If the null hypothesis is true, it is fairly likely this value will be close to 0! In fact, 0 seems like the most likely value, but values close to 0 also seem fairly likely. Values farther from 0 seem much less likely. This sounds a lot like the definition of a normal distribution! Interesting! In this case, by assuming \\(H_0\\), that no signal exists, we are also making an assumption of how our observed variable will behave - in this case that the difference between group anxiety scores should follow a normal distribution. We can then use our signal (i.e., \\(\\bar{x}_{under} - \\bar{x}_{PhD}\\)) and our noise (here, it may be the standard deviation of anxiety observations) to calculate a test statistic. A test statistic \\(t\\) represents a value calculated from the \\(signal\\) and \\(noise\\) that corresponds to a standard probability distribution, \\(T\\). Once we calculate \\(t\\), we can then ask what the probability is of observing a value of \\(t\\) or a greater value, by calculating the corresponding area under the curve from \\(T\\). We can denote this probability as: \\[P(T\\geq t|H_0)\\] In other words, what is the probability of observing a test statistic \\(t\\) from distribution \\(T\\), assuming that \\(H_0\\) is true! In this case, \\(t\\) is a standardized measure of the difference between the two groups mean anxiety scores and \\(T\\) is the standard normal distribution. We call this value the p-value. The p-value represents the probability of observing a signal of a given strength or stronger, assuming that (\\(H_0\\)) no signal actually exists. If the p-value is quite small, then that indicates to us that our assumption is probably wrong and that it is quite likely that a signal exists. Notice, this doesnt prove that \\(H_0\\) is false, it is just a way of reflecting that it is highly improbable. The term significant is then applied to p-values with values less than some pre-established threshold \\(\\alpha\\). \\(\\alpha\\) is typically set to 0.05 in most research practices. Fisher importantly argued that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon. In other words, a study with significant findings should not be viewed as sufficient alone to determine what is factual. 6.5.1 A P-Value is Not Now, if this description has been confusing, thats ok. P-values are notoriously confusing and challenging to define in lay terms. As we learn about more statistical tests and how to apply them, we will learn how the p-value is calculated and how it is to be interpreted. The important thing to takeaway is that the p-value represent the probability of observing your data assuming the null hypothesis is true. The smaller a p-value, the less likely we think our data could have happened assuming the null hypothesis is true - as a result, since we a certain in our data (i.e., since its real and not theoretical/assumed), this forces us to question the validity of assuming the null hypothesis. Unfortunately, it is easy to misinterpret a p-value or misunderstand what it is. Cyril Pernet put together a nice list of things that a p-value is not: A p-value is not a measure of the strength or magnitude of a signal (i.e., a small p-value does not mean the signal is strong or meaningful) A p-value does not reflect the probability of replicating the signal observed (i.e., getting a small p-value doesnt mean replication studies are likely to get the same result) A small p-value is not evidence of any alternate hypothesis, it is only a reflection of the relationship between the data and the null The p-value is not the probability that the null hypothesis is true This last one is quite important because one of the biggest mistakes that researchers make is that they interpret the \\(p\\)-value to represent how likely it is that the null hypothesis is true, or: \\(P(H_0|data)\\). As we have defined, though, the \\(p\\)-value represents the probability of observing our data assuming that the null hypothesis is true, or: \\(P(data|H_0)\\). 6.5.1.1 An Interesting Note About This ow, this is a bit of a downer, because, as researchers we are actually far more interested in \\(P(H_0|data)\\) than we are in the \\(p\\)-value, \\(P(data|H_0)\\). There is a cool formula called Bayes Rule that tells us that \\(P(H_0|data)\\) and \\(P(data|H_0)\\) are proportional to one another. If one value goes up, so does the other and vice versa. As such, we may understand our \\(p\\)-value (\\(P(data|H_0)\\)) being a proxy for the value we actually care about, which is the probability of the hypothesis itself (\\(P(H_0|data)\\)). So, this is quite interesting. As scientists, we want to know if our hypothesis is valid or not. But, the \\(p\\)-value, one of the most prominent metrics in statistics, is about the probability of our data assuming our hypothesis is true. While these ideas are related, it is important to take note that, using Fishers approach we are actually not reflecting directly on the validity of our statistical hypothesis. We have a proxy measure (the \\(p\\)-value) which we understand to be inter-related. This issue is the same for the Neyman-Pearson approach we will discuss later! 6.5.2 Lets Do an Example: A P-Value Primer - Big Babies Lets now run through an example study to get a sense of the general process we undertake when we calculate a p-value. So, we are researching the birth weight of newborn babies in a small town. We had heard some rumors that babies born in this town are all really big! Doctors joke there must be something in the water. So, we want to know, are the babies born in this town actually bigger than normal babies. We happen to know that the weight of newborn babies is normally distributed, that the average weight of a newborn baby is \\(\\mu = 7.5\\) pounds, and that the standard deviation of birth weights is \\(\\sigma = 1.2\\) pounds. We start by setting our null hypothesis. Since the null is the assumption that no signal exists, the null would be that birth weights of babies in this small town are the same as babies generally. If we let \\(\\mu_{ST}\\) be the weight of newborn babies in this small town, then our null hypothesis would be: \\[H_0: \\mu_{ST} = \\mu\\] This is actually the same, mathematically, as saying that: \\[H_0: \\mu_{ST} - \\mu = 0\\] This is quite useful because the difference between the average baby weight in the small town and the average baby weight generally represents our signal. So, lets say we got birth records from a sample of babies in this small town and we calculated that the average birth weight of the sample of babies is \\(\\bar{x}\\). If we assume that \\(H_0\\) is true, then it makes sense that the value of \\(\\bar{x} - \\mu\\) would be normally distributed around the value 0. This is because if we take a random sample of babies from this small town and calculate their average weight \\(\\bar{x}\\) it is quite probable that \\(\\bar{x} \\neq \\mu\\) because there is always going to be random noise in the data, even if we are assuming that \\(\\mu_{ST} = \\mu\\). However, if \\(H_0\\) is actually true, then we can understand that values of \\(\\bar{x} - \\mu\\) close to 0 are more probable than values further from 0. Further, it seems likely that if \\(H_0\\) were true, that \\(\\bar{x}\\) has the same probability of being less than \\(\\mu\\) as being greater than \\(\\mu\\). In other words, it appears that if \\(H_0\\) is true, that \\(\\bar{x} - \\mu\\) is normally distributed around 0! Last week, we discussed the \\(Z\\)-distribution (or the standard normal distribution). Further, we talked about how any normal distribution can be standardized by dividing the values by the standard deviation. We can actually transform our value \\(\\bar{x} - \\mu\\) so that it corresponds to the \\(Z\\)-distribution. We use the following equation to do so: \\[z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] So, now we have this value \\(z\\). This is our test statistic. \\(z\\) has several cool properties: 1) it captures the strength of the signal in the dataset, 2) it captures the noise in the dataset, and 3) it corresponds to a \\(Z\\)-distribution which captures the behavior of the signal assuming that \\(H_0\\) were true. Now, all we need to do is calculate \\(P(Z &gt;=z | H_0)\\). So, lets throw some numbers in. We got the weight of \\(n = 16\\) babies and found that their average weight \\(\\bar{x} = 8.1\\) pounds. So we can calculate \\(z\\) as follows: \\[z = \\frac{8.1 - 7.5}{\\frac{1.2}{\\sqrt{16}}} = \\frac{0.6}{0.3} = 2\\] So, now we can map the value \\(z = 2\\) onto the \\(z\\)-distribution like so: x&lt;- seq(-5,5,by=.1) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;) lines(c(2,2),c(0,1), col = &quot;red&quot;) We are almost there! To calculate the p-value, we need to calculate the probability that our test-statistic has a magnitude of 2 or greater. There are two approaches when using a two-tailed distribution. A two-tailed test or a one-tailed test. In a two-tailed test, we simply care about any observation more extreme than our calculated value \\(z\\). Since \\(z = 2\\), this means any value greater than or equal to 2 or less than or equal to -2. In a normally distributed variable centerred around 0, 2 and -2 have the same probability of occurring. So, for the two tail test, we fill in the tails starting at -2 and 2: x &lt;- seq(-5, 5, by = .01) y &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(2,x[x&gt;=2]) index_val &lt;- which(x == 2) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x&lt;=-2],-2) neg_index &lt;- which(x == -2) neg_y &lt;- c(y[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) Our p-value is the area of the filled in sections of the curve, which we learned how to calculate last chapter, by using the auc function like so: round(MESS::auc(poly_x,poly_y) + MESS::auc(neg_x,neg_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.045 We see that we got a p-value of 0.045. If we were using \\(\\alpha = 0.05\\) then we can see that \\(0.045 &lt; \\alpha\\) and we would describe this finding as significant. In other words, we feel that it is quite improbable that we would observe babies of this average weight if there was no difference between babies in this small town and babies generally. However, we actually were more interested in whether or not babies in this small town were bigger than babies generally. Thus, values of \\(z\\) less than 0 are not of interest because they correspond to values of \\(\\bar{x}\\) where the babies in the small town have a lower birthweight. So, we can run a one-tailed test where we only look at the positive end of the distribution. If we plot that it looks like so: x &lt;- seq(-5, 5, by = .01) y &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(2,x[x&gt;=2]) index_val &lt;- which(x == 2) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) And the p-value can be calculated by the same method, which, since the normal distribution is symmetrical, we would anticipate is half the value of our two-tailed test: round(MESS::auc(poly_x,poly_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.023 And voila! We have done it! We have calculated our p-value. We would read this as saying that, assuming that our null hypothesis was true, there was a 2.3% chance we would have observed a mean baby weight of these 16 babies in this small town of 8.1 pounds or greater. If using an \\(\\alpha\\)-threshold of 0.05, we would describe this finding as significant. Now, our overall research question was about whether or not babies in this small town are, on average, larger than normal babies. Did we effectively answer that with this test? I would say we have found an important part of answering this question, but that our work is not complete. Yes, this provides evidence that it is probable that babies in this town dont have the same average birth weight as babies generally - this is evidence of the claims that babies born in this town are larger than average. If this were a real study though, we would need to also argue that our finding is not influenced by sampling or measurement bias! I raise this point just to indicate that a significant finding alone doesnt answer our research question, it just provides a piece of information we can use to answer our research question. 6.5.3 General Steps to Calculating P-Value So, the general steps to calculating a p-value are as follows: We assume \\(H_0\\) that no signal exists in the population The signal in the sample data is measured and identified This signal is converted into a test statistic which corresponds to a specific theoretical distribution The test statistic is compared to the distribution (area under the curve) in order to calculate the p-value Any time a p-value is calculated, these are the steps that are undertaken! Many of the statistical methods we employ were designed specifically because they allow this process to occurif you are ever learning about a method and you think why on earth are we doing this? the answer is likely that this is how we can convert our sample signal into a test statistic that can be compared to a theoretical distribution. 6.6 But, There Must Be an Alternative Fishers approach focuses on falsifying the null hypothesis via significance testing. The null hypothesis is assumed to be true - the goal is not to try to argue that the null hypothesis is actually true or actually false through Fishers approach. It is the researchers job to reflect on what the findings mean, how to interpret the results in relation to the over-arching scientific research question. Fishers approach takes a cautious approach to scientific discovery - we cant really know what is true and what is false, but we can get a sense for what is probable! In this way, scientific knowledge can be built up over time by replicating studies and by comparing results across settings and samples. It is then the job of researchers to reflect on these results to make conclusions about the nature of various phenomena. However, there are many cases where simply reflecting on the probability of the data observed falls short of intended purposes. This is especially true in industry and corporate settings - perhaps a company wants to figure out if one manufacturing approach is better than another or if their new measurement tool is accurately calibrated. In such circumstances, simply ruminating on and reflecting upon the results of a series of tests is not practical. There are certain cases where a decision has to be made based on the information at hand! Enter two of Fishers contemporaries, Jerzy Neyman and Egon Pearson. Neyman and Pearson developed a competing system for statistical inference, which was informed by involvement in manufacturing (i.e., they were interested in using statistics to figure out if a batch of a product on a production line was abnormal). Instead of simply reflecting on the probability of the data, assuming a null hypothesis, the Neyman-Pearson approach involves deciding between the main hypothesis (that no detectable signal exists) and an alternate hypothesis (that a detectable signal exists). Instead of significance testing, we refer to this process as hypothesis testing. In the Neyman-Pearson approach, two hypotheses are specified: \\(H_M\\): The main hypothesis, that no detectable signal exists \\(H_A\\): The alternate hypothesis, that a detectable signal exists The goal of the Neyman-Pearson approach is to pick one of these two hypotheses. To make this selection, they use the same significance criterion as Fisher. The \\(p\\)-value is calculated and if the \\(p\\)-value is less than \\(\\alpha\\), then the main hypothesis is rejected and the alternative hypothesis is accepted. If the \\(p\\)-value is greater than \\(\\alpha\\) then the main hypothesis is accepted and the alternative hypothesis is rejected. This is different from Fishers approach - while Fishers approach advocates for identifying significant results, Fisher was also a proponent that a conclusion about the null hypothesis cannot be made based on the results of a single study. Choosing between the main and alternate hypothesis introduces a new concern - is the determination correct? If you reject the null, should you have? If you accept the null, should you have? While Fisher treated the p-value as a probability, Neyman-Pearson treat it as a decision rule. Since it is a probability, however, that means that if a decision is made there is a chance that the decision is wrong. The Neyman-Pearson approach has a strong appeal to any person - Fishers approach requires cautious (and often ambiguous) interpretation, whereas the Neyman-Pearson approach has the finality of making a clear decision. Unfortunately, applying this logic to individual research studies has resulted in overly-conclusive scientific reporting across the social sciences. Choosing between the main and the alternate hypothesis also introduces an important concept: effect sizes. If the null hypothesis states that no signal exists and the alternate states that one exists, then the effect size represents the magnitude and the direction (positive or negative) of the signal. However, as we have discussed, in the Fisherian approach, we generally assume that there will be a signal in our sample data - we are just trying to figure out if we think that the observed signal is consistent with the null hypothesis. That means that relatively small observed effect sizes are likely to support the acceptance of the null even if that small effect size is the true value (i.e., the null is false) - not all deviations from the null hypothesis are detectable. 6.6.1 Visualizing the Null and the Alternate, an example So, to better understand why this notion of detectability is important, let us take our babies example and visualize it. Remember, we had compared the mean weight of babies in this small town (rumored to produce giant babies) versus the mean weight of average babies. We measured the signal by subtracting the mean weight of small town babies by the mean weight of average babies and then we standardized the value to a \\(z\\) value which is assumed (under the null hypothesis) to be described by the standard normal distribution (\\(z\\)-distribution). We depict the standard normal distribution below: x&lt;- seq(-5,5,by=.1) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) In the Fisherian approach, we used \\(\\alpha = 0.05\\) as a threshold for determining significance. In the Neyman-Pearson approach, \\(\\alpha\\) is used to define the critical region of a hypothesis test. As we recall from our definition of the normal distribution, 95% of observations are expected to occur within 2 standard deviations of the mean OR that only 5% of observations are expected to occur outside of 2 standard deviations of the mean. For a normally distributed variable (such as \\(z\\)), the critical regions defined by \\(\\alpha = 0.05\\) correspond to the two tails of the distribution outside of two standard deviations of the mean. Technically, for a \\(z\\)-distribution, 95% of expected observations fall between -1.96 and 1.96 (not -2 and 2, as seems intuitive), which we can visualize like so: x&lt;- seq(-5,5,by=.01) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab=&quot;z&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) In the Neyman-Pearson approach, any calculated value of \\(z\\) that falls in the critical regions leads us to reject the null hypothesis and accept the alternate hypothesis. If \\(z\\) does not fall in the critical regions, then Neyman-Pearson indicates that you should accept the main/null hypothesis. What this means that, for a \\(z\\)-test, any value of \\(z\\) which falls between -1.96 and 1.96 is taken as in support of the main hypothesis. As such, the Neyman-Pearson null hypothesis is actually a little bit different than the Fisher null hypothesis. The Fisher null hypothesis would be that \\(H_0: z = 0\\) and we calculate a \\(p\\)-value to determine how likely our data is under that assumption. Whereas, the Neyman-Pearson null hypothesis actually corresponds to \\(H_0: z \\in 0 \\pm 1.96\\). The \\(\\in\\) symbol is just read as saying is in, so we can read that as \\(z\\) is within the range of 0 plus or minus 1.96. In the case of a \\(z\\)-test, 1.96 represents the minimum detectable effect size. For all values of \\(z\\) within this range (-1.96, 1.96) we will accept the main hypothesis. For all values of \\(z\\) outside of this range, we will reject the main hypothesis. We are using the same \\(p\\)-value criterion as Fisher to make our decision, but now instead of reflecting on the probability of the data, we are making a binary decision. What this then means is that the alternate hypothesis under Neyman-Pearson is that \\(H_A: z \\notin 0 \\pm 1.96\\). If we reject the main hypothesis and accept the alternative, what we are saying is that we think that our signal \\(z\\) is actually described by a different distribution, specifically, a normal distribution whose mean is not centered around 0. There are actually an infinite number of potential alternative hypothesis distributions that could be observed (i.e., because there are an infinite number of continuous values which could be the mean). Here, we visualize some of the possible alternative distributions. x&lt;- seq(-5,5,by=.01) y&lt;- dnorm(x,mean = 0, sd = 1) y_neg1&lt;- dnorm(x,mean = -1, sd = 1) y_neg2&lt;- dnorm(x,mean = -2, sd = 1) y1&lt;- dnorm(x,mean = 1, sd = 1) y2&lt;- dnorm(x,mean = 2, sd = 1) y3&lt;- dnorm(x,mean = 3, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) #lines(x, y2, col = &quot;green&quot;) lines(x, y3, col = &quot;slateblue1&quot;) This might be a bit overwhelming to look at, but thats because there is literally an infinite number of potential alternative distributions. Lets look at the green distribution only real quick: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) The green distribution represents an alternative to the null distribution. The green distribution indicates that the true standardized difference between the small town babies and the average baby is \\(z = 1\\) (or \\(H_A: z \\in 1 \\pm 1.96\\). Just like with the standard distribution, we assume that if we calculated \\(z\\) for a sample, that the probable values of \\(z\\) would be normally distributed around 1. Here is the thing though, the probable values of this alternative overlap with a lot of the probable values of our null distribution. If the true alternate is an effect size of \\(z = 1\\), then we simply dont have a lot of power to actually identify it. Power refers to our ability to reject the null/main hypothesis when it should be rejected. In the case above, we will only reject the null hypothesis if \\(z\\) falls outside the -1.96 through 1.96 range. Which means that if the true signal is \\(z = 1\\), then we have little power to identify it. We can actually calculate the probability of correctly rejecting the null hypothesis when the alternate hypothesis is \\(H_A: z \\in 1 \\pm 1.96\\) by taking the area of the alternate curve within the critical regions, like so: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y1[index_val:length(y1)]) ## we need the negative values too #neg_x &lt;- c(x[x &lt;= -1.96],-1.96) #neg_index &lt;- which(x == -1.96) #neg_y &lt;- c(y1[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;green&quot;) #polygon(neg_x,neg_y,col = &quot;green&quot;) round(MESS::auc(poly_x,poly_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.168 What this tells us is that, if the real effect is \\(z = 1\\), that we will only correctly reject the null 16.8% of the time - or that our power = 0.168. That is really not good at all! It can be useful to plot this a different way, like so: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## beta x beta_x &lt;- c(-1.96, x[x &gt;= -1.96 &amp; x &lt;= 1.96], 1.96) beta_y &lt;- c(0,y1[neg_index:index_val],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) polygon(beta_x,beta_y,col = &quot;green&quot;) This shaded green region represents the values of \\(z\\), assuming this is real distribution of the signal, where we wrongly accept the null/main hypothesis. We refer to this region as \\(\\beta\\). Since the area under a curve equals 1, we can see that \\(\\beta = 1 - power\\). In this case, \\(\\beta = 1 - .168 = .832\\). Now, since there are infinitely many possible alternative hypotheses, Neyman and Pearson needed a way to establish what a detectable alternative hypothesis is. One way is to make \\(\\alpha\\) less restrictive. So, instead of \\(\\alpha = 0.05\\), we could set \\(\\alpha = 0.1\\). This would result in the following adjustment to our previous plot: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.65,x[x&gt;=1.65]) index_val &lt;- which(x == 1) + 65 poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.65],-1.65) neg_index &lt;- which(x == -1.65) neg_y &lt;- c(y[1:neg_index],0) ## beta x beta_x &lt;- c(-1.65, x[x &gt;= -1.65 &amp; x &lt;= 1.65], 1.65) beta_y &lt;- c(0,y1[neg_index:(index_val-1)],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) polygon(beta_x,beta_y,col = &quot;green&quot;) As we can see, adjusting \\(\\alpha\\) caused the region of \\(\\beta\\) to change as well. That is because \\(\\alpha\\) and \\(\\beta\\) are intertwined - if you increase \\(\\alpha\\), then \\(\\beta\\) will decrease and vice versa. BUT, this approach is not ideal. If you increase \\(\\alpha\\) then you are essentially making it more probable you will reject your null hypothesis. We set \\(\\alpha\\) to a low value of 0.05 so that we can be confident in our rejection. The higher \\(\\alpha\\) becomes, the riskier our rejections of the null hypothesis become (i.e., our chances of wrongly rejecting the null are now higher). So, Neyman-Pearson came up with a different idea. The issue is that their approach is not ideal for identifying relatively small signals. But, if we know the size of the signal we are looking for, then we can design our study to have enough power to identify that signal. So, Neyman-Pearson suggest defining an explicit alternate hypothesis in which the power to detect it is at least 80%, which is the equivalent to \\(\\beta = 0.2\\). In the case, of our example, this would mean shifting the alternate hypothesis further to the right so that the shaded green region has an area of 0.2. This would correspond with a \\(z\\)-value of approximately 2.2, which we can depict like so: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) y1 &lt;- dnorm(x, mean = 2.8, sd = 1) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## beta x beta_x &lt;- c(-1.96, x[x &gt;= -1.96 &amp; x &lt;= 1.96], 1.96) beta_y &lt;- c(0,y1[neg_index:index_val],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) polygon(beta_x,beta_y,col = &quot;green&quot;) round(MESS::auc(beta_x, beta_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.2 So, in this case Neyman and Pearson would describe \\(z = 2.8\\) as the expected effect size. The logic is that if the true effect size (i.e., the alternate hypothesis) is that \\(z = 2.8\\), using their approach, they will correctly reject the null/main and accept the alternate hypothesis 80% of the time! If the true value of \\(z\\) is greater than 2.8, then this power will increase. What this means is, if the true signal you are trying to observe is closer to 0 than 2.8, you will have limited power to reject the null. For practical purposes, this would indicate that there isnt strong justification to use the test because the goal is to correctly pick the null/main or the alternate. If you cant successfully pick the alternate, then there is no point. We discussed before that you can improve power by increasing \\(\\alpha\\). This is not ideal because it makes our rejections less conservative. We only want to reject the null hypothesis if we feel really confident we should. The other way to improve power is to sample more people for the study. We can see that if we look at the formula for calculating \\(z\\) that it is built right in - the greater our sample size \\(n\\), the greater the magnitude of \\(z\\), and thus the greater power we have: \\[z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] \\(n\\) represents the number of people in our study. As \\(n\\) gets bigger, so does \\(z\\)! This may be confusing because we have a fraction nested inside a fraction. So, if \\(n\\) is bigger than the value of \\(\\frac{\\sigma}{\\sqrt{n}}\\) will get smaller. The smaller a denominator, the larger the value becomes. So if the denominator of the whole fraction gets smaller, than the magnitude of \\(z\\) will go up! As we sample more people, the value of \\(z\\) will shift further and further away from 0! That means that \\(\\beta\\) will decrease and, by definition, the power of our study will increase. Power analyses are used to determine what size of \\(n\\) needs to be recruited so that we can detect a given expected effect size with a power of at least 0.8. The power analysis for a given method is dependent on how the test statistic is calculated. In the case of \\(z\\), we want to know how many people we need so that \\(z = 2.8\\), which we established above is associated with a power of 0.8 for a \\(z\\)-test. If we expect that babies in the small town have a mean weight \\(\\bar{x} = 8.1\\) pounds (our expected effect size is that babies in this small town are 0.6 pounds heavier at birth then the average baby, i.e., 8.1 - 7.5 = 0.6), recalling that the standard deviation of birth weight was \\(\\sigma = 1.2\\) and we want to calculate \\(n\\) for \\(z = 2.8\\), then we can do the following: \\[z = 2.8 = \\frac{8.1 - 7.5}{\\frac{1.2}{\\sqrt{n}}}\\] We can scramble the values around to find that: \\[\\sqrt{n} = \\frac{1.2*2.8}{8.1 - 7.5} = \\frac{2.64}{0.6} = 5.6\\] Since this is the square root, we square 4.4 to get \\(4.4^2 = 31.36\\). We always round up in power analyses because we cannot recruit 31.36 persons - this tells us if we expect that babies in this small town have a birth weight of 8.1 pounds, we need to recruit at least 32 babies to have adequate power to identify that this is true! 6.6.2 The Neyman-Pearson Decision Matrix What is immediately clear is that Neyman-Pearsons emphasis on making a decision has introduced a different type of complication to our idea of statistical inference. Fisher was not concerned with deciding if the null hypothesis was true or false - these statistical tests provide information on the probability of observations and it is the researchers job to contextualize the findings within the broader discourse of the research question. There is not a risk of being wrong in Fishers approach. In Neyman-Pearson, you must decide between the null and an alternative hypothesis. Even though we use expected effect size to make sure our study has enough power, the alternate hypothesis in Neyman-Pearson is typically just the opposite of the null. The Neyman-Pearson decision process is often depicted as a decision matrix, like so: Ideally, we correctly choose between the main/null and the alternative hypotheses. This is always the goal of this approach. Two types of errors can occur: a Type 1 Error, in which the null hypothesis is rejected when it should have been accepted; and a Type 2 Error, in which the null hypothesis is accepted when it should have been rejected. 6.6.2.1 Type 1 Error, Alpha, and Long-Term Success We can understand that the probability of making a Type 1 Error in the long run when the null is true is \\(\\alpha\\). As displayed above, if our \\(p\\)-value is less than 0.05, we reject the null - even in cases when the null should not be rejected. Our \\(p\\)-value, as defined by Fisher, is the probability of observing our data assuming the null is true. Our critical regions which are defined by \\(\\alpha\\) represent observed signal which, in total, have a 5% chance of occurring assuming the null hypothesis is true. That means, if we run 100 experiments with a true null hypothesis, we would expect to make a Type 1 Error 5 out of 100 (5%) times. This concept is often misunderstood within the social sciences. Neyman and Pearson were concerned with identifying abnormalities in production. If a factory makes thousands of products, then their tests can be run over and over and over again. In other words, Neyman and Pearson were working in an environment where there was constant experimental replication. They were concerned with success in the long run. They wanted to make sure if they ran their test 1,000 times, that they maximized the number of times that they were correct. However, in the endeavor to build scientific knowledge, we may be lucky to have the opportunity to replicate a study even one time, especially outside the context of randomized control trials. And, often, the replications undertaken are not true replications - they may have different sampling and measurement strategies that make them different. As a result, sometimes researchers use Neyman-Pearsons idea of a \\(p\\)-value and \\(\\alpha\\) to measure how likely it is that the null hypothesis is falsethis unfortunately is not an appropriate way to interpret the \\(p\\)-value. When we explicitly accept the null hypothesis under Neyman-Pearson, we are thinking about our long-term success running and re-running (and re-running and re-running) the same experiment. The misuse of the Neyman-Pearson decision matrix is often a result of researchers proclaiming that a single study can determine if the null or the alternate hypothesis is correct. 6.6.2.2 Type 2 Error, Beta, and Long-Term Success Likewise, the probability of making a Type 2 Error in the long run when the null is false is \\(\\beta\\). If our \\(p\\)-value is greater than \\(\\alpha\\), then we accept the null - even if we should reject the null. While the true value of \\(\\beta\\) is dependent on the true (and unknown) signal, we calculate \\(\\beta\\) such that we can at least identify an expected effect size correctly 80% of the time or more. The idea here is that, if the null hypothesis is false and the alternative hypothesis is true and the true signal is at least the magnitude of the expected effect size, then if we run 100 experiments, we should correctly reject the null and accept the alternative 80% of the time (or more). 6.6.2.3 Thinking Long-Term Under Neyman-Pearsons framework, it appears that one of the best ways to identify if the null or the alternate hypothesis is true, is to replicate our study design many, many times. If the null is true, we would expect to reject the null only 5% of the time. If the null is false, we would expect to reject the null at least 80% of the time. If you only run one iteration of an experiment, you simply do not have enough information to identify if the null is true or not. Imagine flipping a coin one time, getting a heads, and declaring there is a 100% chance of flipping heads - that would be a poor conclusion! Unfortunately, many researchers will use the Neyman-Pearson decision approach for one study and then use that to declare that the null hypothesis is false. Repeating the study design and observing the decision multiple times is the best way to truly capture which hypothesis should be accepted. 6.7 Putting It All Together-ish: Null Hypothesis Significance Testing Today, null hypothesis significance testing (NHST) is the primary framework used for statistical inference. NHST is often described as a mixture of the Fisher and Neyman-Pearson approaches, though it is not formally defined. To add to the potential confusion, for many statistical methods, applying Fisher and Neyman-Pearson lead to identical presentation of results. It is hard to explicitly define NHST because it isnt a well-defined framework. It has arisen over several generations of statistical practice, as researchers have applied (and mis-applied) the approaches of Fisher and Neyman-Pearson. Generally, on the surface, NHST looks very similar to Neyman-Pearson. A null and alternate hypothesis is specified. A statistical test is run. If the corresponding \\(p\\)-value is less than \\(\\alpha\\) then the researcher declares their results significant and rejects the null hypothesis. Otherwise, if the \\(p\\)-value is greater than \\(\\alpha\\), the result is deemed non-significant and the researcher opts to accept (or fail to reject) the null hypothesis. Often, non-signfiicance is misinterpreted as meaning that the result of a test has no value. As well, given concerns that a single study cannot be used as proof that the null is true, some researchers choose to fail to reject the null instead of accepting it based on non-significant results. However, in practice, most researchers tend to treat acceptance and failure to reject identically when it comes to answering their overall research question (i.e., they still treat the result as having no value in answering their scientific research question). As such, NHST has become a rather mechanical exercise. Data is fed into a statistical test. The result is either significant or not. If the result is significant then the null can be rejected. It is very common for researchers to then answer their over-arching scientific research question by asserting that they found a significant result. For example, lets say a study asked, Is there a relationship between childhood drug use and educational attainment? It would be fairly normal, under NHST practices, to read the following conclusion, We found a significant relationship between childhood drug use and educational attainment, without any reflection about the nature of this relationship. Significance, derived from the \\(p\\)-value, is a reflection of how likely our observed data is assuming the null hypothesis is true. Significance does not reflect on the likelihood of the null hypothesis nor does it reflect on the likelihood nor description of some alternative hypothesis. Therefore, it is not good statistical practice to use significance, alone, as an assertion that the null hypothesis is false or that some alternate hypothesis is true! 6.7.1 So, NHST Isnt a Good Idea? I realize I have introduced NHST in a pretty negative light. We have discussed the Fisher and Neyman-Pearson approaches because they both represent important frameworks for statistical inference. However, they are distinct and should be applied when it is appropriate to do so. The Fisherian approach is well-suited for research that is unlikely to be repeated (as is the case with most human subjects social science research). This is because Fishers approach does not attempt to decide whether the null hypothesis is true or false (as doing so with a single study is not well-founded), but instead focuses on how probable the null is given the observed data. Under Fishers approach, it is the researchers responsibility to take the results of statistical tests and their broad subject expertise to then answer their research question. Neyman-Pearson were more interested in decision-making in contexts where the same statistical test could be executed over and over again, such as in manufacturing (where every product produced may be subject to some test). Their notion of acceptance and rejection of the null hypothesis is based on long-term probabilities. If they run a test 100 times, their goal is maximize the number of times the test is correct. This often implies that there exists a need to decide if something is true or false. In much of scientific inquiry, there is no imperative to decide if some hypothesis is true or not - in fact, it is generally considered unwise to declare that anything is true, as there is always more data that may lead us to rethink our decision. In many applied settings, making a decision is important: should a patient receive treatment A or treatment B; is a batch of our product normal or abnormal? Neyman-Pearsons approach is more appropriate in settings where a decision must be made, whereas Fishers approach is often better suited for open-ended scientific inquiry. I bring this up to say that it is your responsibility as a researcher to decide the best approach. Often, the statistical tests used under Fisher and Neyman-Pearson are the same. The primary difference is in the presentation of the results. Under Fisher, it is your responsibility to reflect on the probability of the data (the \\(p\\)-value) in your effort to answer your scientific research question. There is no impetus to accept or reject the null hypothesis, the \\(p\\)-value and other results of statistical tests simply represent evidence that can be used in litigating your research question. Under Neyman-Pearson, significance is used to make a decision based on the data observed. This is often not congruent with the goals of scientific inquiry. For example, under Neyman-Pearson we would treat a \\(p\\)-value of 0.049 as significant but a \\(p\\)-value of 0.051 as not significant even though we understand that they indicate almost the same exact probability of the sample data assuming the null is true! However, Neyman-Pearson also importantly introduced the concepts of power and effect size, which provides a way to reflect on what the alternative to the null may actually be! 6.7.2 Misuse of NHST As I have alluded to, NHST is often poorly used. This is not intended to say that scientists are doing a bad job - instead, it is more a reflection of what kind of work is rewarded and how that has resulted in the proliferation of poor practices. The misapplication of NHST often is when researchers use NHST through the Neyman-Pearson framework when it is more appropriate to use Fishers. This occurs when researchers use a single study to declare a null hypothesis is false, when Neyman-Pearson fully intended their decision-making strategy to apply to long-run replications of the same test. This approach is far easier for both researchers and reviewers - the word significant becomes a signal that the research is important. As a result, researchers choose statistical tests with the goal of acheiving significance and reviewers and journals reward papers that find significance. Neyman-Pearsons strategy is intuitive and conclusive - unfortunately, social science research rarely warrants conclusive declarations. As a result, NHST is often taught as a hunt for significance. The Neyman-Pearson decision matrix is often taught, but the Fisherian approach is ignored. There is widespread misunderstanding of what a \\(p\\)-value is and, as a result, significant is often used as a word to justify the conclusion that the study has confirmed the authors scientific hypotheses. This is reinforced by the literature, where papers that approach significance testing in this way are rewarded and published. This indicates to other researchers and prospective researchers that this is best way to approach statistical inference. As such, we have discussed both Fisherian and Neyman-Pearson approaches. We have done so so that we are empowered to apply the correct one - often, a mixture of the two is warranted, but we need to be clear about why we are interpreting our results the way that we are. The goal is always to answer our scientific research questions - the results of our statistical tests represent pieces of evidence in the effort to answer our scientific research question. 6.7.3 What Should I Do? Which framework you apply will be dependent on the nature of your research question and your data. For every study, you will need to ask what the most appropriate framework to use is. Often, a mixture of both frameworks is warranted, but it is important to explicitly understand which parts of each are appropriate. If your research is focused on a decision-making process, then Neyman-Pearson should be applied in full. If your research is focused on building scientific knowledge, then the Fisherian approach makes sense. Since this class is intended for human subjects social science researchers (such as epidemiologists), I will say that it is unlikely that Neyman-Pearsons framework should be used alone (or at all, honestly). I generally recommend applying Fishers approach for interpreting \\(p\\)-values in relation to the null hypothesis, while also incorporating Neyman-Pearsons idea of measuring effect sizes. Lets remember, the goal of our research endeavors is to answer our scientific research question. We are using the results of our statistical analyses as evidence for the answer that we land on at the end of the study - the results of our statistical analyses do not inherently answer our research question by themselves. We must do some translation. First, in studies aiming to build upon scientific knowledge, a single study should never be viewed as conclusive evidence of whether something is true or false. Fishers approach is well-suited for navigating the uncertainty of making conclusions, whereas the Neyman-Pearson approach often results in hard conclusions that are not fully supported by the data. In other words, most scientific research questions are not adequately answered by a yes or a no, by a true or by a false - using methods that make a yes/no decision do not seem warranted. Second, while Fisher was not concered with an alternate hypothesis, the alternate hypothesis is usually what is most interesting to us! If we want to know the relationship between two variables, the null hypothesis that there is no relationship is usually not interesting to us at all! So, it is important that we have a way to capture the potential alternative hypothesis. The best way to do this is by capturing the effect size of the relationship between two variables, as Neyman-Pearson do. An effect size is the magnitude and direction of a relationship between two variables within the sample - this effect size can be understood to be a best estimate of the population-level effect size if the null hypothesis is false. Just about every test, regardless of approaching it from the Fisher or Neyman-Pearson framework, measures effect size. Fisher is just concerned with whether or not the observed signal (effect size) is probable under the null hypothesis, whereas Neyman-Pearson were interested in asking if the measured signal is evidence of an alternate hypothesis. As such, Neyman-Pearson also introduced the concept of confidence intervals, which represent a range of probable values that this actual population-level effect may fall within, which usually is centered around the measured effect size. Effect sizes and confidence intervals represent a very useful set of metrics for providing an answer to our scientific research question. Now, these are my suggestions to you. You will likely encounter scientists who would not agree. You will likely encounter scientists who are not familiar with Fishers approach or Neyman-Pearsons. You may submit a paper and have reviewers confused why you have approached statistical inference from the Fisherian approach when they are accustomed to the Neyman-Pearson. The important thing is that, no matter what, you feel comfortable justifying the decisions you have made. 6.7.4 Wading Through The Meta-Uncertainty of Statistics In this chapter we have introduced Fisherian, Neyman-Pearson, and NHST approaches to hypothesis testing. Further, we have reflected on how the NHST approach is limited and I have suggested that we apply Fishers approach with some Neyman-Pearson concepts (effect size and confidence intervals) thrown in. Throughout this text, we will approach statistical methods through such a lens. I want to note that a challenge in your research careers will be navigating working with colleagues and reviewers who are not familiar with the shortcomings of NHST. NHST is appealling because it can be taught in a very formulaic fashion - you plug numbers in, run the test, get the \\(p\\)-value, and then make your decision. This is not meant as a criticism of any researcher, this is a result of statistics education and the widespread adoption of NHST within social science research. So, what this means is that you will likely have many instances when your understanding of NHST and statistics differs from your peers. You will need to get comfortable navigating such discussions - a big part of this will be in defending the choices that you think are best. It is likely that there will be cases where editors and reviewers ask you to edit your paper to be more in line with NHST, even if it doesnt make sense. Undertaking research is often a negotiation - being comfortable understanding NHST and the Fisherian and Neyman-Pearson approaches will help you defend your choices and argue for appropriate statistical analyses. 6.8 In Conclusion The goal of any study is to answer your scientific research question. Two frameworks - the Fisherian and the Neyman-Pearson - have been developed to guide frequentist statistical practices. For the social scientist, the Fisherian approach combined with Neyman-Pearson concepts of effect size and confidence intervals represent an important strategy for scientific inquiry. The Neyman-Pearson decision matrix is generally warranted where replication is undertaken and where the primary goal is to inform decision-making. The misapplication of Neyman-Pearsons decision approach has lead to many poor practices within the social sciences. By understanding both Fisher and Neyman-Pearson approaches, we can choose which option is best based on the goals and design of our study! "],["descriptive-statistics-table-one.html", "Chapter 7 Descriptive Statistics: Table One 7.1 Sample Versus Population 7.2 Inclusion Criteria 7.3 The Sample Does Not Always Match the Population 7.4 Introducing: Table One! 7.5 Rewind a Bit: What Exactly are Descriptive Statistics 7.6 Primary Types of Descriptives Seen in Table One 7.7 Creating Table One 7.8 In Conclusion", " Chapter 7 Descriptive Statistics: Table One One of the most powerful parts of quantitative research methods is that we can take data from a sample (i.e., those recruited into a study) and we can attempt to make inferences about the broader population. This is the heart of what we refer to as inferential statistics. While much of this has come up in prior chapters, lets go over a few quick terms. 7.1 Sample Versus Population Our study sample is the set of people who are recruited into our study. See, we cannot recruit every person out there. Lets say we want to do a study of adults in the USthat is hundreds of millions of people. We cannot include everyone in our study. We can even think smaller, say we want to ask questions about people who use cocaine in the USthat is still millions of people! Generally, when a sample is recruited for a study, the intention is to capture a specific population. A population is a set of people who we would like to ask a research question about, but that (in all likelihood) we cannot recruit in its entirety. For example, lets say we are interested in whether or not income level is associated with increased readiness to quit smoking cigarettes among older adults (45+) who smoke in the United States. Our population in this study is adults, 45 years of age and older, who smoke cigarettes, living in the United States. 7.2 Inclusion Criteria Our goal is then to recruit a study sample that is representative of this broader population. Studies do this by defining a set of inclusion criteria which drives overall recruitment. In our example, we have several clear inclusion criteria: Must be 45 years of age or older Must live in the United States Must currently smoke cigarettes When we are writing a research paper, it is always important to discuss what the inclusion criteria were for a study, because that tells us the studys population of interest. 7.3 The Sample Does Not Always Match the Population Heres the thing - just because a study recruited a sample with the intention of being representative of a population, that does not mean the recruitment goes perfectly (hint: it never does). Lets say we get the data for our example study, we have recruited 1,000 people who smoke cigarettes aged 45 and older who live in the US. It turns out, though, that 950 of the people recruited are non-Hispanic White.1 As we can see, while we did recruit a sample that met our inclusion criteria, it turns out that we have over-sampled non-Hispanic White people (i.e., non-Hispanic White people do not represent 95% of all people who smoke aged 45+ in the US). This doesnt mean our data is invalid, but it forces us to re-evaluate what population our study sample actually represents. This will change how we discuss the conclusions of our paper - especially if we believe that race plays an important role in the process under study. Now, usually by the time we receive the data to do analyses, the recruitment is done and we simply have to make do with what data we have. While the inclusion criteria can tell us what the intended study population was, it is our job as statisticians to determine what the actual study population is, based on the characteristics of the study sample. This is where descriptive statistics come into play. As the name implies, descriptive statistics represent a set of techniques we can use to describe our study sample. The most important statistical task to take at the beginning of any human subjects study is to describe who your sample is! If we fail to do this, then we have no idea what population our results might actually pertain to. Describing our study sample well is one of the most crucial tasks we have in undertaking applications of inferential statistics. Soummhow do we go about doing that then? 7.4 Introducing: Table One! In almost every human subjects study, the first table is the table which describes your study sample. It is where you see statistics such as the average age of participants, the gender breakdown, educational attainment, among other variables of interest in your study. While there is no rule that this has to be the first table, Table One is a necessity in every single human subjects research paper. You can see in the photo below a picture of Table One from one of my own studiesthis is a type of table we see frequently: In this table, we actually see that the descriptives are stratified by city and by homelessness status. It is fairly common that Table One is stratified by a key variable (e.g., treatment versus control, exposure status, etc). As we learn how to make Table One, we will also discuss how to choose when to stratify our table and how to do so easily in R! 7.5 Rewind a Bit: What Exactly are Descriptive Statistics So, before we talk about putting together Table One, we need to discuss what descriptive statistics are and what tools we have to extract them. At its simplest, descriptive statistics represent a wide range of tools that allow us to numerically reflect on who our sample is. Generally, when the term descriptives (often short-hand for descriptive statistics) is used, what we actually mean is a univariate summary of the variables of interest in our study, which we will discuss in detail below. However, the line between descriptive and inferential statistics is often a bit more blurry - tests of association between multiple variables and more complicated tests (such as regression) also contain descriptive information about the sample, though we typically do not refer to these as descriptive because they also provide inferential information. 7.6 Primary Types of Descriptives Seen in Table One Table One generally presents univariate summaries of each variable of interest. Often, we are taught about 4 primary types of variables: nominal, ordinal, discrete, and continuous. Nominal variables are categorical and where the different categories do not have a rank-ordering (e.g., gender, which school you attended, your name, etc). Ordinal variables are categorical and where the different categories have a natural ordering (e.g., letter grades on a test, sizes of drinks at Starbucks). Discrete variables are numeric and can be represented by a whole number - these generally measure things that cannot be sub-divided. For example, if someone asks you how many pets you have, the answer must be a whole valuesomeone cant have 2.23 pets. Continuous variables are numeric and can generally take on any fractional or decimal value. The line between these can sometimes be blurry. For example, we could ask someone how old they are and use it as a discrete variable (I am 29 years old) or we can use it as a continuous variable (I am exactly 29.7234 years old). For generating Table One, we can simply start by just distinguishing between categorical variables and numeric variables. 7.6.1 Descriptive Statistics for Categorical Data For a categorical variable (whether ordinal or nominal), we generally calculate the frequency and proportion of the different levels of that variable. For example, lets say we have a variable where we asked 20 participants which color of the rainbow was their favorite and the responses were like so: ## Here are the responses from the twenty participants ## I have written them out like so, just so it is easy to confirm our results fave_colors &lt;- c(&quot;Red&quot;,&quot;Red&quot;, &quot;Orange&quot;, &quot;Yellow&quot;,&quot;Yellow&quot;,&quot;Yellow&quot;, &quot;Green&quot;,&quot;Green&quot;,&quot;Green&quot;,&quot;Green&quot;, &quot;Blue&quot;, &quot;Indigo&quot;,&quot;Indigo&quot;,&quot;Indigo&quot;,&quot;Indigo&quot;,&quot;Indigo&quot;, &quot;Purple&quot;,&quot;Purple&quot;,&quot;Purple&quot;,&quot;Purple&quot;) ## R won&#39;t recognize this vector as a categorical variable just yet ## We can use the factor() function to tell it is categorical fave_colors &lt;- factor(fave_colors) 7.6.1.1 Frequency The frequency is the number of people who responded with each of the different levels. There are many ways to extract this information in R, but a common one is to use the summary() function summary(fave_colors) ## Blue Green Indigo Orange Purple Red Yellow ## 1 4 5 1 4 2 3 Here we can see the frequency of each level of our variable! 7.6.1.2 Proportion Of interest as well is the proportion each category was elected  this is simply the percentage of respondents who cose that categorical level. For example, we see that 5 people listed Indigo as their favorite color. Since there were 20 total participants, we can calculate \\(5/20 = 0.25 = 25\\%\\). But that is pretty painstaking. There are many ways to calculate this in R. Here is one such way: ## The proportion for each level equals the (number of responses for that level)/(total number of responses) ## The cool thing is that &quot;summary(fave_colors)&quot; contains the number of responses for each level ## So we can actually just divide &quot;summary(fave_colors)&quot; by the total number of participants ## We can get the total number of participants with the &quot;length()&quot; function, like so: summary(fave_colors)/length(fave_colors) ## Blue Green Indigo Orange Purple Red Yellow ## 0.05 0.20 0.25 0.05 0.20 0.10 0.15 ## The length function can be used on a vector and just returns the number of elements in the vector ## Now this gave us the decimal form of the proportion. You can translate each value into a percentage by multipling the whole thing by 100 100*summary(fave_colors)/length(fave_colors) ## Blue Green Indigo Orange Purple Red Yellow ## 5 20 25 5 20 10 15 Here we can see that 5% responded with Blue, 20% Green, et cetera et cetera. Now, somebody might think its funny if you put this in Table One starting with the color BlueI am not going to re-arrange here, but when we make our table, it would be good to order the variables in their natural order (Red, Orange, Yellow, Green, Blue, Indigo, Violet). We want to make things as easy as possible on our reader. 7.6.2 Descriptive Statistics for Numeric Data For a numeric variable, we generally calculate a measure of central tendency and a measure of dispersion. 7.6.2.1 Central Tendency In lay terms, we can understand central tendency as being the average of a variable. There are three types of averages we can consider: Mean average, which is the sum of all participants responses divided by the number of responses Median average, which is the middle value of all participants responses sorted from lowest to highest and Mode average, which is the most common response to the question Generally, we are most interested in the mean and the median averages. However, the mode can be particularly important to calculate if the data is very zero-heavy. Though, even then we tend not to explicitly refer to calculating the mode. It is quite simple to calculate the mean and median in R using the \\(mean()\\) and \\(median()\\) functions. ## Let&#39;s say we have a vector of values values &lt;- c(1, 3,3, 4,4,4, 5,5,5,5,5, 6,6,6, 7,7, 8) ## If we want to calculate the mean, we can simply write: mean(values) ## [1] 4.941176 ## Likewise, if we want the median, we can just type median(values) ## [1] 5 7.6.2.2 Dispersion Now, along with central tendency, it is important to also calculate the dispersion of the variable. Dispersion is simply how spread out the values of the responses are. Perhaps we have two studies where we ask people how old they are: In the first study, oddly, every person reports that they are 37 years old. We may understand that there is actually no dispersion. In the second study, while the mean age is 37, the age range is from 27 to 47 years old, displaying that the age of participants is dispersed more broadly around the mean value How we calculate dispersion is dependent on the assumed distribution of the variable. If a variable is normally distributed, we can capture two measures of dispersion called variance and standard deviation. We refer to population-level variance as \\(\\sigma^2\\) and standard deviation as \\(\\sigma\\). When we calculate sample-level variance and standard deviation, we refer to them as \\(s^2\\) and \\(s\\). As discussed before, the normal distribution is defined by a mean value and the standard deviation. Lets say we have a \\(n\\) observations of a variable \\(X = \\{x_1, x_2,\\dots,x_n\\}\\) we assume is normally distributed. Let \\(\\bar{x}\\) be the mean value of \\(X\\). In order to estimate the normal distribution a variable follows, we can calculate the variance as follows: \\[\\sigma^2 = \\sum_{i = 1}^{n} \\frac{(x_i - \\bar{x})}{n - 1}\\] As we can see, variance is a measure of how far from the mean each observation is. The farther values are from the mean, the greater the variance. Dividing by \\(n-1\\) is known as Bessels correction. By making the denominator smaller (as opposed to \\(n\\)), we are increasing the estimated variance. This is to make a more conservative estimate of the variance. In order to calculate the standard deviation, we simply take the square root, like so: \\[\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\sum_{i = 1}^{n} \\frac{(x_i - \\bar{x})}{n - 1}}\\] It is good to know how these are calculated and to understand what they capture, but these metrics can be calculated in R using the \\(var()\\) and \\(sd()\\) functions, like so: ## variance of a set of observations var(values) ## [1] 2.933824 ## standard deviation sd(values) ## [1] 1.712841 ## As well, we noted that standard deviation is the square root of variance ## so we can actually check if that&#39;s true like so sd(values) == sqrt(var(values)) ## [1] TRUE As we cannot always be sure of the underlying distribution of a variable, it can be useful to also reflect on the range of the data (i.e., minimum and maximum values) as well as percentile distribution. Many of you are likely familiar with the idea of percentiles from standardized testing. If you get a score in the 90th percentile, that essentially means your score is higher than 90% of other test takers but that it is lower than 10%. One useful measure of dispersion is the interquartile range (IQR) - this is a measure of the middle 50% of the observations. The IQR is the difference between the 75th percentile and the 25th percentile. We can get this information by using the summary function on a numeric variable like so: summary(values) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 4.000 5.000 4.941 6.000 8.000 1st Qu. represents the 25th percentile and 3rd Qu. represents the 75th percentile. The IQR is then defined as difference between 3rd Qu. and 1st Qu. which in this case is 2. As we can see, the summary function also provides us with the minimum and maximum values (i.e., the range) of the observations as well! There are a couple of other ways to calculate IQR. The first is to use the \\(IQR()\\) function, like so: IQR(values) ## [1] 2 Which, we see gives us 2. The other is more generalizable. Sometimes, for reasons, we may want to look at different percentile values. Perhaps the 95% Interval, from the 2.5th to the 97.5th percentiles are of interest to us. We can use the \\(quantile()\\), like so: ## First, define a vector of the percentiles you want to identify ## They should be in decimal form, so if you want the 2.5th percentile you would write 0.025 percs &lt;- c(0.025, 0.975) ## Then you run the quantile() function, feeding the variable and the percentiles in as arguments quantile(values, percs) ## 2.5% 97.5% ## 1.8 7.6 This gives you the quantile range and you can customize it to extract any percentiles that you wish. Since we had so few observations, the quantile function estimated the underlying distribution of the variable and then calculated these percentile values from the identifying distribution. 7.7 Creating Table One Now that we have gone over the types of univariate descriptive statistics that are commonly found in Table One, we need to actually create Table One. Sometimes, Table One is really easy to make by hand. Maybe you have 5 variables you care about (lets say, Age, Gender, Race, Educational Attainment, and Lifetime Number of Cigarettes Smoked). It wouldnt be too hard to just do that by hand! But, sometimes, Table One can get out of hand. Check out this descriptive table from a manuscript I wrote: That is a giant table! Fortunately, I didnt have to generate it entirely by hand. I could use the power of my computer to help me along the way! 7.7.1 The tableone package To do this we are going to download and use the \\(tableone\\) package in R. It was designed to help us generate our descriptive statistics table. ## Uncomment this line of code in order to install the library #install.packages(&quot;tableone&quot;) ## Then we will load tableone into our library library(tableone) The tableone package was designed to help us make our table one, even if it is quite complicated! To display how to use it, we are going to use a dataset already built into R - the \\(mtcars\\) dataset. ## R has some practice datasets built in ## The mtcars dataset has data on 32 models of cars ## It has variables like miles per gallon (mpg), how many cylinders the car has (cyl), among others data &lt;- mtcars ## Let us peak at the data really quickly head(data) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Okay, so I know that a lot of information I care about can be accessed using the summary() function, like so: summary(data) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 In theory, I could use this to populate my table one, but I want R to just make my table one for me! To do so, we are going to use the \\(createTableOne()\\) function, like so: ## first we will create a vector of the variables we want to include in our table variables &lt;- c(&quot;mpg&quot;, # miles per gallon &quot;cyl&quot;, # number of cylinders in engine (4,6,8) &quot;am&quot;, # transmission (0 = automatic 1 = manual) &quot;hp&quot;) # Gross Horsepower ## Next we want to specify any categorical data in our variables cat_variables &lt;- c(&quot;cyl&quot;,&quot;am&quot;) ## Next we create the table object table &lt;- CreateTableOne(vars = variables, data = data, factorVars = cat_variables) ## Finally, we use the print function to print the table to the console\\ print(table, showAllLevels = T) ## ## level Overall ## n 32 ## mpg (mean (SD)) 20.09 (6.03) ## cyl (%) 4 11 (34.4) ## 6 7 (21.9) ## 8 14 (43.8) ## am (%) 0 19 (59.4) ## 1 13 (40.6) ## hp (mean (SD)) 146.69 (68.56) As we can see, this prints out the info wed like to include in table one! That is super cool! Now, for my study, I am actually comparing the performance of cars with an automatic transmission versus a manual transmission. So Id like two columns - one with the descriptives for cars with an automatic and one with the descriptives for cars with a manual. I can do that by modifying the code like so: ## first we will create a vector of the variables we want to include in our table variables &lt;- c(&quot;mpg&quot;, # miles per gallon &quot;cyl&quot;, # number of cylinders in engine (4,6,8) &quot;am&quot;, # transmission (0 = automatic 1 = manual) &quot;hp&quot;) # Gross Horsepower ## Next we want to specify any categorical data in our variables cat_variables &lt;- c(&quot;cyl&quot;,&quot;am&quot;) ## Next we want to specify our stratification variables stratify &lt;- c(&quot;am&quot;) ## Next we create the table object table &lt;- CreateTableOne(vars = variables, data = data, factorVars = cat_variables, strata = stratify) ## Finally, we use the print function to print the table to the console\\ print(table, showAllLevels = T) ## Stratified by am ## level 0 1 p test ## n 19 13 ## mpg (mean (SD)) 17.15 (3.83) 24.39 (6.17) &lt;0.001 ## cyl (%) 4 3 ( 15.8) 8 ( 61.5) 0.013 ## 6 4 ( 21.1) 3 ( 23.1) ## 8 12 ( 63.2) 2 ( 15.4) ## am (%) 0 19 (100.0) 0 ( 0.0) &lt;0.001 ## 1 0 ( 0.0) 13 (100.0) ## hp (mean (SD)) 160.26 (53.91) 126.85 (84.06) 0.180 Beautiful! Now we have two columns, one for automatic transmission and one for manual. To neaten this up I am going to do a few things: I am going to remove am from the table since it is what I am stratifying by. I am going to factor am so that it reads Automatic and Manual I am going to add a column that represents the whole sample data$am &lt;- factor(data$am, levels = c(0,1), labels = c(&quot;Automatic&quot;,&quot;Manual&quot;)) ## first we will create a vector of the variables we want to include in our table variables &lt;- c(&quot;mpg&quot;, # miles per gallon &quot;cyl&quot;, # number of cylinders in engine (4,6,8) &quot;hp&quot;) # Gross Horsepower ## Next we want to specify any categorical data in our variables cat_variables &lt;- c(&quot;cyl&quot;) ## Next we want to specify our stratification variables stratify &lt;- c(&quot;am&quot;) ## Next we create the table object table &lt;- CreateTableOne(vars = variables, data = data, factorVars = cat_variables, strata = stratify, addOverall = T) ## Finally, we use the print function to print the table to the console\\ print(table, showAllLevels = T) ## Stratified by am ## level Overall Automatic Manual p ## n 32 19 13 ## mpg (mean (SD)) 20.09 (6.03) 17.15 (3.83) 24.39 (6.17) &lt;0.001 ## cyl (%) 4 11 (34.4) 3 (15.8) 8 (61.5) 0.013 ## 6 7 (21.9) 4 (21.1) 3 (23.1) ## 8 14 (43.8) 12 (63.2) 2 (15.4) ## hp (mean (SD)) 146.69 (68.56) 160.26 (53.91) 126.85 (84.06) 0.180 ## Stratified by am ## test ## n ## mpg (mean (SD)) ## cyl (%) ## ## ## hp (mean (SD)) That is very cool! Now, we can see from the table above the Manual cars get more miles per gallon than Automatic ones. We even see a p-value indicating its a significant difference. The tableone package has actually run the appropriate comparison test (in this case, a t-test) to compare the two groups. We are going to talk more about these tests in a future lesson. Now, this is already great! You can create a table and copy in each value. But, it gets even better. We can actually write this table into a .csv file that we can open in Excel or Google Sheets, like so: ## We are going to write the table to a .csv write.csv(print(table, showAllLevels = T), file = &quot;table1.csv&quot;) ## Stratified by am ## level Overall Automatic Manual p ## n 32 19 13 ## mpg (mean (SD)) 20.09 (6.03) 17.15 (3.83) 24.39 (6.17) &lt;0.001 ## cyl (%) 4 11 (34.4) 3 (15.8) 8 (61.5) 0.013 ## 6 7 (21.9) 4 (21.1) 3 (23.1) ## 8 14 (43.8) 12 (63.2) 2 (15.4) ## hp (mean (SD)) 146.69 (68.56) 160.26 (53.91) 126.85 (84.06) 0.180 ## Stratified by am ## test ## n ## mpg (mean (SD)) ## cyl (%) ## ## ## hp (mean (SD)) Now we are able to open our table in Excel like so: Now, we can use Excel and make our table look nice. We didnt have to format anything or change any of the values! 7.8 In Conclusion Table One is the most important table in your manuscript. It allows us to identify who our study sample is so we can understand what population our results are generalizable too. By using the tableone package in R, we can make rather complex tables quite easily! I provide this exact example, because it is common in many types of population survey that non-Hispanic White people are over-sampled "],["comparing-two-groups-w-independent-samples-t-test.html", "Chapter 8 Comparing Two Groups w/ Independent Samples T-test 8.1 Statistical Hypotheses of the Independent Samples T-Test 8.2 Logic of the t-test 8.3 Introducing the Students t-Distribution 8.4 Mapping The Signal Onto The t-Distribution 8.5 Three Variations of the \\(t\\)-test 8.6 What Are the Assumptions We Make Prior to Running an Independent Samples T-test 8.7 Last But Not Least - How Do We Run A t-Test in R", " Chapter 8 Comparing Two Groups w/ Independent Samples T-test A common objective we have in our reserach work is to compare groups of people. For example, we might wish to know: if PhD students are more anxious than undergrad students; if men are more likely to binge drink than women; or, if students in different school districts have different performance on standardized tests. The \\(t\\)-test is one such test we can use, when the data calls for it! Let us say we have a random sample that we have split into two groups, \\(G_1\\) and \\(G_2\\). We have measured a numeric variable \\(X\\), which we assume is normally distributed. For example, \\(G_1\\) might represent men and \\(G_2\\) might represent women and \\(X\\) might be systolic blood pressure. We are curious if group membership \\((G_1 vs G_2)\\) is associated with different values of \\(X\\). One way we can frame this as a scientific question is to ask if: The mean value of \\(X\\) is different between groups \\(G_1\\) and \\(G_2\\). Lets define \\(\\mu_{G1}\\) as the population-level mean of \\(X\\) for \\(G_1\\), and \\(\\mu_{G2}\\) as that for \\(G_2\\). What we are essentially trying to ask is, does \\(\\mu_{G1} = \\mu_{G2}\\)? 8.1 Statistical Hypotheses of the Independent Samples T-Test The null and alternate hypothesis of the t-test are typically presented as: \\(H_0\\): \\(\\mu_{G1} = \\mu_{G2}\\) \\(H_A\\): \\(\\mu_{G1} \\neq \\mu_{G2}\\) Importantly, \\(\\mu_{G1} = \\mu_{G2}\\) is the same as \\(\\mu_{G1} - \\mu_{G2} = 0\\). Therefore, we can also consider the null and alternate hypothesis as follows: \\(H_0\\): \\(\\mu_{G1} - \\mu_{G1} = 0\\) \\(H_A\\): \\(\\mu_{G1} - \\mu_{G1} \\neq 0\\) So, the independent sample t-test will attempt to assess the probability of our observed data assuming that \\(H_0\\) is true. 8.2 Logic of the t-test Before we learn how to run a t-test, we need to think through what our null hypothesis means and what it might mean to reject it. When we run a statistical test, we assume that our null hypothesis is true. If we assume that the null is true and that \\(\\mu_{G1} - \\mu_{G2} = 0\\) at the population-level then it follows that if we were to sample a set of people from \\(G_1\\) and \\(G_2\\) and looked at the difference in their group means of \\(X\\), that 0 is the most likely value. While we use \\(\\mu_{G_1}\\) and \\(\\mu_{G_2}\\) to represent the population-level mean, we use \\(\\bar{x}_{G1}\\) and \\(\\bar{x}_{G2}\\) to represent the mean values of our sample groups. However, we know that sampling pretty much never results in a perfect representation of the populations, so it is actually fairly likely that the difference in sample group means (\\(\\bar{x}_{G1} - \\bar{x}_{G2}\\)) wont equal 0. This shouldnt worry us too much, it is fairly likely that if the null is true then it would make sense if \\(\\bar{x}_{G1} - \\bar{x}_{G2}\\) was a value pretty close to 0 (whether positive or negative). In fact, if the null hypothesis is true, then we would assume that values of \\(\\bar{x}_{G1} - \\bar{x}_{G2}\\) further from 0 would be less likely than values closer to 0 and that negative and positive values are equally likely. Hmmthat actually sounds a lot like the normal distribution doesnt it? 0 is the most likely value values closer to 0 are more likely than values further from 0 positive and negative values appear equally as likely to occur (i.e. symmetry) But 8.3 Introducing the Students t-Distribution A normal distribution is defined by a mean value \\(\\mu\\) and a standard deviation \\(\\sigma\\). Unfortunately, even if we know we have measured a normally distributed variable \\(X\\), we may not know the population-level standard deviation of \\(X\\). This is fairly common in epidemiologic research, because we often dont have population-level data about the groups we are researching (such as people who inject drugs, or undergraduate students who vape, or people who access syringe exchanges). The \\(t\\)-distribution is a variation of the standard normal distribution (\\(Z\\)-distribution), to be used when the standard deviation of the population is unknown. As we discussed before, any normal distribution (\\(N(\\mu,\\sigma)\\)) can be transformed to the \\(Z\\)-distribution (\\(N(0,1)\\)). Similarly, we may understand the \\(t\\)-distribution as a standardized distribution. The \\(t\\)-distribution is intended to be a more conservative version of the \\(Z\\)-distribution, where we assume a wider variability in observations. Essentially, the less information we know, the less certain we are that observations will be near the mean. We define the \\(t\\)-distribution as a function of the number of degrees of freedom we have available to measure the variability in our data: Degrees of freedom refer to the number of parameters that are able to vary freely, given some assumed outcome. For example, let us say you have have 100 participants and you know that their mean average is 60 years old. Well, there are countless possibilities for how 100 participants age could average out to 60 (e.g., everyone could be 60 years old, half could be 59 and half could be 61, etc etc). In other words, we can see that peoples ages can vary freely while still maintaining an average age of 60 years old. But, let us imagine we know the exact age of 99 of the 100 individuals. The average age of the population is 60 years of age. Can the age of the final person vary freely? No! There is actually only one value that could take the first 99 values and get the average to 60 years of age. For example, if the average age of the first 99 people is 60 years of age, then the age of the final person must be 60 years of age. If they were older, then the average would move above 60, and vice versa if they were younger. As such to calculate a mean, we must spend one degree of freedom. A normal distribution is defined by a mean value \\(\\mu\\) and a standard deviation \\(\\sigma\\). If we have \\(n\\) observations and measure sample-mean \\(\\bar{x}\\) and standard deviation \\(s\\), as discussed above, we must spend one degree of freedom to calculate \\(\\bar{x}\\). This means that we have \\(n - 1\\) degrees of freedom to calculate \\(s\\). The fewer observations we have (i.e., the smaller that \\(n\\) is), the less information we have to estimate the variation of our observed variable \\(X\\). As such, the \\(t\\)-distribution is intended to capture uncertainty in the measurement of the standard deviation from a small sample. The fewer degrees of freedom (i.e., the smaller our sample), the less certain that our measured standard deviation \\(s\\) represents our population-level standard deviation \\(\\sigma\\). To capture this, the \\(t\\)-distribution is shorter and wider than the normal distribution. Essentially, under the \\(t\\)-distribution, values farther from 0 are more likely than under the \\(Z\\)-distribution. As we collect more data (i.e., as \\(n\\) gets larger), the \\(t\\)-distributions shapes approaches that of the \\(Z\\)-distribution. To picture what this means, lets plot some distributions. First we are going to plot the \\(Z\\)-distribution along with \\(t(1)\\), the \\(t\\)-distribution with one degree of freedom: ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .1) ## Let&#39;s now define our normal function using the dnorm() function, where the mean value is 0 and the standard deviation is also 1 y_normal &lt;- dnorm(x, mean = 0, sd = 1) ## Let&#39;s also define our t-distribution with 1 degree of freedom on the same scale using the dt() function y_df1 &lt;- dt(x, df = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y_normal, type=&quot;l&quot;, ylab = &quot;Density&quot;) ## And we will also plot the t-distribution in red lines(x,y_df1, col = &quot;red&quot;) As we can see, the \\(t\\)-distribution is very similar to the normal distributionit is just shorter and wider. Remember, we discussed in a previous chapter that the height of a curve for a given value of \\(X\\) represents the probability of observing that value. In the case of the \\(t\\)-distribution above, it is clear that it is less likely to observe a value of 0 than the normal distribution, but more likely to observe extreme values (along both tails) than the normal distribution. This is the conservative adjustment made to the distribution to account for our lack of information to determine the variability of the data. Now, as the number of observations \\(n\\) increases, the \\(t\\)-distribution begins to look a lot like the normal distribution. In fact, a common rule of thumb is that if \\(n \\geq 30\\) then we can actually just assume that the \\(t\\)-distribution is the same as the normal distribution. Lets look at some \\(t\\)-distributions plotted along side the normal distribution to see how, as the number of degrees of freedom increase, the \\(t\\)-distribution starts to look a lot like the normal distribution. ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .1) ## Let&#39;s now define our normal function using the dnorm() function, where the mean value is 0 and the standard deviation is also 1 y_normal &lt;- dnorm(x, mean = 0, sd = 1) ## Then let&#39;s generate t-distributions with degrees of freedom equal to 1, 3, 5, and 10 y_df1 &lt;- dt(x, df = 1) y_df3 &lt;- dt(x, df = 3) y_df5 &lt;- dt(x, df = 5) y_df10 &lt;- dt(x, df = 10) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y_normal, type=&quot;l&quot;, ylab = &quot;Density&quot;) ## And plot the additional t-distributions as well lines(x,y_df10, col = &quot;green&quot;) lines(x,y_df5, col = &quot;purple&quot;) lines(x,y_df3, col = &quot;orange&quot;) lines(x,y_df1, col = &quot;red&quot;) So, our null hypothesis is that \\(\\mu_{G1} - \\mu_{G2} = 0\\). As we articulated before, if our null hypothesis is true, \\(\\bar{x}_{G1} - \\bar{x}_{G2}\\) appears to behave like a normally distributed variable. BUT since we do not know the population-level standard deviation, we want to be more cautious. As such, we assume that \\(\\bar{x}_{G1} - \\bar{x}_{G2}\\) follows a \\(t\\)-distribution, which has wider tails. If we let \\(n_{G1}\\) represent the number of people in \\(G_1\\) and if we let \\(n_{G2}\\) represent the number of people in \\(G_2\\), then we use a \\(t\\)-distribution with \\((n_{G1} - 1) + (n_{G2} - 1) =n_{G1} + n_{G2} - 2\\) degrees of freedom. This is because we have \\((n_{G1} - 1)\\) degrees of freedom to calculate the variability of \\(X\\) among \\(G_1\\), and \\((n_{G2} - 1)\\) to calculate that among \\(G_2\\). 8.4 Mapping The Signal Onto The t-Distribution So, we want to know how probable our data is under the null hypothesis that \\(\\mu_{G1} = \\mu_{G2}\\). Our signal is the difference in mean value of \\(X\\) across our two groups, or: \\(\\bar{x}_{G1} - \\bar{x}_{G2}\\). Since the most likely value of our signal, assuming the null is true, is 0, we can understand that its corresponding distribution is centered around 0 (just like the \\(Z\\)-distribution). However, we must scale our signal by the noise in the data - in other words, we must standardize the signal to correspond to the appropriate \\(t\\)-distribution, which is a variation of the \\(Z\\)-distribution whose standard deviation is 1. 8.4.1 Calculating the Standard Error of the Mean (Content Warning: This next section gets pretty math-y. If you are having a hard time following the equations, that is perfectly fine. As long as we have computers, we will never need to do these calculations by hand. And if we ever enter a world without computers anymore, I have a feeling statistics will be the least of our concerns. BUT, this is a central aspect of the \\(t\\)-test and it is good to try your best to follow and understand. You may notice that the logic behind this is exactly the same as what we did when we ran a \\(Z\\)-test in prior weeks) To standardize our signal, we will divide it by the standard error of the mean of our observed values of \\(X\\). While you will generally not need to make these calculations, it is important to understand how it is calculated. The standard error is an estimation of the population-level standard deviation, which gets more precise (or smaller) as the sample size (\\(n\\)) gets larger. The typical equation for the standard error is: \\[SE = \\frac{s}{\\sqrt{n}}\\] But, because we are comparing two groups, we actually use a slight variation on this equation: \\[SE = \\frac{s}{\\sqrt{n_{G1} + n_{G2}}}\\] \\(s\\) is the sample standard deviation. It is derived from the measured variance \\(s^2\\), which represents the average distance of each \\(n\\) observations of \\(X\\) from the mean value \\(\\bar{x}\\): \\[s^2 = \\frac{1}{n-1}*\\sum_{i=1}^{n}(x_i - \\bar{x})\\] However, in the independent samples \\(t\\)-test, we are comparing two groups \\(G_1\\) and \\(G_2\\). As we shall discuss, one assumption of the \\(t\\)-test is that the population-level variance of variable \\(X\\) is the same (or very similar) for both groups. This allows us to calculate the pooled variance of \\(X\\) within both \\(G_1\\) and \\(G_2\\) like so: \\[s^2 = \\frac{(n_{G1} - 1)*s^2_{G1} + (n_{G2} - 1)*s^2_{G2}}{n_G1 + n_G2 - 2}\\] Where \\(s^2_{G1}\\) is the sample variance of \\(X\\) for \\(G_1\\) and \\(s^2_{G2}\\) is that for \\(G_2\\). To get the sample standard deviation, we simply take the square root like so: \\[s = \\sqrt{s^2} = \\sqrt{\\frac{(n_{G1} - 1)*s^2_{G1} + (n_{G2} - 1)*s^2_{G2}}{n_G1 + n_G2 - 2}}\\] Remember how we mentioned that for this test we have \\(n_G1 + n_G2 - 2\\) degrees of freedom to estimate the variability in our data? You will notice that number is prominantly displayed in our equation above. From here we can calculate our standard error, which was: \\[SE = \\frac{s}{\\sqrt{n_{G1} + n_{G2}}}\\] As we can see, there are a lot of steps to calculating the standard error of the mean: We must calculate the variance of \\(X\\) within both \\(G_1\\), \\(s^2_{G1}\\), and \\(G_2\\), \\(s^2_{G2}\\). We must calculate the pooled variance, \\(s^2\\). Calculate the standard deviation \\(s\\) by taking the square root of the pooled variance. Calculate the standard error: \\(SE = \\frac{s}{\\sqrt{n_{G1} + n_{G2}}}\\) 8.4.2 Calculating our test statistic, t So, we went through all of that mathematically intensive trouble to standardize \\(\\bar{x}_{G1} - \\bar{x}_{G2}\\) so that we can test our null hypothesis. Lets formalize this standardized value. The t-statistic is defined as follows: \\[t = \\frac{\\bar{x}_{G1} - \\bar{x}_{G2}}{SE}\\] Often, the equation for calculating \\(t\\) has the formula for the standard error written out and reads: \\[t = \\frac{\\bar{x}_{G1} - \\bar{x}_{G2}}{s*\\sqrt{\\frac{1}{n_{G1}} + \\frac{1}{n_{G2}}}}\\] By calculating \\(t\\), we have taken our signal (\\(\\bar{x}_{G1} - \\bar{x}_{G2}\\)) and standardized it to a \\(t\\)-distribution with \\(n_{G1} + n_{G2} - 2\\) degrees of freedom! We can now map this value onto this distribution and make our area under the curve calculations. Lets pretend that, in this hypothetical study, we had sampled 100 people in \\(G_1\\) and 100 people \\(G_2\\). It turns out that \\(\\bar{x}_G1 = 21\\) and \\(\\bar{x}_G2 = 22\\) and we calculated a pooled standard deviation of 3. Thus, we can calculate: \\[t = \\frac{21-22}{3 * \\sqrt{\\frac{1}{100} + \\frac{1}{100}}} \\approx -2.36\\] Now, to test the likelihood of observing this signal in our data (or a signal more extreme) assuming that the null hypothesis is true, we can start by mapping our calculated \\(t = -2.36\\) on a \\(t\\)-distribution with \\(100 + 100 - 2 = 198\\) degrees of freedom, like so: ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .1) ## t-distribution with 198 df y_df198 &lt;- dt(x, df = 198) ## We will now plot the distribution data as a line (type = &quot;l&quot;) plot(x,y_df198, type=&quot;l&quot;, ylab = &quot;Density&quot;) ## And we will also plot the the t-value as a dotted vertical line xt &lt;- c(-2.36,-2.36) yt &lt;- c(0,1) lines(xt,yt, col = &quot;red&quot;) Typically, when we run a \\(t\\)-test, we will run a two-tailed \\(t\\)-test. This means that we will check the probability of observing any signal more extreme than \\(t = -2.36\\). This will include all values less than -2.36 and all values greater than 2.36, like so: ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .01) ## Let&#39;s define our t-distribution with 10 degrees of freedom y_df198 &lt;- dt(x, df = 198) ## Filling in the plot we want all the values of X less than or equal to -1.2 and X more than or equal to 1.2 neg_x &lt;- x[x &lt; -2.35] pos_x &lt;- x[x &gt;= 2.36] ## Then we want to get the values of y_df10 that correspond to these areas ## See if you can figure out why this works, don&#39;t worry, being able to do this is not INTEGRAL to becoming a statistician (sorry) neg_y &lt;- y_df198[1:length(neg_x)] pos_y &lt;- y_df198[(length(y_df198)-length(pos_x)+1):length(y_df198)] ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y_df198, type=&quot;l&quot;, ylab = &quot;Density&quot;) ## The polygon function has some weird behavior so I add ## these lines of codes to help it understand I want it to ## draw the area under the curve below the distribution, not above it neg_x &lt;- c(neg_x, -2.36) neg_y &lt;- c(neg_y, 0) pos_x &lt;- c(pos_x, 2.36) pos_y &lt;- c(pos_y, 0) ## Then we can plot the area polygon(neg_x,neg_y, col = &quot;slateblue1&quot;) polygon(pos_x,pos_y, col = &quot;slateblue1&quot;) Now, we can calculate our \\(p\\)-value by taking the area under curve of these regions: ## So now we want to take the integral of both of these sections and add them together AUC &lt;- MESS::auc(neg_x,neg_y) + MESS::auc(pos_x,pos_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,4) ## [1] 0.0192 Our \\(p\\)-value is 0.0192, indicating that we think there is only a 1.92% chance of observing the signal within our data (or a signal more extreme) assuming that the null hypothesis is true. Assuming a standard \\(\\alpha\\) threshold of 0.05, we would consider this result as signficant evidence that our null hypothesis, that \\(\\mu_{G1} = \\mu{G2}\\), may be incorrect. 8.4.3 Two-Tailed Versus One-Tailed T-Test We refer to this as a two-tailed t-test because, as you can see in the image, we look at extreme values in both tails of the distribution. Generally, we do a two-tailed test when we want to know if the mean values are different. But, there are some cases where we assume that the effect can only occur in one direction. For example, let us say we have a study where we want to know if people who got a treatment had a greater reduction in blood pressure than people who did not receive the treatment. We assume that the treatment can only help people. Lets say \\(\\mu_C\\) represents the average reduction among the control group and \\(\\mu_T\\) that among the treatment. Instead of the alternate hypothesis being that \\(\\mu_T \\neq \\mu_C\\), we instead make it more specific to be that \\(\\mu_T &gt; \\mu_C\\) (i.e., that the reduction in the treatment group is greater than that of the control group). So, now, let us say we ran a t-test to test this and got \\(t = 2.3\\). We now only calculate the area under the curve for the positive tail, like so: ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .1) ## Let&#39;s define our t-distribution with 10 degrees of freedom y_df198 &lt;- dt(x, df = 198) ## Filling in the plot we want all the values of X less than or equal to -1.2 and X more than or equal to 1.2 pos_x &lt;- x[x &gt;= 2.3] ## Then we want to get the values of y_df10 that correspond to these areas pos_y &lt;- y_df198[(length(y_df198)-length(pos_x)+1):length(y_df198)] ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y_df198, type=&quot;l&quot;, ylab = &quot;Density&quot;) ## The polygon function has some weird behavior so I add ## these lines of codes to help it understand I want it to ## draw the area under the curve below the distribution, not above it pos_x &lt;- c(pos_x, 2.3) pos_y &lt;- c(pos_y, 0) ## Then we can plot the area polygon(pos_x,pos_y, col = &quot;slateblue1&quot;) ## So now we want to take the integral of both of these sections and add them together AUC &lt;- MESS::auc(pos_x,pos_y) ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,4) ## [1] 0.0113 We see that our \\(p\\)-value is 0.0113! 8.5 Three Variations of the \\(t\\)-test 8.5.1 Independent Samples t-test Here we have defined the independent samples \\(t\\)-test. Given two groups, \\(G_1\\) and \\(G_2\\), we can compare the mean value of a random normally distributed variable \\(X\\) by calculating: \\[t = \\frac{\\bar{x}_{G1} - \\bar{x}_{G2}}{s*\\sqrt{\\frac{1}{n_{G1}} + \\frac{1}{n_{G2}}}}\\] and the by comparing this value \\(t\\) to a \\(t\\)-distribution with \\(n_{G1} + n_{G2} - 2\\) degrees of freedom. A p-value is computed by taking the area under the curve for all values more extreme than the observed test statistic \\(t\\). In this case, our hypotheses are as follows: \\(H_0\\): \\(\\mu_{G1} = \\mu_{G2}\\) \\(H_A\\): \\(\\mu_{G1} \\neq \\mu_{G2}\\) A significant finding indicates that our observed data is very unlikely, if the null hypothesis is assumed. Thus, this supplies us with evidence that the null hypothesis is incorrect. 8.5.2 One Sample \\(t\\)-test A variation of the \\(t\\)-test is the one sample \\(t\\)-test. This is done when we wish to compare the mean value of a normally distributed variable \\(X\\) of a group \\(G\\) to a specific value. For example, perhaps I sell flour in 1-pound bags and it is too time costly to weigh every single bag one-at-a-time. So, I developed a quick method of filling each bag with 1-pound of flour without measuring. To test if my method works, I decide to see if the mean weight of 100 bags of flour equals 1 pound. So our hypotheses are as follows: \\(H_0\\): \\(\\mu = 1\\) \\(H_A\\): \\(\\mu \\neq 1\\) where \\(\\mu\\) is the average weight of a bag of flower. To run the 1-sample \\(t\\)-test, we calculate the mean of our 100 bags \\(\\bar{x}\\) and the standard deviation of observations \\(s\\) and we calculate \\(t\\) as follows: \\[t = \\frac{\\bar{x} - 1}{s*\\sqrt{\\frac{1}{n}}}\\] we then calculate our \\(p\\)-value by comparing \\(t\\) to a \\(t\\)-distribution with \\(n - 1\\) degrees of freedom (which, in this case, is 99). 8.5.3 Paired Samples \\(t\\)-test One important variation of the \\(t\\)-test is the paired samples \\(t\\)-test. This is done when we take the same measurement from the sample at two separate time points and we wish to assess if the mean value has changed or remained the same. This can be a good way to assess if an intervention has changed the participants performance on a knowledge-based task. So, for each participant we measure a variable at two time points. For a participant, we want to measure the difference in their score from time 1 \\(x_1\\) and time 2 \\(x_2\\). We define this difference as \\(d = x_1 - x_2\\). We then calculate the mean value of \\(d\\) for all participants \\(\\bar{d}\\) and the standard deviation of \\(d\\), \\(s_d\\). Our null and alternate hypotheses are as follows: \\(H_0\\): \\(\\bar{d} = 0\\) \\(H_A\\): \\(\\bar{d} \\neq 0\\) We can then calculate our \\(t\\) test statistic as follows: \\[t = \\frac{\\bar{d}}{s_d*\\sqrt{\\frac{1}{n}}}\\] We then calculate our \\(p\\)-value by comparing \\(t\\) to a \\(t\\)-distribution with \\(n-1\\) degrees of freedom. 8.6 What Are the Assumptions We Make Prior to Running an Independent Samples T-test Prior to running a \\(t\\)-test, we must ensure that our data is appropriate for doing so. This means we need to know what assumptions must be met by our data and we need to be able to check if they hold. Let us say that we have some variable \\(X\\): 8.6.1 Assumption #1: Our Variable of Interest, \\(X\\), Must Be Measured on an Ordinal or Continuous Scale Our variable \\(X\\) must be measured measured on an ordinal or continuous scale. Generally, this assumption is straightforward - you cannot run a t-test when your variable of interest is categorical. Running the \\(summary()\\) function in R is a good starting place, but typically this assumption can be checked by looking at the dataset visually. 8.6.2 Assumption #2: Data Must Be Drawn From a Random Sample The effectiveness of the \\(t\\)-test is dependent on administering an effective random sample of the groups of interest. In research methods courses, we will often discuss the risk of sampling bias. If there is bias within our sampling procedure, than the \\(t\\)-test may simply detect this bias. For example, perhaps you want to compare the mean value of \\(X\\) of a sample of men and women. Perhaps, inadvertantly, you were more likely to sample men with higher values of \\(X\\) compared to men with lower values of \\(X\\). The \\(t\\)-test cannot decipher if variation in the data is a result of actual population differences or if the variation is the result of sampling bias. It is crucial to reflect on whether or not this assumption has been met. 8.6.2.1 Assumption #2.5: Groups Must Be Independent When we run an independent samples \\(t\\)-test, the two groups must be independent of one another, meaning they must represent distinct populations. This basically means that someone cannot qualify to be in both groups, as that would indicate certain individuals may represent both groups. For example, we might want to compare the running speed of baseball and basketball players  but if someone plays both baseball and basketball, they are actually representative of both populations. If such a person were included in the study, they would qualify to be in both groups and would violate the assumption of indpendence. Typically, this assumption is checked by examining the inclusion criteria for each group being compared and ensuring that, by definition, they are independent. 8.6.3 Assumption #3: Normality of Observations of \\(X\\) The mean value of \\(X\\) for each group being studied should be normally distributed. The larger our overall sample size, the weaker this assumption becomes and, as the sample size grows, the t-test is more robust to violations of the assumption of normality. In statistics, you will often see the word robust  if a method is robust, it means that it still works well despite certain violations in our underlying assumptions about the datas distribution. So, it is important that we know how to check if a variable is normally distributed in our groups. I will generate two normally distributed variables and plot them in to display how we can check this. ## Let&#39;s generate data for two hypothetical groups ## In our study, we recruited 100 men and 100 women ## Let&#39;s use R to generate some random data ## We use the rnorm() function to randomly sample values from a normal distribution ## We do this once for men and once for women ## I have arbitrarily chosen a mean and standard variation ## Typically, this information is simply contained in a dataset male_systolic &lt;- rnorm(100, mean = 120, sd = 2) female_systolic &lt;- rnorm(100, mean = 115, sd = 2) ## First, we need to generate density functions for both variables using the density() function male_density &lt;- density(male_systolic) female_density &lt;- density(female_systolic) ## So now I want to plot the distributions of each of these groups ## First I create my x-axis, which should contain all of the values in the dataset ## So we need to calculate the minimum and maximum ends of the x-axis ## First we calculate the minimum value of all measurements minimum_x &lt;- min(c(male_density$x,female_density$x)) ## Then we calculate the maximum value of all measurements maximum_x &lt;- max(c(male_density$x,female_density$x)) ## Then we do the same thing with the y-axis minimum_y &lt;- 0 ## The minimum value of the y-axis, by default should be 0 maximum_y &lt;- max(c(male_density$y,female_density$y)) ## Now we can plot both the distributions on one graph ## We will plot the male distribution in green ## And the female distribution in blue plot(male_density, ## plot the male function type=&quot;l&quot;, ## plot it as a line col = &quot;green&quot;, ## plot it in the color green xlim = c(minimum_x, maximum_x), ## set the x-axis boundaries ylim = c(minimum_y, maximum_y)) ## set the y-axis boundaries ## Next we print the female density function lines(female_density, col = &quot;blue&quot;) As we can see, the variables dont perfectly follow the standard normal distribution, but we never expect the distribution of data from a sample to perfectly match up. Another, simpler way, to plot the distribution of our data is using the \\(hist()\\) function to generate a histogram, like so: ## We will first generate a histogram of our male group hist(male_systolic) ## Then we will generate a histogram of our female group hist(female_systolic) This was a bit quicker to code than the previous way. We can still see a similar pattern in our data, though it isnt quite clear if this suffices. So far, this is a very qualitative way of determing if the variables follow the normal distribution. We can actually run the Shapiro-Wilk test to determine if the data is normally distributed. The Shaprio-Wilk test attempts to measure how closely the data follows the normal distribution (the null hypothesis being that the data is normally distributed). If the result is significant, then we reject the null and assume that the data is non-normal, violating our assumption. ## We can do this with the shapiro.test function ## We do this for both of our groups shapiro.test(male_systolic) ## ## Shapiro-Wilk normality test ## ## data: male_systolic ## W = 0.99102, p-value = 0.7464 shapiro.test(female_systolic) ## ## Shapiro-Wilk normality test ## ## data: female_systolic ## W = 0.99138, p-value = 0.7749 Voila! It appears that the result is non-signficant and thus we feel comfortable moving forward assuming that our variables are normally distributed across each group. 8.6.4 Assumption #4: Homogeneity of Variance The independent sample \\(t\\)-test assumes that the variance of the two groups is the same. We have assumed that the values are normally distributed and normal distributions are defined by a central tendency and a standard deviation. Essentially, this assumption is just that the standard deviation of both variables is roughly the same. We can use the Levene Test to determine if the variance of \\(X\\) for both groups is the same. ## We are going to use the leveneTest in the &quot;car&quot; package ## This function assumes our data is structured as a data.frame with a separate variable for our grouping vairbale (gender) and our vvalue X (systolic_BP) ## We want a data frame with two columns ## Often, your data will already be in a data.frame ## For practical purposes I am generating a new data.frame here ## One column for gender, one column with the bp values ## we want BPs corresponding to women in the sample to be assigned the gender value &quot;Female&quot;, and same with men in the sample and &quot;Male&quot; ## First we create a vector that contains the words &quot;Male&quot; and &quot;Female&quot; ## This will be the gender column of our data frame ## We simply create a vector with the word male the same number of times as men in teh sample, and then repeat that for women inthe sample. ## We do that with the rep() function, which creates a vector by repeating the first argument n times. ## We set n = length(male_systolic), which gives us the number of blood pressure observations in our sample gender_values &lt;- c(rep(&quot;Male&quot;,length(male_systolic)), rep(&quot;Female&quot;,length(female_systolic))) ## We then create a vector of blood pressure values by merging the vectors containing results of men and then women ## We do men first because that is how we created our gender_values variable bp_values &lt;- c(male_systolic, female_systolic) ## Finally, we create our dataframe using the cbind function and the data.frame function ## cbind stands for &quot;column bind&quot; and just tells the computer to merge the vectors as columns. df &lt;- data.frame(cbind(gender_values, bp_values)) ## Usually when we do this, the computer picks weird names for the variables ## We can set the names ourselves colnames(df) &lt;- c(&quot;Gender&quot;,&quot;Systolic_BP&quot;) ## As well, the computer always guesses the kind of variable each column is supposed to be and sometimes the computer is wrontg ## So we can tell the computer how to read the variable ## In this case we use the as.numeric() function to tell the computer that this variable should be read as numeric df$Systolic_BP &lt;- as.numeric(df$Systolic_BP) ## And finally, we can run the test! ## We use the ~ formula notation that you will see more throughout this course ## Generally, formulas are in the form of Y ~ X1 + X2 + ... + XN ## We usually read this as saying Y is regressed on X1, X2, .., and XN ## However, don&#39;t worry too much about that right now, we can use the leveneTest in the car package to test for homogeneity of variance of our variables car::leveneTest(Systolic_BP ~ Gender, data = df) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.0231 0.8793 ## 198 As we can see, the p-value is quite high (\\(Pr(&gt;F)\\)), so we dont have any reason to believe the variances of the two variables are not homogenous (note, this doesnt mean that they are homogenous, just that we feel safe assuming that it is so!) 8.7 Last But Not Least - How Do We Run A t-Test in R Running a t-test in R is quite simple. Given a normally distributed variable \\(X\\) measured among two groups \\(G_1\\) and \\(G_2\\), we need to create two vectors. The first vector contains all values of \\(X\\) for \\(G_1\\) and the second vector contains all values of \\(X\\) for \\(G_2\\). We then supply them to the \\(t.test()\\) function as arguments. We shall use the systolic blood pressure data frame we generated when running the levene test above. As such, we shall create a vector with all the blood pressure observations of men and those women and then supply them to \\(t.test()\\) to run the \\(t\\)-test. ## Vector for men in the sample men &lt;- df$Systolic_BP[df$Gender == &quot;Male&quot;] ## Vector for women in the sample women &lt;- df$Systolic_BP[df$Gender == &quot;Female&quot;] ## Now we shall run the t-test ## I am setting var.equal to be TRUE because the function ## Will attempt to introduce corrections if it senses the variations ## Of the two groups is different. t.test(men, women, var.equal = T) ## ## Two Sample t-test ## ## data: men and women ## t = 18.152, df = 198, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 81.34567 101.17433 ## sample estimates: ## mean of x mean of y ## 146.13 54.87 Voila! We have run a \\(t\\)-test. That was much easier than learning about the \\(t\\)-test. We can see that the function calculated that \\(t = 18.787\\) with 198 degrees of freedom. This resulted in a \\(p\\)-value less than &lt;0.001. The \\(t\\)-test function can be used to specify other elements such as if you want to run a one-tailed or two-tailed test, a one sample t-test, a paired t-test, among many other options. "],["pearsons-chi2-test.html", "Chapter 9 Pearsons \\(\\chi^2\\) Test 9.1 Expectations v. Observations 9.2 The \\(\\chi^2\\) Distribution 9.3 The \\(\\chi^2\\) Test of Independence 9.4 Other Variations of the \\(\\chi^2\\) Test", " Chapter 9 Pearsons \\(\\chi^2\\) Test The \\(\\chi^2\\) (chi-squared, pronounced like the last syllable of sky) test allows us to assess if observed differences in the behavior of categorical data is explained by random chance or represents a meaningful pattern. This method was developed by Karl Pearson and introduced in 1900 - it is a foundational method in modern statistics. 9.1 Expectations v. Observations Given two categorical variables \\(X\\) and \\(Y\\), it often of interest to ask if these two variables are associated with one another. However, categorical data can be challenging to work with because we cannot map the values of categorical data onto some distribution. The \\(\\chi^2\\) test was developed to give us a way to map observed patterns in the data between categorical variables onto a theoretical distribution, thus allowing us to conduct a signifance test. The goal with the \\(\\chi^2\\) Test of Independence is to determine if \\(X\\) and \\(Y\\) are related to one another (i.e., if they are dependent on one another)? For example, we might want to know if income level (measured categorically) is related to current smoking status. Since both variables are categorical, we do not have a good way of mapping the behavior of either variable on to a theoretical distribution. But, we can define how we expect the distribution of values of \\(X\\) and \\(Y\\) within our sample to be if they are not related to one another (i.e., if they are independent of one another). Lets use an example to display how we can define the expected behavior of two categorical variables if we assume they are independent of one another. 9.1.1 Income Level and Smoking Status Lets say we have recruited 400 people into a study examining the relationship of income and smoking status. We have measured annual income categorically (&lt;$20k, $20-50k, &gt;$50k) and smoking status categorically (not a current smoker, current smoker). Looking at the sample, we see that 100 people reported an income &lt;$20k, 200 people reported an income $20-50k, and 100 people reported an income &gt;$50k. Further, we see that 100 people reported currently smoking and the other 300 reported not currently smoking. We can start by generating a cross-tabulation we will use as a framework to build our analysis on, like so: The total row and column display the total number of people that fall into each category. The total value in the bottom right thus displays the total number of people in our study. Within the dotted lines represents the number of people who reported each combination of income and smoking. For example, the empty cell in the top left corner represents the number of people who reported an annual income below $20k and who reported not currently smoking. Before we examine the observed disitribution of our data, we can actually map how we expect the variables to be distributed assuming that income and cigarette smoking are not related. Well, if we assume that these two variables are independent (hint: sounds like a null hypothesis), then we would expect the values of both variables would be evenly distributed across both groups. We are able to calculate the expected value in each cell using the equation, \\(\\frac{n_{row}*n_{column}}{n}\\), where \\(n_{row}\\) is the total number of participants in that given row, \\(n_{column}\\) is the total number in that given column, and \\(n\\) is the total number of participants. Lets calculate the top-left cell below: Here we can see that \\(n_{row} = 100\\), \\(n_{column} = 300\\), and \\(n = 400\\). Thus, the expected number of people reporting income less than $20k who dont currently smoke is: \\(\\frac{300*100}{400} = 75\\). We will place the expected value in parentheses in the cell like so: We can repeat this process for each cell, resulting in the following table of expected values: Here, we are assuming that the distribution of income is the same among people who dont currently smoke and those who currently do. Likewise, under our assumption, we are expecting that distribution of current smoking to be the same across each income level (in this case, 75% not current smoking and 25% current smoking). This represents the expected distribution of our two variables under the assumption that they are independent. Once we have established expected values, we can fill in the actual observed values within our sample. The following table now includes the values (not in parentheses) of what we observed in our sample: We can see, right away, that our observations differ from our expected values. We can see that a greater proportion of people making less than $20k currently smoke than expected and a smaller proportion of those making more than $50k than expected. We need a way to quantify this difference. The difference between our expected values \\(E_{row,column}\\) and observed values \\(O_{row,column}\\) within each cell can be understood to represent our signal. This difference represents how different our observed data is from what we expected under our assumption that the two variables are independent. We could try to sum up all of these differences, like so: \\[\\sum{(O_{row,column} - E_{row,column})}\\] This would be the sum of the difference between the expected and observed value in each cell. Interestingly though, this will always equal 0, so it is not a very useful metric. Lets do the calculation by hand to display that it equals 0, for our example table above: \\[ \\begin{align} &amp; (50 - 75) + (50 - 25) + (160 - 150) + (40 - 50) + (90 - 75) + (10 - 25)\\\\ = &amp;-25 + 25 + 10 + (-10) + 15 + (-15)\\\\ = &amp;0 + 0 + 0\\\\ = &amp;0 \\end{align} \\] This is a similar issue we face when we try to calculate standard deviation \\(s\\) for a normally distributed variable and why we have to calculate the variance \\(s^2\\) first. Importantly, if you take the square of any value (positive or negative), the result will be positive. So if you take the sum of several squares, your result will only be 0 if your data perfectly matches your expected values. We can define this new sum like so: \\[\\sum{(O_{row,column} - E_{row,column})^2}\\] Now, when we take this sum, we get a meaningful value, like so: \\[ \\begin{align} &amp; (50 - 75)^2 + (50 - 25)^2 + (160 - 150)^2 + (40 - 50)^2 + (90 - 75)^2 + (10 - 25)^2\\\\ = &amp;-25^2 + 25^2 + 10^2+ (-10)^2 + 15^2 + (-15)^2\\\\ = &amp;625 + 625 + 100 + 100 + 225 + 225\\\\ = &amp;1900 \\end{align} \\] This is much better and still represents the strength of our signal (i.e., difference between observed and expected values)! However, we can notice that the size of this value is also dependent on the size of our study sample. We could have sampled twice as many people with an identical proportional distribution of observed results and the strength of the signal would appear much larger, even though the proportional distribution across groups is the same. As such, it is important that we take a step to standardize our calculation of the strength of our signal. We do this by dividing the difference between each observed and expected value by the expected value. Now, instead of representing the crude difference between observed and expected values, each term represents the difference between observed and expected values relative to the expected value. We can define this mathematically like so: \\[\\chi^2 = \\sum{ \\frac{(O_{row,column} - E_{row,column})^2}{E_{row,column}}}\\] In this instance, we are able to calculate: \\[ \\begin{align} \\chi^2 = &amp; \\frac{(50 - 75)^2}{75} + \\frac{(50 - 25)^2}{25} + \\frac{(160 - 150)^2}{150} + \\frac{(40 - 50)^2}{50} + \\frac{(90 - 75)^2}{75} + \\frac{(10 - 25)^2}{25}\\\\ \\chi^2 = &amp;625/75 + 625/25 + 100/150 + 100/50 + 225/75 + 225/25\\\\ \\chi^2 = &amp;48 \\end{align} \\] We have now calculated a standardized value representing the strength of our signal under the assumption that no signal exists. It turns out that this value corresponds to a \\(\\chi^2\\) distribution with \\((rows - 1)*(columns - 1) = (3-1)*(2-1) = 2*1 = 2\\) degrees of freedom. This is very cool, because it allows us to use this metric to assess the probability of our observed data under our assumption that our two variables are not related to one another. But, this should immediately beg the question: what is a \\(\\chi^2\\) distribution? 9.2 The \\(\\chi^2\\) Distribution Earlier, we learned about how the normal distribution arises from how we understand certain natural phenomenon to randomly occur. A normally distributed variable is one where the mean value is the most likely value to observe, where values closer to the mean are more likely than values further from the mean, and where values less than the mean are equally as likely to occur as those greater than the mean. The standard normal distribution (\\(Z\\)-distribution) is depicted like so: ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .1) ## Let&#39;s now define our normal function using the dnorm() function, where the mean value is 0 and the standard deviation is also 1 y_normal &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y_normal, type=&quot;l&quot;, ylab = &quot;Density&quot;) Interestingly, if we square the values of the \\(Z\\)-distribution, we get the resultant distribution. By squaring the distribution, I mean that we take any point (\\(x\\),\\(y\\)) in the \\(Z\\)-distribution and we map it onto the point (\\(x^2\\),\\(y^2\\)): ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(0, 5, by = .01) ## Now Let&#39;s generate our chi-distribution chi_y &lt;- dchisq(x,1) ## Now let us plot this plot(x,chi_y, type=&quot;l&quot;, ylab = &quot;Density&quot;) We refer to this distribution as the \\(\\chi^2\\) distribution with 1 degree of freedom, or \\(\\chi_1^2\\). It is less intuitive than the normal distribution, because instead of directly describing a natural phenomenon, it is describing the behavior of a normally distributed variable. We can understand that a random variable can be explained by a \\(\\chi^2\\)-distribution if the variable is constructed from the square of a normally distributed variable. Lets examine the distribution more closely. We can see that a value of 0 is the most likely and that values greater than 0 rapidly become much more rare. Since \\(\\chi_1^2\\) is the square of the \\(Z\\)-distribution, negative values are not possible (squared numbers are always positive). By definition, ~68% of observations of a variable assumed to follow the \\(Z\\)-distribution will fall between \\(-1\\) and \\(1\\) (i.e., one standard deviation of the mean). We can plot that like so: ## Let&#39;s create our x-axis, ranging from -5 to 5, with increments of 0.1 x &lt;- seq(-5, 5, by = .01) ## Let&#39;s now define our normal function using the dnorm() function, where the mean value is 0 and the standard deviation is also 1 y_normal &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y_normal, type=&quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(-1,x[x&gt;=-1 &amp; x&lt;=1],1) index_low &lt;- which(x == -1) index_high &lt;- which(x == 1) poly_y &lt;- c(0,y_normal[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.68 When we square a value between \\(-1\\) and \\(1\\), it will result in a value between \\(0\\) and \\(1\\). Both \\(-1^2\\) and \\(1^2\\) are equal to \\(1\\). All values between \\(-1\\) and \\(1\\) will, when squared, result in a positive number even closer to \\(0\\). Thus, since the \\(\\chi_1^2\\) distribution is just the \\(Z\\)-distribution squared, we can understand that ~68% of observations on a variable explained by the \\(\\chi_1^2\\) distribution will fall between \\(0\\) and \\(1\\). We can plot this like so: ## Let&#39;s create our x-axis, 0 to 2. We will do by a tiny increment because we can&#39;t do area under the curve for infinity x &lt;- seq(0, 2, by = .00001) ## We will sample our chi-squared distribution y_chi &lt;- dchisq(x, df = 1) ## We will now plot the distribution plot(x,y_chi, type=&quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, ylim = c(0,5)) ## we want all values of x and y where x is less than or equal to 1 ## The following four lines of code do this poly_x &lt;- c(.00001,x[x&gt;=.00001 &amp; x&lt;=1],1) index_low &lt;- which(x == .00001) index_high &lt;- which(x == 1) poly_y &lt;- c(0,y_chi[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.68 As we can see, the area under the curve for the \\(Z\\)-distribution between -1 and 1 is the same as that for \\(\\chi_1^2\\) between 0 and 1. We can do this for any such set of analogous intervals because of how the \\(\\chi_1^2\\)-distribution is created by squaring the \\(Z\\)-distribution. 9.2.1 The \\(\\chi_k^2\\) Distribution with \\(k\\) Degrees of Freedom If we have a variable \\(X\\) that is assumed to follow the standard normal \\(Z\\)-distribution, then we can understand that \\(X^2\\) follows the \\(\\chi_1^2\\) distribution. However, this is only one example of a \\(\\chi^2\\) distribution. We can define the broader family of such distributions as follows: Let us say we have \\(k\\) variables \\(X_1, X_2,\\dots,X_k\\) that each are independent and follow the standard normal \\(Z\\)-distribution. Let us calculate a new variable \\(Y\\) by taking the sum of their squares, like so: \\[Y = \\sum_{i=1}^{k}X_i^2\\] \\(Y\\) is, then, understood to be distributed according to \\(\\chi_k^2\\), or the \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom. Lets plot several forms of \\(\\chi^2\\) distribution with different numbers of degrees of freedom like so: x &lt;- seq(0, 5, by = .001) y1 &lt;- dchisq(x,1) y2 &lt;- dchisq(x,2) y3 &lt;- dchisq(x,3) y4 &lt;- dchisq(x,4) plot(x,y1,type=&quot;l&quot;,xlab = &quot;X&quot;, ylab = &quot;Density&quot;, ylim = c(0,1)) lines(x,y2, col = &quot;blue&quot;) lines(x,y3, col = &quot;green&quot;) lines(x,y4, col = &quot;red&quot;) Youll notice that as the number of degrees of freedom increases, that the shape of the curve shifts rather drastically. At an intuitive level, while we can understand that values close to 0 are more likely for one standard normal variable \\(X\\), the likelihood of the value of every single one of our \\(k\\) variables \\(X_i\\) being close to 0 is less likely. Additionally, the more values we sum together, the larger our resultant value will be. The important thing to takeaway is that \\(\\chi_k^2\\) describes the probability distribution of a variable \\(Y\\) defined as the sum of \\(k\\) independent squares. Which leads us back to our example 9.3 The \\(\\chi^2\\) Test of Independence Earlier, we discussed how when we want to test if two categorical variables are related to one another, we can generate a cross-tabulation which allows us to calculate: \\[\\chi^2 = \\sum{ \\frac{(O_{row,column} - E_{row,column})^2}{E_{row,column}}}\\] Woah! That is a sum of squares! It is generally quite easy to identify a sum of squares formula because we have a sum (\\(\\sum\\)) of squared values (\\((O_{row,column} - E_{row,column})^2\\)). Importantly, the \\(\\chi_k^2\\) distribution describes the the sum of squares of \\(k\\) normally distributed variables. Well, let us consider the term \\((O_{row,column} - E_{row,column})\\). If our two variables are independent (in our example, if smoking and income are independent), then: the most likely value is 0 (i.e., that the observed count equals the expected count); values closer to 0 appear more likely than values closer than 0; and values less than 0 seem equally as probable as values greater than 0. At a glance, assuming the two variables are independent, it appears that these differences follow a normal distribution. What this means is that this value we have calculated corresponds to a specific \\(\\chi^2\\) distribution! This will allow us to test the probability of our observed data and calculate a \\(p\\)-value. But, before we can do so we must determine the correct number of degrees of freedom to define our \\(\\chi_k^2\\) distribution. As we can see in our equation, we are summing together squares for each row X column combination. That means that we are summing together \\(n_{row} * n_{column}\\) squares, which in the case of our example is 3*2 = 6. Instinctively, we might guess that we want to map this calculated value onto \\(\\chi_6^2\\), or the \\(\\chi^2\\) distribution with 6 degrees of freedom BUT, the \\(\\chi_k^2\\) distribution is in relation to \\(k\\) independent normally distributed variables. We can understand, however, that the series of differences we are summing are not totally independent. If we observe more people in one cell of our table then we can understand that this influences how many people remain to be distributed across the other cells of the table. Let us go through our example table from before to display what is meant by this. Let us begin with the cells of the table being empty, like so: Let us start by inserting the observed number of people reporting an income of less than $20k and who report not currently smoking. Looking at our prior tables, we know that this value is 50: Interestingly, we will now notice that we actually have enough information in our table to identify how many people making less than $20k smoke. Thats because we know that there are 100 people making less than $20k and there only remains one more smoking category through which to distribute them. We can see that the 50 remaining people who make less than $20k must be current smokers. I shall now include this value in the table. I write it in red because we do not need to actually analyze the data to establish this value - we already have enough information (in other words, this value is not independent of our observation in the first cell): Now, we do not have enough information to automatically fill in additional cells. So, we check our data and see how many people reported an income between $20k and $50k and report not current smoking. We see that there are 160 such people in our sample, so we fill that into our table: With this piece of information added to the table, we see that we can now automatically fill in the values for $20-50k current smokers (\\(200 - 160 = 40\\)) and for &gt;$50k not current smokers (\\(300 - (150 + 60) = 90\\)), like so: With these cells filled in, we can see that we can also fill in the value for the bottom right cell by the same process. This final cell can be calculated in many ways, but since its the finall cell we can see that we currently have allocated \\(50 + 50 + 160 + 40 + 90 = 390\\) participants throughout the table. Since our whole sample has \\(400\\) people we can see that the final cell has \\(400 - 390 = 10\\) participants in it, like so: Interesting, while we have 6 cells total, we see that only 2 of the values are able to vary freely. Meaning that once we fill in 2 of our 6 cells, we know the values of the remaining 4 cells. Thus, we can see that we only have 2 degrees of freedom to measure our signal. As such, we will compare our test statistic \\(\\chi^2 = \\sum{\\frac{(O_{row,column} - E_{row,column})^2}{E_{row,column}}}\\) to \\(\\chi_2^2\\) (aka the \\(\\chi^2\\) distribution with 2 degrees of freedom). We can easily calculate the appropriate number of degrees of freedom by calculating \\((rows - 1)*(columns - 1)\\), where \\(rows\\) is the number of rows in our table and \\(columns\\) is the number of columns in our table. In this example we can see this would equal \\((3 - 1)*(2 - 1) = 2*1 = 2\\). Finally, recall before we calculated our test statistic as: \\[ \\begin{align} \\chi^2 = &amp; \\frac{(50 - 75)^2}{75} + \\frac{(50 - 25)^2}{25} + \\frac{(160 - 150)^2}{150} + \\frac{(40 - 50)^2}{50} + \\frac{(90 - 75)^2}{75} + \\frac{(10 - 25)^2}{25}\\\\ \\chi^2 = &amp;625/75 + 625/25 + 100/150 + 100/50 + 225/75 + 225/25\\\\ \\chi^2 = &amp;48 \\end{align} \\] So, now the final step of our test is to compare our test statistic (\\(\\chi^2 = 48\\)) to the \\(\\chi^2\\)-distribution with 2 degrees of freedom, or \\(\\chi_2^2\\). Specifically, we want to know what is the probability of observing a value of 48 or a more extreme value, assuming the null hypothesis is true (that our two variables are independent). This can be depicted in probability language like so: \\[P(\\chi^2 \\geq 48 | \\chi_2^2)\\] We can make this calculation by calculating the area under the curve like so. In order to aid in the visualization of this calculation, the graph I present is very zoomed in so that we can visually see the area being drawn: ## Let&#39;s create our x-axis, 0 to 60 - we are making the scale go to 60 since our test statistic is 48 x &lt;- seq(0, 60, by = .01) ## We will sample our chi-squared distribution y_chi &lt;- dchisq(x, df = 1) ## We will now plot the distribution ## We are zooming in very close to the x-axis because the height of the curve ## at chi^2 = 48 is very close to 0 ## By zooming, we can see the area under the curve we are calculating the area of plot(x,y_chi, type=&quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, ylim = c(0,.0000000001)) ## Now we will shade in the area under the curve greater than 48 poly_x &lt;- c(48,x[x&gt;=48],60) index_low &lt;- which(x == 48) index_high &lt;- which(x == 60) poly_y &lt;- c(0,y_chi[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We can print out our AUC calculation - our p-value AUC ## [1] 4.247269e-12 In this case we see that \\(P(\\chi^2 \\geq 48 | \\chi_2^2) \\approx 4.25e(-12)\\). The \\(e(-12)\\) tells us to place a decimal point and 12 leading zeros before the value, so \\(4.25e(-12) = 0.000000000000425\\). Typically, when we have a value this small, it is convential to simply write \\(&lt; 0.001\\). In this case, \\(P(\\chi^2 \\geq 48 | \\chi_2^2) &lt; 0.001\\). 9.3.1 Formally Defining the Test Generally, we can define the \\(\\chi^2\\) Test of Independence like so. Given two categorical variables \\(X\\) and \\(Y\\), we want to assess if these two variables are related to one another. We start by assuming that they are not (i.e., we assume they are independent). Our null and alternate hypotheses can be defined like so: \\(H_0\\): \\(X\\) and \\(Y\\) are independent of one another. (Knowing the value of one variable does not help you predict the value of the other) \\(H_A\\): \\(X\\) and \\(Y\\) are dependent on one another. We can then create a table representing the cross-tabulation of \\(X\\) and \\(Y\\) (this table is a visual aid for conducting the test). Each row of the table represents a level of \\(X\\) and each column of the table represents a level of \\(Y\\). Thus, the number of rows is equal to the number of levels of \\(X\\) and the number of columns is equal to the number of levels of \\(Y\\). Under our null hypothesis, we assume that \\(X\\) and \\(Y\\) are not related. Under this assumption, we thus can expect values of each variable to be evenly distributed. For each combination of a level \\(x\\) from \\(X\\) and \\(y\\) from \\(Y\\), we can calculate the expected number of people reporting both \\(x\\) and \\(y\\), as follows: \\[E_{x,y} = \\frac{n_x*n_y}{n}\\] Where \\(n_x\\) represents the total number of participants reporting level \\(x\\) from \\(X\\) and \\(n_y\\) represents the number reporting level \\(y\\) from \\(Y\\). After calculating the expected values for each cell \\(E_{x,y}\\), we can compare them to the observed value for that combination of \\(x\\) and \\(y\\), denoted as \\(O_{x,y}\\). With these values established, we can now calculate our \\(\\chi^2\\) test statistic like so: \\[\\chi^2 = \\sum{ \\frac{(O_{x,y} - E_{x,y})^2}{E_{x,y}}}\\] Now, we want to compare our test statistic to the appropriate \\(\\chi_k^2\\) distribution. If we let \\(levels(X)\\) and \\(levels(Y)\\) represent the number of categories each of our variables \\(X\\) and \\(Y\\) has, then we can calculate that \\(k = (levels(X) - 1)*(levels(Y)-1)\\). \\(levels(X)\\) is equivalent to the number of rows in our table (\\(rows\\)) and \\(levels(Y)\\) is equivalent to the number of columns in our table, so we can understand the \\(k = (rows - 1)*(columns - 1)\\). So, the final step is to ask how probable our observed test statistic or a more extreme value under our null hypothesis. The behavior of our test statistic under the null hypothesis is assumed to follow the \\(\\chi_k^2\\) distribution. So, this allows us to ask what the probability of observing our test statistic \\(\\chi^2\\) or a greater value (i.e., \\(\\geq \\chi^2\\)), assuming that our test statistic is assumed to follow the \\(\\chi_k^2\\)-distribution, or: \\[P(\\geq \\chi^2 | \\chi_k^2)\\] This represents our \\(p\\)-value. A value closer to 0 indicates that our observed data is more unlikely under our null hypothesis. Assuming a standard significance brightline of \\(\\alpha = 0.05\\), if \\(p &lt; \\alpha\\), we consider our result to be significant and this represents evidence that our null hypothesis may be incorrect. Under such circumstances, we may choose to reject the null hypothesis that \\(X\\) and \\(Y\\) are independent in favor of the alternative that \\(X\\) and \\(Y\\) are dependent on one another. To summarize the steps, given two categorical variables \\(X\\) and \\(Y\\): First, we calculate the expected value of each combination of \\(X\\) and \\(Y\\), \\(E_{x,y}\\) Second, we calculate the observed value for each combination of \\(X\\) and \\(Y\\) in our data, \\(O_{x,y}\\) Third, we calculate our test statistic \\(\\chi^2 = \\sum{\\frac{(O_{x,y} - E_{x,y})^2}{E_{x,y}}}\\) Fourth, we calculate the number of degrees of freedom \\(k = (levels(X) - 1)*(levels(Y) - 1)\\) Finally, we compare our test statistic to the \\(\\chi_k^2\\)-distribution to calculate our \\(p\\)-value. The table is a visual tool for conducting the test. When computing a \\(\\chi^2\\) test by hand, the table is a useful way of conducting it. 9.3.2 What are the assumptions of the \\(\\chi^2\\) Test of Independence Given two variables \\(X\\) and \\(Y\\), the \\(chi^2\\) Test of Independence can be used to assess the relation of these variables if: \\(X\\) and \\(Y\\) are both categorical. The levels of each variable \\(X\\) and \\(Y\\) are mutually exclusive. In other words, each participant must belong to one and only one level of both \\(X\\) and \\(Y\\). Each observation is independent - in other words, our data comes from a random sample of independent observations. The value of \\(E_{x,y}\\) should be 5 or greater in at least 80% of table cells and \\(E_{x,y}\\) must be at least 1 for every cell. When the final assumption is not met, the test statistic is not well described by the corresponding \\(\\chi_k^2\\) distribution, leading to biased results. Checking the assumptions can be done by 1) examining \\(X\\) and \\(Y\\) to confirm that they are categorical and that levels are mutually exclusive and 2) generating the table of expected and observed values, as was done at the beginning of this text. 9.3.3 Running the \\(\\chi^2\\) Test in R In order to run the rest in R, we must generate a table with our observed values. Let us use the example table above to display: We will generate the table like so: ## First we will define each row row1 &lt;- c(50, 50) row2 &lt;- c(160, 40) row3 &lt;- c(90, 10) ## Next, we will use the matrix() function to create our table ## The matrix function creates an object like a data.frame, but it is a bit simpler matrix &lt;- matrix(c(row1,row2,row3), # First argument is a vector containing each row ncol = 2, # Second argument is stating how many columns the table has byrow = TRUE) # Third argument specifies we are filling table by Row ## We can then define row names and column names rownames(matrix) &lt;- c(&quot;&lt;$20k&quot;,&quot;$20-50k&quot;,&quot;&gt;$50k&quot;) colnames(matrix) &lt;- c(&quot;Not Current Smoker&quot;,&quot;Current Smoker&quot;) ## We can then convert our matrix into a table object, which needs to happen to run the test table &lt;- as.table(matrix) ## Let&#39;s peak at our table really quickly table ## Not Current Smoker Current Smoker ## &lt;$20k 50 50 ## $20-50k 160 40 ## &gt;$50k 90 10 Now that we have generated our table, we can run the \\(\\chi^2\\) test: ## Perform the test chisq.test(table) ## ## Pearson&#39;s Chi-squared test ## ## data: table ## X-squared = 48, df = 2, p-value = 3.775e-11 Like in our example by hand, our test statistic is \\(\\chi^2 = 48\\), we have 2 degrees of freedom, and our \\(p\\)-value is &lt;0.001. The \\(p\\)-value is not identical to what we calculated prior, and this is due to differences in how area under the curve was calculated. Often, the \\(\\chi^2\\) test will be run behind the scenes. For example, the \\(tableone\\) package will automatically run the test when generating a table with categorical variables. 9.4 Other Variations of the \\(\\chi^2\\) Test In this text, we have discussed the \\(\\chi^2\\) Test of Independence. There are two other common variations of the \\(\\chi^2\\) test that use the same general approach: the Goodness of Fit Test; and the Homogeneity Test. Here we shall go over the Goodness of Fit test. 9.4.1 Goodness of Fit Test The goodness of fit test when we want to assess if the distribution of a single categorical variable \\(X\\) matches a pre-defined distribution. For example, lets say \\(X\\) is a three-category variable capturing if someone is right-handed, left-handed, or ambidextruous. Well, let us say that multiple studies have established that approximately 85% of people are right-handed, 13% are left-handed, and 2% are ambidextruous. Let us say we recruit a sample, we can ask if our sample matches this distribution. Our statistical hypotheses are: \\(H_0\\): The distribution of \\(X\\) fits the predetermined distribution \\(H_A\\): The distribution of \\(X\\) does not fit the distribution. We still start by calculating expected values, but now only have one variable \\(X\\). The expected value \\(E_x\\) is equal to the number of people in our sample \\(n\\) times the population proportion for level \\(x\\). So, if we have a sample of 200 people and we know that 85% of people are right-handed, then we can expect that \\(E_{right-handed} = 200*0.85 = 170\\). Like with before, we calculate our \\(\\chi^2\\) value by comparing the expected values \\(E_x\\) with the observed values \\(O_x\\): \\[\\chi^2 = \\sum{ \\frac{(O_{x} - E_{x})^2}{E_{x}}}\\] This test statistic is understood to have \\(k = levels(X) - 1\\) degrees of freedom and we compare the test statistic to the \\(\\chi_k^2\\) distribution. "],["covariance-and-correlation.html", "Chapter 10 Covariance and Correlation 10.1 Covariance 10.2 Correlation", " Chapter 10 Covariance and Correlation In our statistical work, it is often of interest to us to assess the relationship between variables within our sample data. Let us say we have two numeric, normally distributed random variables \\(X\\) and \\(Y\\) which we have measured within our sample, it is often useful to understand how changes in the value of \\(X\\) are associated with changes in the value of \\(Y\\), and vice versa. Capturing the covariance of \\(X\\) and \\(Y\\) and the correlation of \\(X\\) and \\(Y\\) provide us two metrics by which we can measure the association between these two variables. In this text, we will discuss what covariance and correlation are, how they are calculated, what they can tell us, and how we can use R to invetigate them in our data. 10.1 Covariance Covariance is a measure of how two random variables \\(X\\) and \\(Y\\) vary together. For now, we will discuss measuring covariance between two numeric variables \\(X\\) and \\(Y\\). For example, let us say \\(X\\) is age and \\(Y\\) is height. We can understand that for adolescents (lets say people aged 18 and younger), that as age (\\(X\\)) increases, so does height (\\(Y\\)). As age varies (i.e., as adolescents get older), their height varies in a corresponding pattern (i.e., older adolescents are taller than younger adolescents). Likewise, if we encounter a taller adolescent, it is fair to guess they are older than shorter adolescents. In this situation, we can understand that there is a positive covariance between age and height because increases in the value of one are associated with increases in the value of the other. However, let us say we are looking at \\(X\\) is age and \\(Y\\) is height amongst people aged 18 through 65. Assuming people typically stop growing in height by age 18, we can see that among adults, no relationship exists between age (\\(X\\)) and height (\\(Y\\)). Knowing how old someone is provides no information about how tall they are and, likewise, knowing how tall someone is provides no information about how old they are. In such a situation, we can understand the covariance of \\(X\\) and \\(Y\\) is approximately 0! Lets consider one last example. Again, let \\(X\\) be age and \\(Y\\) be height. Lets imagine we are now looking at individuals aged 65 and older, elders. It turns out as people get into their golden years, they tend to get a bit shorter. In this case, as elders age increases, we notice that height decreases. Within this sample, older people tend to be shorter and shorter people tend to be older. In this situation, we can understand that there is a negative covariance between age and height because increases in the value of one are associated with decreases in the value of the other. 10.1.1 Measuring Covariance Mathematically, we refer to the covariance of \\(X\\) and \\(Y\\) with the expression: \\(cov(X,Y)\\). Let us assume that we have a sample of \\(n\\) individuals and we have measured \\(X\\) and \\(Y\\) among all \\(n\\) of these participants. If we let \\(\\bar{x}\\) equal the mean of our \\(n\\) measures of \\(X\\) and if we let \\(\\bar{y}\\) equal the mean of our \\(n\\) measures of \\(y\\), then we can define covariance between \\(X\\) and \\(Y\\), mathematically like so: \\[cov(X,Y) = \\frac{\\sum_{i=1}^n((x_i - \\bar{x})*(y_i - \\bar{y}))}{n-1}\\] What is this equation measuring, precisely? For each participant, \\(i\\), we are calculating \\((x_i - \\bar{x})*(y_i - \\bar{y})\\), where \\(x_i\\) is the value of \\(X\\) of participant \\(i\\) and \\(y_i\\) is the value of \\(Y\\) of participant \\(i\\). Well, \\((x_i - \\bar{x})\\) contains two important pieces of information: 1) how far from the mean value of \\(X\\), \\(\\bar{x}\\), the \\(i^{th}\\) participant is and 2) if the value of \\(x_i\\) is less than or greater than the mean value. We receive this same types of information for \\((y_i - \\bar{y})\\). 10.1.1.1 Positive Covariance Before we discussed how positive covariance captures when two variables increase together - i.e., when greater values of \\(X\\) are associated with greater values of \\(Y\\) (and vice versa). We refer to it as positive because, when this is the case, the value of \\(cov(X,Y) &gt; 0\\). Why would that be so? Let us consider our example of age (\\(X\\)) and height (\\(Y\\)) among youth aged 0 through 18, where we intuitively understand that \\(cov(X,Y) &gt; 0\\). Let us say that the average age in our sample is \\(\\bar{x} = 9\\) years old and the average height is \\(\\bar{y} = 52\\) inches. Let us say that participant \\(i\\) is 5 years old and 40 inches tall. We can then calculate for this participant: \\[ \\begin{align} &amp;(x_i - \\bar{x})*(y_i - \\bar{y}) \\\\ = &amp; (5 - 9)*(40 - 52) \\\\ = &amp; (-4)*(-12) \\\\ = &amp; 36 \\end{align} \\] Notice here, when we multiply two negative values, we get a positive value. This makes sense, we want to calculate a positive value when lower values of \\(X\\) and are associated with lower value of \\(Y\\). We identify these values as negative, because they are both below their respective mean values. Likewise, we want to calculate a positive value when higher values of \\(X\\) are associated with higher values of \\(Y\\). We sample someone else and they are 18 years old and they are 68 inches tall. For this participant we can then calculate: \\[ \\begin{align} &amp;(x_i - \\bar{x})*(y_i - \\bar{y}) \\\\ = &amp; (18 - 9)*(68 - 52) \\\\ = &amp; (9)*(16) \\\\ = &amp; 144 \\end{align} \\] Our equation for covariance is able to capture that people with above average age also tend to have above average height in our sample and that people with below average height also tend to have below average age. Often, we primarily care if \\(cov(X,Y)\\) is less than 0 (negative covariance), equal or close to 0 (no covariance), or greater than 0 (positive covariance). 10.1.1.2 Negative Covariance Likewise, it is good to see how our equation for measuring covariance captures negative covariance. We discussed that negative covariance is when higher values of \\(X\\) are associated with lower values of \\(Y\\) and, thus, when higher values of \\(Y\\) are associated with lower values of \\(X\\). We refer to it as negative, because in such cases \\(cov(X,Y) &lt; 0\\). Let us consider our example of age (\\(X\\)) and height (\\(Y\\)) among the elderly, where we intuitively understand that \\(cov(X,Y) &lt; 0\\). Let us say that the average age in our sample is \\(\\bar{x} = 75\\) years old and the average height is \\(\\bar{y} = 64\\) inches. Let us say that participant \\(i\\) is 68 years old and 66 inches tall. We can then calculate for this participant: \\[ \\begin{align} &amp;(x_i - \\bar{x})*(y_i - \\bar{y}) \\\\ = &amp; (68 - 75)*(68 - 64) \\\\ = &amp; (-7)*(4) \\\\ = &amp; -28 \\end{align} \\] In this instance, we get a negative value. This is because participant \\(i\\) had below average age compared to the sample and above average height. This results in multiplying together one negative value and one positive value. Likewise, we could sample an older participant whose age is 92 and their height is 59 inches. We would then calculate: \\[ \\begin{align} &amp;(x_i - \\bar{x})*(y_i - \\bar{y}) \\\\ = &amp; (92 - 75)*(59 - 64) \\\\ = &amp; (18)*(-5) \\\\ = &amp; -90 \\end{align} \\] Since this participant has above average age and below average height, the value calculated is negative. Thus, if this pattern emerges across the full sample, this will result in \\(cov(X,Y) &lt; 0\\). Of course, there are likely to be individuals who are both of above average age and height or both of below average age and height, resulting in a positive value. That is okay. Our measure of covariance is trying to capture a pattern in the relationship between \\(X\\) and \\(Y\\) - not every participant will fall perfectly into that pattern. 10.1.2 Covariance is a Measure of Association In the examples provided, we can understand that there likely is a causal relationship between age and height. As children get older, they get taller. But we only know that this is causal because we have a specific understanding of how age influences height - we typically do not think of changes in height resulting in change in age. (Or, to be a bit of a philosophy troll - it is possible that our idea of age as a construct is cultural and it is totally possible, that, in another culture, height may be a more salient measure of the aging process then a measure of timemeaning that causality is extremely challenging to identify) Importantly, covariance is a measure of the association of two variables \\(X\\) and \\(Y\\). Covariance does not identify why \\(X\\) and \\(Y\\) vary together, it simply identifies that they do and to what extent. 10.1.3 Measuring and Visualizing Covariance in R Calculating the covariance between two variables \\(X\\) and \\(Y\\) in R is easy, we can just use the \\(cov()\\) function. We are going to use the mtcars data.frame that is included with R. It is a data.frame where each observation is a different car and we have a range of variables about each car, such as the miles per gallon it gets (mpg) and its horse power (hp). I would guess that cars with greater horsepower get fewer miles to the gallon - I imagine a powerful pickup truck gets worse gas milage than a small sedan. However, Id like to confirm this in the data - my understanding is that there should be a negative covariance between a cars miles per gallon and its horse power. I can calculate the covariance in R like so: ## Take the covariance cov(mtcars$mpg, mtcars$hp) ## [1] -320.7321 As we see, we get a value of approximately -321, indicating a negative covariance between these two variables. 10.1.3.1 Visualizing Covariance with Scatterplots -320.7321 is not easy to intuitively understand. Often, it is easier to understand covariance visually. To do so, we can use a scatterplot which plots values of our variable \\(X\\) against our variable \\(Y\\). Each participants observations \\((x_i,y_i)\\) are plotted on the graph. Here, we will introduce the \\(ggplot2\\) package in R. This is a very powerful and useful package for generating visualizations in R. It can be a bit tricky, but we will learn how to use it through experience with it. Let us start by generating a basic scatter plot of miles per gallon (\\(X\\)) and horsepower (\\(Y\\)) by pairing the \\(ggplot()\\) function with the \\(geom_point()\\) function: ## First we need to load the ggplot2 package library(ggplot2) ## Now we will create our visualization ## The first argument is our data.frame ## The second argument defines which variable is on the x-axis and which is on the y ## Finally, by writing &quot;+ geom_point()&quot; we are telling the computer to plot it as a scatterplot ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point() As we can see, higher horse power values tend to have lower miles per gallon (and thus points are concentrated in the top left of the plot). We see that cars with higher miles per gallon tend to have lower horse power. This visually displays negative covariance. However, our plot does not look that nice. So I am going to use some additional features of the \\(ggplot2\\) package to make the plot look nicer: ## We are going to use the labs function to set the name of the X-axis, Y-axis, and title ## We are going to change the appearance using the theme_minimal() function ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point() + labs( x = &quot;Miles Per Gallon&quot;, y = &quot;Horsepower&quot;, title = &quot;Relationship Between MPG and Horsepower&quot; ) + theme_minimal() You can customize many aspects of your scatterplot. One that I want to point out is that ggplot2 has a series of theme functions that can be used to change the appearance. I am a fan of \\(theme_minimal()\\) but there are many to explore, for example: ## theme_classic() is a classic! ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point() + labs( x = &quot;Miles Per Gallon&quot;, y = &quot;Horsepower&quot;, title = &quot;Relationship Between MPG and Horsepower&quot; ) + theme_classic() ## Or we can load themes from the ggthemes library library(ggthemes) ## Warning: package &#39;ggthemes&#39; was built under R version 3.6.3 ## The solarized theme gives some fun flair! ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point() + labs( x = &quot;Miles Per Gallon&quot;, y = &quot;Horsepower&quot;, title = &quot;Relationship Between MPG and Horsepower&quot; ) + theme_solarized() The \\(ggplot2\\) package will come in handy whenever we want to create plots because it allows us to customize many of the features in our plot. In practice, you will learn how to use \\(ggplot2\\) by applying it in your work - there are many features and what you want to plot will shape which features you end up using. The important takeaway here is that scatterplots represent a powerful tool for visually inspecting the relationship between two variables and identifying covariance. Lets plot an example to visually see an example of positive correlation. Below I will plot the relationship between the weight of the vehicle (in tons) and its horsepower. A heavier car likely needs more horsepower to drive and heavier cars (such as trucks) are often desired to have greater horsepower for tasks like towing: ggplot(mtcars, aes(x = wt, y = hp)) + geom_point() + labs( x = &quot;Weight (in tons)&quot;, y = &quot;Horsepower&quot;, title = &quot;Relationship Between Weight and Horsepower&quot; ) + theme_solarized() Here we can see that lighter cars tend to have lower horsepower (concentrated in the bottom left). Heavier cars tend to have higher horsepower. Positive correlation can visually be identified by a trend in points from the bottom left corner to the top right corner. A scatterplot is also useful for indentifying no (or weak) covariance. In the following photo, there are three scatterplots, one displaying positive covariance, one displaying negative covariance, and the last displaying weak covariance: https://careerfoundry.com/en/blog/data-analytics/covariance-vs-correlation/ With positive covariance we visually see a pattern where the data travels from the lower left to upper right. This pattern arises when greater values of \\(X\\) are associated with greater values of \\(Y\\). With negative covariance, we visually see a pattern where the data travels from the upper left to the lower right. This pattern arises when greater values of \\(X\\) are associated with smaller values of \\(Y\\). Finally, with weak covariance, we are unable to visually detect a pattern, indicating that \\(X\\) and \\(Y\\) behave independently of one another. 10.1.3.2 Covariance Matrix Often though, we have a lot of variables in our dataset and we are interested in identifying pairings of variables that vary together. We can generate what is called a covariance matrix, a two-dimensional array where each row represents a variable and each column represents a variable. To do so, we simply use the \\(cov()\\) function and supply our data.frame as the argument. This tells the computer to take the covariance of every pairing of variables in our data.frame. I use the \\(round()\\) function to tell R to round all the values to one decimal, for the sake of visually looking at it! round(cov(mtcars),1) ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 36.3 -9.2 -633.1 -320.7 2.2 -5.1 4.5 2.0 1.8 2.1 -5.4 ## cyl -9.2 3.2 199.7 101.9 -0.7 1.4 -1.9 -0.7 -0.5 -0.6 1.5 ## disp -633.1 199.7 15360.8 6721.2 -47.1 107.7 -96.1 -44.4 -36.6 -50.8 79.1 ## hp -320.7 101.9 6721.2 4700.9 -16.5 44.2 -86.8 -25.0 -8.3 -6.4 83.0 ## drat 2.2 -0.7 -47.1 -16.5 0.3 -0.4 0.1 0.1 0.2 0.3 -0.1 ## wt -5.1 1.4 107.7 44.2 -0.4 1.0 -0.3 -0.3 -0.3 -0.4 0.7 ## qsec 4.5 -1.9 -96.1 -86.8 0.1 -0.3 3.2 0.7 -0.2 -0.3 -1.9 ## vs 2.0 -0.7 -44.4 -25.0 0.1 -0.3 0.7 0.3 0.0 0.1 -0.5 ## am 1.8 -0.5 -36.6 -8.3 0.2 -0.3 -0.2 0.0 0.2 0.3 0.0 ## gear 2.1 -0.6 -50.8 -6.4 0.3 -0.4 -0.3 0.1 0.3 0.5 0.3 ## carb -5.4 1.5 79.1 83.0 -0.1 0.7 -1.9 -0.5 0.0 0.3 2.6 As we can see, each row corresponds to a variable and each column corresponds to a variable. Each value represents the covariance of the two corresponding variables. You will notice that along the main diagnoal (starting from the top-left down to the bottom-right), that some entries display the covariance of a variable with itself. For example, the top-left entry the row is mpg and the column is mpg. We refer to this value as the variance \\(\\sigma^2\\), which we have discussed in prior chapters. 10.2 Correlation As we can see, though, the measurement of covariance is not easy to interpret and it is dependent on the scale of what we are measuring. For example, in the \\(mtcars\\) dataset we measured the covariance between miles per gallon and horsepower and found that \\(cov(mpg, hp) \\approx -321\\). However, if we had measured fuel efficiency in miles per liter, our measure of covariance would change because (even though miles per liter represents the same value as miles per gallon) miles per liter is measured on a different scale (per liter) than miles per gallon. Thus, it can be useful to have a standardized way to measure the association between two numeric variables. This is where we introduce the concept of correlation. Correlation is a standardized measure of covariance which measures the strength and direction of the association between two variables \\(X\\) and \\(Y\\). At the population-level, we denote correlation between \\(X\\) and \\(Y\\) with \\(\\rho_{X,Y}\\) (the Greek letter rho, pronounded row) and it is equal to: \\[\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X * \\sigma_Y}\\] where \\(\\sigma_X\\) is the standard deviation of \\(X\\) and \\(\\sigma_Y\\) is the standard deviation of \\(Y\\). Interestingly, by dividing the covariance by the product of \\(\\sigma_X\\) and \\(\\sigma_Y\\), the correlation is always a value that falls between -1 and 1. 10.2.1 Computing the Correlation Coefficient for a Sample Typically, we do not have the population estimates of covariance and standard deviation, so we need a formula by which we can calculate correlation from a sample. The most common form of correlation to take is called the Pearson correlation coefficient \\(r_{X,Y}\\) and we can calculate it like so: \\[r_{x,y} = \\frac{\\sum((x_i - \\bar{x})*(y_i - \\bar{y}))}{\\sqrt{\\sum(x_i - \\bar{x})^2 * \\sum(y_i - \\bar{y})^2}}\\] This is the equivalent of taking the \\(cov(x,y)\\) and dividing it by the product of measured standard deviations of \\(X\\) and \\(Y\\), \\(s_X\\) and \\(s_Y\\). 10.2.2 Computing the Coefficient in R That is another intimidating equation. The important thing to know is that the correlation takes the covariance of \\(X\\) and \\(Y\\) and standardizes it by dividing it by the product of the standard deviation of \\(X\\) and \\(Y\\). This process of standardization comes up a lot, doesnt it?! It is quite simple though to compute the correlation between two variables in R by using the \\(cor()\\) function. Lets now take the correlation of miles per gallon and horsepower in the mtcars package, like so: cor(mtcars$mpg, mtcars$hp) ## [1] -0.7761684 This results in a standardized value of -0.776. Because correlation can only range from -1 to 1, we can see that this represents a relatively strong negative correlation between \\(mpg\\) and \\(hp\\)! However, it is good to now discuss how to interpret the correlation coefficient. 10.2.3 Interpreting the Correlation Coefficient Whereas covariance can result in any value from \\(-\\infty\\) to \\(\\infty\\), correlation can result in a value from -1 to 1. If \\(r_{X,Y} = 1\\), this indicates a perfect positive linear relationship between \\(X\\) and \\(Y\\). What is a perfect linear relationship? Well, remember in middle school we learned that a line can be defined using the equation \\(y = mx + b\\), where \\(m\\) is the slope of the line and \\(b\\) is where the line crosses the \\(y\\)-intercept? When \\(r_{X,Y} = 1\\), this means that when we plot our scatterplot of \\(X\\) and \\(Y\\), that the points will fall perfectly on a line and that the slope of the line is positive (meaning that greater values of \\(X\\) are associated with greater values of \\(Y\\)). Let us plot some points to display this. I will generate two vectors that have a perfect linear correlation, like so: X = c(1,2,3,4,5) Y = c(2,4,6,8,10) data &lt;- data.frame(X,Y) ggplot(data, aes(x = X, y = Y)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;Y&quot;, title = &quot;Relationship Between X and Y - rho = 1&quot; ) + theme_solarized() Lets just run our \\(cor\\) function to confirm that \\(r_{X,Y}\\) in fact is equal to 1. cor(data$X, data$Y) ## [1] 1 Likewise, when \\(\\rho = -1\\), this means that there is a perfect negative linear relationship between \\(X\\) and \\(Y\\). In such a situation, we can understand that there is a line \\(y = mx + b\\) that perfectly explains the relationship between \\(X\\) and \\(Y\\) and that the slope \\(m\\) is negative. We can depict such an example like so: X = c(5,4,3,2,1) Y = c(2,4,6,8,10) data &lt;- data.frame(X,Y) ggplot(data, aes(x = X, y = Y)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;Y&quot;, title = &quot;Relationship Between X and Y - rho = -1&quot; ) + theme_solarized() ## Let&#39;s print out the calculated correlation to confirm! cor(data$X, data$Y) ## [1] -1 In practice, we are likely to never observe a perfect correlation between two variables. In fact, if you ever do, it is worth asking if you made a mistake, because perfect correlations typically do not occur in social science research! 10.2.3.1 Positive Correlation (\\(0 &lt; r &lt; 1\\)) But, this means that it is important that we know how to interpret other values of \\(r_{X,Y}\\). When \\(r_{X,Y}\\) is between 0 and 1, this means that greater values of \\(X\\) are associated with greater values of \\(Y\\), but that they do not fall perfectly onto a given line. We can use a simple linear regression (using \\(geom_smooth()\\)) to visualize this. It is important to note, however, that this is simply a visual aid and that linear regression is a distinct approach from calculating correaltion. ggplot(mtcars, aes(x = wt, y = hp)) + geom_point() + labs( x = &quot;Weight (in tons)&quot;, y = &quot;Horsepower&quot;, title = &quot;Relationship Between Weight and Horsepower&quot; ) + geom_smooth(method=&#39;lm&#39;, se = F)+ theme_solarized() ## `geom_smooth()` using formula &#39;y ~ x&#39; As we can see, the relationship between weight and horsepower appears to be well-explained by the plotted line (in blue). We can see that the slope is positive because the line goes from the lower left to the upper right. However, we can also see that the points do not perfectly match up with the line, so our correlation is going to be between 0 and 1. Lets check with the \\(cor()\\) function: cor(mtcars$wt, mtcars$hp) ## [1] 0.6587479 Here we have calculated a value of about 0.66, indicating a strong positive correlation between weight and horsepower. We can intuitively understand that if points fell closer to the line, the value of \\(r\\) would be closer to 1 and if points fell further from the line, the value of \\(r\\) would be closer to 0. You can find many rules for interpreting correlation coefficients. Something like: &lt;0.3 indicates no correlation .3-.5 indicates low correlation .5-.7 indicates moderate correlation .7-1 indicates high correlation However, these are guidelines, not hard and fast rules. It can be good to keep them in mind, especially when a method (such as multiple regression) requires that variables be independent of one another. However, interpretation can be challenging and is dependent on the research context. When given methods rely on assessing correlation, we will discuss how correlation is to be interpreted. Importantly, the correlation coefficient does not measure the magnitude of the relationship between \\(X\\) and \\(Y\\), but instead measures the existence of a linear dependency between the two variables. This is a good place to note that Pearsons Correlation Coefficient requires several assumptions be met for it to perform well: The relationship between \\(X\\) and \\(Y\\) is linear in nature There are no severe outliers in the data \\(X\\) and \\(Y\\) are normally distributed variables 10.2.4 What if the Correlation Between Two Variables Is Not Linear in Nature The Pearson correlation coefficient measures if there exists a linear dependency between two variables \\(X\\) and \\(Y\\). By linear dependency, we just mean that there exists a line (\\(y = mx + b\\)) that explains how \\(X\\) and \\(Y\\) vary together. But, it is totally possible that \\(X\\) and \\(Y\\) vary based on a non-linear function. A non-linear function is a line that is not straight, but that describes a formulaic (mathematical) relationship between \\(X\\) and \\(Y\\). For example, you could data that follows a quadratic function \\(y = x^2\\), which looks like: X = c(0,1,2,3,4,5,6,7) Y = X^2 data &lt;- data.frame(X,Y) ggplot(data, aes(x = X, y = Y)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;Y&quot;, title = &quot;Relationship Between X and Y - y = x^2&quot; ) + theme_solarized() Or perhaps a logarithmic function, like so: X = c(1,2,3,4,5,6,7) Y = log(X) data &lt;- data.frame(X,Y) ggplot(data, aes(x = X, y = Y)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;Y&quot;, title = &quot;Relationship Between X and Y - y = ln(x)&quot; ) + theme_solarized() In these example cases, \\(X\\) and \\(Y\\) vary perfectly according to non-linear functions, but, if we check their correlation score, we do not get 1. Let us look at both examples: ## Let&#39;s define a vector X from 0 to 10 X = c(0,1,2,3,4,5,6,7,8,9,10) ## And define y = x^2 Y = X^2 ## Check the correlation cor(X,Y) ## [1] 0.9631427 ## Let&#39;s define a vector X from 0 to 10 X = c(1,2,3,4,5,6,7,8,9,10) ## and define y = ln(x) Y = log(X) cor(X,Y) ## [1] 0.9516624 Because our values of \\(X\\) and \\(Y\\) (in each case) are perfectly described by a non-linear function, we would want to articulate that this is a perfect correlation (with a score of 1). \\(X\\) and \\(Y\\) vary together in perfect unison, defined by a non-linear function - however, Pearsons correlation coefficient assesses how well some linear function fits to the data. In this case, we still got very high correlation scores, but our data is never going to perfectly match some function (linear or not). The important takeaway here is that Pearsons correlation coefficient is not ideal for calculating correlation when the relationship between \\(X\\) and \\(Y\\) is best described by a non-linear function. There are two common correlation calculations that can be used when it appears that there exists a non-linear relationship between \\(X\\) and \\(Y\\). 10.2.4.1 Spearmans Rank Correlation Coefficient Spearmans Rank Correlation Coefficient \\(r_s\\) is a commonly constructed alternative to Pearsons Correlation Coefficient. Instead of comparing the values of \\(X\\) and \\(Y\\) to one another, we instead compare the ranks of each variable \\(R(X)\\) and \\(R(Y)\\). We are checking if changes in the rank of \\(X\\) vary with changes in the rank of \\(Y\\). In simpler terms, a positive rank correlation would indicate that higher values of \\(X\\) correspond with higher values of \\(Y\\) and a negative rank correlation would indicate the higher values of \\(X\\) correspond with lower values of \\(Y\\). It does not matter if the relationship is linear or non-linear - we are just assessing if the rank-ordering of \\(X\\) is associated with the rank-ordering of \\(Y\\). To calculate \\(R(X)\\), each value in \\(X\\) is assigned a value based on its ranking from smallest to largest. The smallest value is assigned 1, the 2nd smallest value is assigned 2 and so on. For example, if we had the following vector of numbers \\(X\\): \\[X = \\{4,12,0,-2,-5\\}\\] Then \\(R(X)\\) would be equal to: \\[R(X) = \\{4,5,3,2,1\\}\\] As we can see, -5 is the smallest value in \\(X\\) so we assign its value in \\(R(X)\\) to be 1. 12 is the largest value in \\(X\\), so it ends up with the largest rank of 5. If there are multiple datapoints with the same value, then the average rank is taken. For example, lets look at the following vector: \\[X = \\{1,2,2,3,4\\}\\] Technically, the 2nd and 3rd entries are equal to one another. They are tied for the second lowest number, which corresponds to a ranking of 2. However, since there are two values, we want these two values to represent rank 2 and rank 3. To do so, we take the average, and assign them both a rank of 2.5. Then the next lowest value will be assigned a rank of 4, like so: \\[R(X) = \\{1,2.5,2.5,4,5\\}\\] All we will do now is apply Pearsons Correlation on the rank variables. So, if we used the following equation to the Pearson coefficient of \\(X\\) and \\(Y\\): \\[r_{x,y} = \\frac{\\sum((x_i - \\bar{x})*(y_i - \\bar{y}))}{\\sqrt{\\sum(x_i - \\bar{x})^2 * \\sum(y_i - \\bar{y})^2}}\\] Then we simply use the following to assess the Spearmans correlation: \\[r_{R(x),R(y)} = \\frac{\\sum((R(x)_i - \\bar{R(x)})*(R(y)_i - \\bar{R(y)}))}{\\sqrt{\\sum(R(x)_i - \\bar{R((x)})^2 * \\sum(R(y)_i - \\bar{R(y)})^2}}\\] Now, a perfect Spearman correlation of 1 indicates that the rank-ordering of \\(X\\) and \\(Y\\) are identical (i.e., the smallest value of \\(X\\) and \\(Y\\) are from the same observation, the second smallest value of \\(X\\) and \\(Y\\) are from the same observation, etc). If the Spearman correlation is -1, this indicates that the rank-ordering of \\(X\\) and \\(Y\\) are perfect opposites (i.e., the smallest value of \\(X\\) corresponds with the largest value of \\(Y\\), the second smallest value of \\(X\\) corresponds with the second largest value of \\(Y\\), etc). We can easily calculate the Spearman coefficient in R using the same \\(cor()\\) function. Here, we will run the Spearman correlation for data points following \\(y = x^2\\). This should result in \\(r_s = 1\\). ## Let&#39;s define a vector X from 0 to 10 X = c(0,1,2,3,4,5,6,7,8,9,10) ## And define y = x^2 Y = X^2 ## Check the correlation, using SPearman cor(X,Y, method = &quot;spearman&quot;) ## [1] 1 All we had to do was specify the argument \\(method = &quot;spearman&quot;\\) and R knew what to do. The \\(cor()\\) function defaults to the Pearson method, so it does not need to be specified. 10.2.4.2 Kendalls Rank Coefficient Kendalls Rank Coefficient takes a different approach to determining correlation. Let us say we have \\(n\\) observations of \\(X\\) and \\(Y\\). We can define measurements from the \\(i^{th}\\) observation with the pair \\((x_i,y_i)\\). Now, we are going to count how many concordant and discordant pairs there are in the data. Lets take two pairs \\((x_i,y_i)\\) and \\((x_j,y_j)\\). If 1) \\(x_i &gt; x_j \\space\\&amp;\\space y_i &gt; y_j\\) or 2) \\(x_i &lt; x_j\\space \\&amp; \\space y_i &lt; y_j\\), then we consider the pair concordant. In this case, one pair has a greater \\(x\\) and \\(y\\) value than the other pair. A discordant pair this therefore any pair in which one has a greater \\(x\\) value and the other has the greater \\(y\\) value. Lets let \\(C\\) be the number of concordant pairs and \\(D\\) be the number of discordant pairs. Lets create two additional variables as well: 1) Let \\(X_0\\) be the number of pairs where the \\(x\\) values are equal (but \\(y\\) values are not); 2) Let \\(Y_0\\) be the number of pairs where the \\(y\\) value is equal (but the \\(x\\) value is not). We represent this version of Kendalls rank coefficient with \\(\\tau_b\\) (the greek letter tau, pronounced the way you would say Owwww! when you stub your toe). The formula is as follows: \\[\\tau_b = \\frac{C - D}{\\sqrt{(C + D + X_0)*(C + D + Y_0)}}\\] Logically, we can see that the numerator is capturing the ratio of concordance to discordance. If every pairing is concordant, then all the observations are in perfect positive rank order. If every pairing is discordant, then all the observations are in perfect negative rank order. If we have the same amount of concordant and discordant observations, \\(C-D = 0\\), this indicates the data is pretty jumbled together and a pattern is not apparent. The equation is dividing by the total number of pairings, with an adjustment to account for ties. We can easily run this correlation in R using the \\(cor()\\) function like so: ## Let&#39;s define a vector X from 0 to 10 X = c(0,1,2,3,4,5,6,7,8,9,10) ## And define y = x^2 Y = X^2 ## Check the correlation, using SPearman cor(X,Y, method = &quot;kendall&quot;) ## [1] 1 10.2.5 Correlation Matrix The last thing we will discuss in this text is the correlation matrix. Like the covariance matrix, we can generate a table that shows correlation between all the variables in our dataset. This can be incredibly useful when we are examining our data and trying to identify if we have any variables that are highly correlated. This is very important when we get into fitting regression models where variables need to be independent. We typically do not want to include two variables as predictors in a model if they have high correlation scores - as this indicates the sampled values are not independent. The \\(cor()\\) function can be used by simply supplying it with a data.frame, like so: ## This will take the Pearson correlation cor(mtcars[,c(1:5)]) ## mpg cyl disp hp drat ## mpg 1.0000000 -0.8521620 -0.8475514 -0.7761684 0.6811719 ## cyl -0.8521620 1.0000000 0.9020329 0.8324475 -0.6999381 ## disp -0.8475514 0.9020329 1.0000000 0.7909486 -0.7102139 ## hp -0.7761684 0.8324475 0.7909486 1.0000000 -0.4487591 ## drat 0.6811719 -0.6999381 -0.7102139 -0.4487591 1.0000000 ## Can also specify alt-correlations cor(mtcars[,c(1:5)], method = c(&quot;kendall&quot;)) ## mpg cyl disp hp drat ## mpg 1.0000000 -0.7953134 -0.7681311 -0.7428125 0.4645488 ## cyl -0.7953134 1.0000000 0.8144263 0.7851865 -0.5513178 ## disp -0.7681311 0.8144263 1.0000000 0.6659987 -0.4989828 ## hp -0.7428125 0.7851865 0.6659987 1.0000000 -0.3826269 ## drat 0.4645488 -0.5513178 -0.4989828 -0.3826269 1.0000000 As youll notice, the main diagonal of the matrix (top left to bottom right) all equal 1. This is because a variable is inherently perfectly correlated with itself. You can also see that the matrix is symmetric, as the same comparisons are made twice (one below the main diagonal and once above). However, this is not fun to read! I want something that will visually help me detect which variables are highly correlated with one another! We are going to use the \\(corrplot\\) library to generate a Correlogram, like so: ## load the library library(corrplot) ## corrplot 0.90 loaded ## save our correlation matrix cormat &lt;- cor(mtcars[,c(1:10)], method = c(&quot;kendall&quot;)) ## run the corrplot function on our matrix corrplot(cormat) Visually, this is much easier navigate. We can see that big blue circles represent positive correlation (approaching 1) and big red circles represent negative correlation (approaching -1). When circles are fainter, it means there is less correlation between the two variables (with no circle indicating no correlation). We can visually see many patterns in our data. For example: we can see that miles per gallon is negatively correlated with the number of cylinders the vehicle has; we can see that the number of cylinders is positively correlated with the amount of horsepower the vehicle has. This plot can be an incredibly useful tool for identifying correlation in your data. While in this example, I have run the function on some categorical data (i.e., cylinders), generally we run correlation on continuous data. Assessing correlation is important in and of itself, but will also be important when implementing other methods going forward! "]]
