[["index.html", "Applied Statistics for the Ascendant Social Science Researcher: A Course Focused on Developing Skills, Building Confidence, and Not Being So Absolutely Confusing and Miserable Chapter 1 Dealing With Uncertainty 1.1 So, Whats Different About This Book 1.2 Is This Book For You? 1.3 The Different Types of Uncertainty 1.4 Uncertainty Determining the Best Methods to Apply 1.5 Uncertainty in How to Implement the Tools 1.6 Uncertainty in Interpreting Our Results 1.7 In Conclusion", " Applied Statistics for the Ascendant Social Science Researcher: A Course Focused on Developing Skills, Building Confidence, and Not Being So Absolutely Confusing and Miserable Charles Marks 2021-10-11 Chapter 1 Dealing With Uncertainty 1.1 So, Whats Different About This Book Hello there! My name is Charlie - I am a quantitative researcher with a special focus on substance use. Before becoming a public health researcher, I was a computer programmer. You may be reading this book because you are taking a class with me or you may be reading this book because you are looking for a little boost in your applied statistics education. In my own PhD training, I was often very frustrated within my statistics courses. Statistical methods were often presented in very abstract terms - we would read about the methods, we would read papers that used the methods, we would go over some examples in class and talk through them, we would discuss the assumptions of the methods, and so on. Too often, the applications of these methods were missing. It sort of felt like learning about a bicycle instead of learning to ride one (as fair warning, I love analogies). At the end of these classes, my peers and I often felt like we still didnt actually know how to do statistics - we could recite assumptions of linear regressions or discuss the steps needed to undertake a mediation study, but we still didnt feel like we knew what we needed to do to actually conduct a statistical analysis from start to finish. One huge issue I saw in many courses revolved around programming. In order to do statistics, you must become a programmer. Whether you are using R or SAS or Stata, coding is an essential aspect of becoming a confident statistician. However, coding often falls to the wayside as we are taught the foundations of statistics. As a result, when we are actually looking at a dataset for a research project or our dissertation or whatever else, we dont actually know what to do. It turns out, we never actually learned how to ride the bike! So, there are two goals in this book: To teach you about the bike To teach you how to ride the bike In the first two chapters of this book, we will focus on learning about the R coding language, what it can do and its key features. This is important because coding is complicated! If we know what R is, what it can do, and what it cant do, it will make learning to apply statistical methods much more straightforward in the long run. It can be pretty painful to be learning brand new statistical methods and a brand new coding language all at once, so these chapters are dedicated to building confidence in navigating and using R. In the subsequent chapters, we will focus on important statistical methods that we can add to our statistical toolkit. These chapters will focus on what these methods are, how they work, key assumptions in how to use, just like a typical stats class. At the same time, these chapters will focus on displaying steps needed to execute these tests using R. Further, assignments at the end of each chapter can be completed to practice using the method and building confidence in using them! The assignments are also designed to give you practice with cleaning and manipulating datasets. When we receive data, it often wont be ready to go - we will need to clean it up and prepare it for analyses. In fact, this activity is the most time consuming part of being a quantitative analyst and I think it should be more present in our applied statistics training. Finally, the assignments will also focus on how to use R to output results of each of these methods so that we can readily add them to our research reports and manuscripts. There are many tasks that can be automatedI was once talking to a peer who mentioned that they hand-write all of their citations. I was mortified!!! I use citation software and it builds my citations for me (I double check them and modify accordingly of course!). In the same way, we can use R to automate some really time-consuming processes and make our lives a whole lot easier. Learning statistics and learning is a huge challenge for many of us as we step into our careers as researchers. Too often, it is confusing, complicated, and painful. While I cannot promise it wont be challenging, I have designed this book to be readily accessible and to help you develop the hard skills to understand and apply statistical methods within your own research. 1.2 Is This Book For You? Who have I written this book for precisely? My target audience with this book are ascendant social scientists, particularly in public health fields, who have found the quantitative side of research intimidating, confusing, daunting, utterly upsetting, or some combination of these and other negative feelings. Overall, I believe this book can be useful to anyone seeking to become an applied statistician and who is looking for a friendlier and more accessible introduction. While I am truly grateful for the amazing statistics books available on the market, a 900-page tome is not always the most inviting place to start. I hope that this book may serve individuals for whom traditional statistics curricula and textbooks have not been a good starting place. There are so many ways to learn this trade and we all have different learning styles - I hope this book can serve you in your research journey! 1.3 The Different Types of Uncertainty Before we dive into the whole doing statistics thing  I wanted to have a discussion around uncertainty and confidence. Uncertainty and confidence are going to rear their heads in many, many ways. First, there will be many moments where you will feel uncertain about what you are doing, where you may not feel confident that what you are doing is the right thing. Part of the purpose of this book is to help you navigate through this uncertainty and build confidence in applying different statistical concepts. Second, there will be uncertainty in how to interpret and convey findings from the statistical tests we will execute in this class (and that you will likely continue to use throughout your research career). Especially in the field of human subjects research, our research is far from objective in nature. There is uncertainty and, just as important as being able to apply statistical concepts in our research, will be navigating the uncertainty of our findings and determining how confident we are that our findings are meaningful. Here I wish to focus on three ways that uncertainty will arise in your work as a statistician: Uncertainty in determining which methods are best to apply for your given research project, Uncertainty in determining how to execute these methods, once chosen, Uncertainty in interpreting the results of the tests that you have executed. Becoming a quantitative social scientist is first and foremost about wading through uncertainty  uncertainty about the best way to address a problem, uncertainty about interpreting what youve done, uncertainty in determining if what you have done has value. Instead of diving right into learning about and applying statistical techniques, I want to provide some space to really talk about statistics, talk about what the art of statistics is about and what it is not. 1.4 Uncertainty Determining the Best Methods to Apply 1.4.1 Your Statistical Toolkit As applied statisticians, we can understand each statistical technique as a tool that is available to us in our toolkit. Lets consider somebody building a house  a hammer has long been an important tool for completing many of the tasks necessary to build a house. But a hammer alone does not build a house. The builder requires a blueprint, materials, and the ability to use the hammer and other tools to take the blueprint and materials and turn them into a house! In the end, it takes all of these components to build a structurally sound house. The vision we have is about the house  the tools we choose are in service to this vision. As statisticians, statistical techniques represent our tools. They are our hammers, nails, saws, and levels. Our research project, in this analogy, is the house. We design the research project (develop our research questions and relevant hypotheses), we gather our materials (collect our data), and then we build our home (test our hypotheses and write a report answering our research questions). The important thing here is that statistical tests must be grounded within our overall study design and must explicitly answer our research questions. The statistical tools are those we choose in service of the vision of our research project. We choose which statistical tools we should employ based on our research question and the data we have available to us. With a house, it is quite easy to see when the craftsman has made an error of some sort. You know that a roof was built well (enough) if, on a rainy day, water does not drip into the house. It can be a bit more challenging, though, to identify major errors within a quantitative research manuscript because it requires that other people or you identify and point out the error (without even getting into how anxiety inducing fearing you have made a mistake is!). That is why it is important to really understand what the tools we have are and how to use them  otherwise, we risk building a house that leaks when it rains. On your journey as a statistician, you will frequently be wondering what the best tool to use within each of your research projects is. This is the first source of uncertainty that you will face. Part of this class will be about helping you build the confidence to wade through this uncertainty. Your ability to identify the appropriate statistical tests will grow as you gain more experience and practice at this art! Much like the carpenter who becomes more skilled at their trade with more practice, it will take time to develop your statistical skillset and toolkit  this is natural and you shouldnt feel like you need to be a master statistician right at the beginning of your research journey. 1.4.2 Adding to Your Toolkit Now, one element of your statistical journey will be adding new tools to your toolkit. You could build a house with a hammer  but, a pneumatic nail gun is a far more efficient alternative that will save you time and protect you from accidentally breaking your thumb with your hammer. For the uninitiated craftsman, walking into a hardware store can feel daunting. There are so many tools available, and it is unclear what like 90% of these tools are even for! While there are no statistician stores where you can go to purchase the latest regression technique, the reality is much the same  there are countless tools at your disposal and it isnt always clear how to use them all or what they are even for. Many of these tools (such as t-tests) are commonplace like the hammer and other tools (such as marginal effects models) may be a bit more daunting to add to your toolkit. The reason I want to talk about this is to point out that there is never going to a be a perfect statistical tool for your given research project. It is quite likely that there will be many statistical tools that are well-suited for your research purposes. Learning about more statistical techniques expands your toolbox and provides you with more options for completing your research project. Each tool comes with its strengths and weaknesses  it is our job as statisticians to wade through uncertainty and choose the tools that are best suited for completing our research project. I even look back at some of my earliest publications and see that I would have done them differently today, now that I have expanded my toolkit. I am still very proud of these papers and think that they are deserving of their place in the academic literature, but I also see that I am on a journey where I am growing and becoming better at my trade. If you are taking this class, you are likely at the beginning of your statistics journey  you are right where you need to be  1.4.3 Defending Your Choice in Tools A big part of navigating the uncertainty in choosing what methods to use is being able to defend why you chose the tools that you did. A big part of the statistics learning journey is being able to defend your choices. This can help you in two ways: 1) explicitly explaining why you chose a specific tool and defending its appropriateness is a great way to build confidence; and 2) if you cannot defend your choice in tools then it might be a sign that you need to go back to the drawing board and come up with a new idea. Every method and every approach has strengths and weaknesses  if you submit your work to an academic journal, there will always be reviewers who ask you questions about these weaknesses. As statisticians, it is important that we do everything we can to maximize the strengths of our methods while minimizing the weaknesses. It is overly ambitious (impossible, I daresay) to think we can run a study free of limitations. There will always be limitations. One thing I ask that you be cautious of is using a method because it is common to do so. This is how it is always done is not a defense of the method itself, but a defense of the field. I simply suggest that you take note of when a method is defended on its merits, or on its acceptance within the research community. I recall certain moments in my own doctoral training where I raised concerns about the inappropriateness of specific statistical tools being employed and the response I received was often I have used this technique in countless papers, it is fine. Perhaps more upsetting to me were moments where senior researchers simply shrugged as a way to suggest that the hierarchy of professor versus student meant my critiques were not worthy of response. The reason I raise these points is to let you know that choosing the right methods is also a matter of negotiation with the other researchers that you work with  often times there will be a power dynamic in which you, the student, are required to defend your choices to a much higher standard than your senior peers are. Being able to defend your positions is going to go a long way in helping you navigate these negotiations with co-authors, peers, and mentors. 1.5 Uncertainty in How to Implement the Tools So, you are writing a research paper and you have identified that you need to run a linear regression to test the relationship between two variables. Amazing! You have an excellent justification for using this method and it matches up perfectly with your research question and your study design. Now the question is  how do you do it? 1.5.1 Learning to Communicate with the Computer in R We will dedicate the first several chapters of this book to learning how to communicate with our computer in R. Doing so is not a trivial task, hence why we are dedicating so much time in this class to practicing coding in R. As you get more experience doing quantitative research, you will start finding that (often) choosing the correct tool is a lot easier than actually implementing the statistical test. For example, when you have a dichotomous outcome of interest, your brain will get in the habit of suggesting logistic regression as an appropriate tool to consider (we will discuss later in the course!). This is not the only tool available in such circumstances, nor is it always best  but it is often a great starting place. Sometimes, the process of picking the right tool will be instantaneous, much in the same way that a carpenter doesnt have to think hard to know that a hammer is the right tool for driving nails into wood. Now, just because the carpenter knows the hammer is the right tool to use does not mean that driving one hundred nails is trivial or easy work. This work is also much easier for the trained hand who has done it before. It is painstaking and takes time, but with practice becomes much easier (though not necessarily easy). Much in the same way, executing our statistical methods is often more work than actually picking out the appropriate methods. That is why we will spend so much time in this class implementing our statistical tools in R. Especially the first time you implement a tool in R, you will be uncertain if you are doing things correctly. Through repetition and practice you will gain more confidence in applying these techniques. Even better is that when you learn how to implement a tool in R, you can actually save the code you used so you can use it again later. That is why this book is full of copy-pasta files  while memorizing code can help you move faster, it is far from necessary to be an effective statistician. But, you do need to be able to know how to implement these tools, be confident that you have done it correctly, and be able to do so in new circumstances. 1.6 Uncertainty in Interpreting Our Results One of the greatest challenges in quantitative research is interpreting your findings. Many of us are accustomed to reading sentences such as Men are significantly more likely to start smoking cigarettes than women or patients who received the treatment had significant reductions in the disease than those who received the placebo. This word, significant, appears a lot. It is sort of this magic holy grail that we often are hoping to see. Some of you might know to look and see if p is less than 0.05  thats when we know we have that significant finding! While the concept of significance holds valuable information (which we will continue to discuss), as we progress through this class, I want to challenge you to think critically about your results and critically about this buzzword. Personally, I refrain from ever using the word significance in my quantitative papers. I have a paper dedicated to this you can find here, but, in short, the word significant isnt very descriptive. It doesnt actually provide much information when it comes to answering your research question. When we focus on finding significance instead of answering our research questions, the quality of our research suffers as a result. There are two things I want you to consider whenever you are reading a paper. The first is that non-significant findings are still meaningful and worthy of reflection. The second is that significant findings are not inherently meaningful in relation to our research question. For example, we could have a significant finding that men are 1% more likely than women to smoke cigarettes  while this is significant, one could argue that men having a 1% greater likelihood of smoking than women is a bit meaningless from a practical standpoint. Now, if you feel you can argue that a 1% increase in risk is meaningful, then I encourage you to make that argument! Often, the culture of significance is that we do not need to argue that a significant finding is meaningful  I encourage you to always reflect on the meaning of your findings in relation to your research question. Similarly, we could imagine another study where we find that men are 3-times more likely (300% greater likelihood) to smoke cigarettes than women but that the p-value is greater than 0.05. We just didnt have a whole lot of power, the sample size was small, but the signal within the sample indicates that men were much more likely to report smoking. While this doesnt meet the bar of significance like the other study, a 300% increased risk of smoking has much clearer intervention and policy implications. 1.6.1 What Can a Statistical Test Tell Us? One of the big things in this class will not only be learning how to select a statistical tool and how to apply it, but to also know how to interpret what it is telling us. Significance is one thing that a statistical tool can reflect upon, but it is always important to learn how to read all the information that a statistical tool returns to us. A logistic regression doesnt just return a bunch of p-values  it returns point estimates and standard errors that we can then translate into odds ratios and confidence intervals. It also provides us information on how well our model fits to the data, which can be important diagnostic information. Each statistical tool available to us returns its own kind of information. Ideally, we will select statistical tools that provide us information that can help us answer our research questions! Part of learning to apply statistical tools will be learning how to navigate all of the information that they return to us. 1.7 In Conclusion In this Introduction, I wanted to introduce myself and explain the reasons I put this book together! As well, I wanted to reflect on the complex ways that uncertainty can raise its head when applying statistics. We have to wade uthrough ncertainty in knowing what statistical tools can help us answer our research questions, in knowing how to actually execute the tools we have identified as appropriate, and in knowing how to interpret the results of using these tools so we can answer our research question. As we move through this book, we will focus on each of these pieces. As we learn a new technique we will learn how it works, when to use it, how to use it, and how to interpret the results. By the end of this class you will have expanded your toolkit with tools and you will develop the skills to know when to use these tools, how to use them, how to interpret their results, and to defend their use in your own research! "],["welcome.html", "Chapter 2 Welcome to R 2.1 In This Chapter 2.2 So, WaitWhat is R? 2.3 What can we tell the computer to do with R? 2.4 Saving Values 2.5 Types of Data 2.6 A Quick Check-In 2.7 Storing Larger Quantities of Data 2.8 Nice Job Everyone 2.9 Worksheet 2.10 Copy Pasta", " Chapter 2 Welcome to R 2.1 In This Chapter In this first chapter, we are going to go over some of the basics of the R programming language. R is a very powerful language for undertaking statistical and computational tasks, but it certainly can be intimidating trying to learn how to use it. And that is totally ok! There may be moments in this class where you are confused, unsure, stressed, incredibly frustrated with your teacher, or a combination of all of these - not to worry! R is a language and learning a new language can be challenging. This book was designed to expose you to the language and to provide you lots of opportunities to practice using it as well! In fact, dont worry too much about memorizing what you see here, you can always open this chapter back up and find the informationwhen you run your analyses for your research, no one is going to ask you if you were able to do it all from memory. 2.2 So, WaitWhat is R? To put it simply, R is a language that you can use to communicate with your computer. By writing sentences in R, you can tell the computer to undertake specific tasks. So, for example, you might want the computer to add or multiply or divide two numbers together. It turns out math expressions can be written in R quite similar to how we are accustomed to. Here you can see code run in R (in this book, the code is displayed in gray boxes and the computers responses follow): 121 + 27 ## [1] 148 121*27 ## [1] 3267 121/27 ## [1] 4.481481 It turns out computers are really good and really fast at doing math! However, it is important to know that computers cannot do everythingfor example, I cannot tell a computer to Do my homework! or to Clean my room! Unfortunately, those are things you are still going to have to do yourself. So, as we are learning to code in R, really, we are actually learning how to communicate with our computer. Computers interpret you very literally, so if the computer isnt doing what you want it means that something is wrong with what youve told the computer Your computer when you give it weird R code So, the question is 2.3 What can we tell the computer to do with R? 2.3.1 Arithmetic As shown above, arithmetic (addition, subtraction, multiplication, etc) is one of the primary things that R can do. We have some important operators, which you are probably already familiar with: \\(+\\), for addition \\(-\\), for subtraction \\(*\\), for multiplication \\(/\\), for division \\(^\\), for exponents \\(()\\), to place an expression within parentheses There are a couple of other arithmetic operators (such as modulo division), but it is something we can add when the time is right. R follows the PEMDAS order of operations, so it is important that we use parentheses to make sure we are telling the computer exactly what we want. For example: 2 + 3/5 ## [1] 2.6 is not the same as (2 + 3)/5 ## [1] 1 Perhaps we want to know what 25 raised to the power of 2 + 3 is. We might try: 25^2+3 ## [1] 628 However, we know it should be equal to 25 raised to the power of 5 (because 2 + 3 = 5), which equals: 25^5 ## [1] 9765625 So, it looks like parentheses are quite important. The computer cannot guess what it thinks we meant, it just reads what we told it to do. To get it right, we can use parentheses and tell the computer: 25^(2+3) ## [1] 9765625 2.4 Saving Values When we run an operation, it can be very useful to save that information so that we can use it again later. With R, you can ask the computer to save information by assigning it a name. You do this by typing the name of the variable, write an arrow (&lt;-) and then the value you want to save. For example, maybe I want to record my grade on each test in my class. We had three tests and I got a 92 on the first one, a 61 on the second, and 96 on the third. I could save them like so: test1 &lt;- 92 test2 &lt;- 61 test3 &lt;- 96 What is very useful now is that I can type the variable names now instead of the numbers. For example, if I want to calculate my average score I could add together all three test scores and then divide by three like so: average_score &lt;- (test1 + test2 + test3)/3 I saved the average score in the variable that I decided to name average_score. Now if I want to view the average score, I can simply type and the computer will tell me the value stored in that variable: average_score ## [1] 83 2.5 Types of Data Here, we see that computers are really good at working with numbers, and R provides a nice language for asking a computer to work with numbers. However, there are several important basic data types to know in R: numeric (i.e., numbers) character (i.e., text) logical (i.e., TRUE/FALSE) and, factor (more on this later) 2.5.1 Numeric Data Numeric data is any data that is stored such that computer recognizes it as a number. As we have displayed above, numeric data is created by typing out numbers. Numeric data can be integers or have decimal points, they can be negative or positive. Here I will create some numeric data to display: n1 &lt;- 2 n2 &lt;- 12.75 n3 &lt;- -22.1 n4 &lt;- 1 + 1 n5 &lt;- 92/(3^4) 2.5.2 Character Data Character data is essentially just text. Perhaps we want to store the name of participants in a study, this can be useful if we have to follow-up with them  we dont want to forget their names, that would be rude! We indicate that we are providing the computer character data by surrounding it in \"\" quotation marks, like so: participant1 &lt;- &quot;Gabriella&quot; participant2 &lt;- &quot;David&quot; participant3 &lt;- &quot;Remi&quot; participant4 &lt;- &quot;Surya&quot; participant5 &lt;- &quot;Mr. Kitty&quot; Interestingly, we can save numbers as characters, for example, I could do something like: var1 &lt;- &quot;33&quot; var2 &lt;- &quot;10&quot; If I do this, the computer will recognize var1 and var2 as characters, not as numbers. If I tried to add them together, the computer would tell me that you cant add together character data. 2.5.3 Logical Data Next, we have logical data. We use logical data when something can either be True or False. As such, logical values can either be equal to TRUE (or T) and FALSE (or F). R recognizes logical data like so: log1 &lt;- TRUE log2 &lt;- FALSE log3 &lt;- T log4 &lt;- F The computer recognizes TRUE and T to be the same thing and FALSE and F to be the same thing. You can use either one. Logical data is different from character data because its not just the word TRUE or the word FALSEour computer actually understands the concepts of TRUE and FALSE in a very similar way that we do. 2.5.3.1 Logical Operators It is important that we learn a few important logical operators. You are probably familiar with some of these from math class: \\(&gt;\\), greater than \\(&gt;=\\), greater than or equal to \\(&lt;\\), less than \\(&lt;=\\), less than or equal to \\(==\\), equal to \\(!=\\), not equal to \\(|\\), OR \\(&amp;\\), AND When we use these operators, we are asking the computer a question. For example, if we type 5 &gt; 3, we are asking the computer, Is 5 greater than 3? and the computer can either tell us TRUE or FALSE. We can see what the computer says: 5 &gt; 3 ## [1] TRUE If we invert it, and ask the computer if 3 is greater than 5 we see that the computer is able to tell us that this is FALSE: 3 &gt; 5 ## [1] FALSE Some of these operators will make more sense once we are working with a bit more data. Here you can see a few examples of these operators and the computers response. You can look at these to try to get a better sense of how these operators work: 5 &lt; 10 ## [1] TRUE 10 &lt; 10 ## [1] FALSE 10 &lt;= 10 ## [1] TRUE 10 == 10 ## [1] TRUE 10 != 10 ## [1] FALSE 11 != 10 ## [1] TRUE The AND and OR operators are specifically intersting cases, but they are quite important, especially later when we are cleaning our datasets. AND is supplied two logicals, if they are both TRUE, then the computer will respond TRUE. OR is similar, it is supplied two logicals and if one of them is TRUE then the computer will respond TRUE. For example: TRUE &amp; TRUE ## [1] TRUE TRUE &amp; FALSE ## [1] FALSE FALSE | FALSE ## [1] FALSE FALSE | TRUE ## [1] TRUE These might not seem useful this way. Perhaps I want to know if I passed both tests for a class. On Test 1 I got a 75 and on Test 2 I got a 61. My teacher told me I need at least a 60 on each test to pass. The following code checks if I passed: test1 &lt;- 75 test2 &lt;- 61 (test1 &gt;= 60) &amp; (test2 &gt;= 60) ## [1] TRUE Here we can see that we strung together several logical operators. Within the parentheses I ask the computer to check if the score of test 1 was greater than or equal to 60 and if the score of test 2 was greater than or equal to 60. With the \\(&amp;\\) I then asked the computer to check if both were true (i.e., if it was true that I passed both tests). Turns out I did, which is quite a relief!! 2.6 A Quick Check-In This might have been a lot of information already! If your brain is feeling a bit frazzled, dont worry, that is pretty understandable. So far, weve gone over some of the things we can tell the computer to do using Rbut we are learning how to use R so we can do statistics and usually we dont just save a couple of numbers and add them together. Usually our advisor or PI or whoever gives us a dataset and then asks us to run some t-tests or regressions or something far scarier. So the next thing we need to learn is how to store larger quantities of data in R  hint, this is what makes R super cool!!! 2.7 Storing Larger Quantities of Data Most of us are really used to working with Excel spreadsheets. A lot of times, when we get a dataset from a study, it is in a spreadsheet format. Spreadsheets have rows and columnsusually, each row represents a participant in our study and each column represents a piece of information about that participant. In R, we have two types of objects that mimic a spreadsheet. 2.7.1 Vectors The first is called a vector. It is equivalent to either a single row or a single column in a spreadsheet. For example, we can make a vector that represents a row. Maybe I did a study where I asked participants for their name, their age, if they drink alcohol (TRUE/FALSE), and how many drinks they have each week. I could create a vector which represents this person in R. I use the c() syntax to create a vector like so: c(&quot;Charlie&quot;,29,TRUE,4) ## [1] &quot;Charlie&quot; &quot;29&quot; &quot;TRUE&quot; &quot;4&quot; Just like with our other data types, we can save a vector like so: vector_1 &lt;- c(&quot;Charlie&quot;,29,TRUE,4) As you can see, this vector has four pieces of data in it. Youll notice that it contains different types of data: the first is a character, the second and fourth are numeric, and the third is logical. Now, you can access different elements in the vector by using bracket notation. For example, if I want to pull out the first element I can write: vector_1[1] ## [1] &quot;Charlie&quot; Similarly I could do the same with the other elements by writing: vector_1[2] ## [1] &quot;29&quot; vector_1[3] ## [1] &quot;TRUE&quot; vector_1[4] ## [1] &quot;4&quot; Now this next example might seem a bit silly, but I could even put a numeric value into a variable and use that to pull out an element, for example: num &lt;- 3 vector_1[num] ## [1] &quot;TRUE&quot; As you can see, because the variable num was assigned the value three, when I ran vector_1[num] it pulled out the third element of the vector. We can also use vectors as if it is a column of a spreadsheet. For example, lets say I am a teacher and my 5 students had three exams. I stored their grades in a vector for each test, like so: test_1 &lt;- c(99, 82, 33, 100, 68) test_2 &lt;- c(44, 54, 65, 78, 99) test_3 &lt;- c(82, 89, 91, 90, 87) If we think of these as columns, we can see that student 1 got a 99 on the first test, a 44 on the second test, and an 82 on the third test. Then student 2 got an 82 on the first, a 54 on the second, and an 89 on the third. So, earlier, we calculated the average grade for three tests for one student. But now we have five students. We could do each student by hand, but imagine a teacher with 100 students! It would take a lot of time to calculate the average score of each test by hand. Well, this is something that R is really good at. You can actually do math on vectors. For example, I can use R to tell the computer to calculate the average for each student by doing the following: average_scores &lt;- (test_1 + test_2 + test_3)/3 average_scores ## [1] 75.00000 75.00000 63.00000 89.33333 84.66667 So, why did that work??? When you tell the computer to add together two vectors, it adds together the first element of each vector to make a new first element. Then it adds together the second element of each vector to make a new second element. Lets look at a couple examples to understand this better: c(1,2,3) + c(1,2,3) ## [1] 2 4 6 c(1,2,3) * c(1,2,3) ## [1] 1 4 9 c(1,2,3) - c(1,2,3) ## [1] 0 0 0 This is incredibly useful when we are building a dataset. Perhaps we have an index where there are 10 questions and the final score is the sum of all the answers. You can just add together the vectors representing each question and get the final score!! Dont worry, there will be examples of that later on! Also notice how in that example we divided the vector by 3 to get the average score. You can also do vector arithmetic with a single number. That number is just applied to every element in the vector. For example: c(1,2,3) + 1 ## [1] 2 3 4 c(1,2,3) * 2 ## [1] 2 4 6 c(1,2,3) / 3 ## [1] 0.3333333 0.6666667 1.0000000 At this point, it is totally understandable if your brain is just like ENOUGH! Like I said earlier, dont worry about memorizing this. There are practice exercises you can do for homework where you can play with this and get more familiar. Coding is kind of like riding a bicycle - a lot of times it only makes sense after you do it for yourself and practice until it feels natural! 2.7.2 Data Frames The second object is called the data frame. This is the most important object that you will use as a statistician. It is pretty much just an Excel spreadsheet. Just like a spreadsheet it has columns and rows. Each column has a name that tells what is in that column. For example, you can have a data frame where the first column contains participants age (you might call this column Age), the second column their gender, the third and so on and so on. Just like with a spreadsheet, each row represents an obseervation of our data. Often times, we will just load a data frame object from a file, like a .csv or an Excel worksheet (or even from a SAS, SPSS, or Stata file). There are ways to create your own data frame too. Here we are going to create our own data frame, but, for the most part you will just be loading a file with your data already in it (you will get to do this a lot in future chapters). 2.7.2.1 Create a Data Frame Here I am going to create a data frame representing a class of 5 students who each took 3 tests. Each test was scored out of 100. So first, I am going to create a vector for each column in our data frame. students &lt;- c(&quot;Amy&quot;,&quot;Bart&quot;,&quot;Cathy&quot;,&quot;Delilah&quot;,&quot;Edwina&quot;) test1 &lt;- c(82,77,100,78,58) test2 &lt;- c(99,100,98,64,61) test3 &lt;- c(80,91,99,81,59) Next, we are going to create our data frame by using the data.frame() syntax. This is similar to the c() syntax for creating a vector. Technically, these are called functions, which we will cover in more detail in the following chapter. We create the data frame like so: data.frame(students, test1, test2, test3) ## students test1 test2 test3 ## 1 Amy 82 99 80 ## 2 Bart 77 100 91 ## 3 Cathy 100 98 99 ## 4 Delilah 78 64 81 ## 5 Edwina 58 61 59 As you can see, this looks really similar to a spreadsheet in Excel! We can see that each row represents an observation (in this case, a student), and each column represents a piece of information about each participant (their name, test scores). Lets play around with this data frame to see how data frames work. First, we are going to save the data frame like so: df &lt;- data.frame(students, test1, test2, test3) 2.7.2.2 Working With a Data Frame So, now that we have saved the data frame, we first need to learn how to access information in the data frame. One way to do this is very similar to how we can access information in a vector. In a vector, we can access the 5th element of the vector by typing vector_name[5]. We use the brackets and the 5 to tell the computer to get the 5th element out of the vector. However, data frames have two-dimensions, rows and columns. So we can use the bracket syntax to access elements of a data frame, but we have to supply two pieces of information: the row number and the column number. If we want to access the element in the 1st row and 2nd column (82) then we would type the following: df[1,2] ## [1] 82 Similarly, we can access an entire row (lets say we want the third row), by only supplying the row number. To do this, we would type the following: df[3,] ## students test1 test2 test3 ## 3 Cathy 100 98 99 Now notice that we still included the comma. If you leave out the comma and just type df[3], the computer will throw an error because it wont understand what you are asking for. Similarly, you can access a column (lets say the 4th column) by typing: df[,4] ## [1] 80 91 99 81 59 You can also access mutliple columns or multiple rows by providing a vector with the row/column numbers you want. Lets say you want the three columns representing the test scores (2 - 4). Well you can tell the computer the following: df[,c(2,3,4)] ## test1 test2 test3 ## 1 82 99 80 ## 2 77 100 91 ## 3 100 98 99 ## 4 78 64 81 ## 5 58 61 59 As you can see, typing it in this way, the computer returns the 2nd, 3rd, and 4th columns back to you. This may come in handy later on when we are cleaning up our data sets  I have worked with some datasets that have thousands of columns and sometimes we only need like 15. It can be nice to subset our data frame to only have those columns. If say we wanted to de-identify our class roster and just have test scores, we could do the following: deidentified_df &lt;- df[,c(2,3,4)] By doing this, we now have a data frame without student names. Sometimes our datasets have sensitive information and it can be good to know how to remove it. There is another useful way to do this as well. You can also tell the computer to remove rows or columns. In the previous example, we told the computer to give us our data frame df, but only with the 2nd, 3rd, and 4th columns. We could also do this via subtraction by telling the computer to give us df with the 1st column removed, like so: df[,-c(1)] ## test1 test2 test3 ## 1 82 99 80 ## 2 77 100 91 ## 3 100 98 99 ## 4 78 64 81 ## 5 58 61 59 In this case, df[,-c(1)] and df[,c(2,3,4)] actually do the same exact thing. Something really important to know is that, often times, there are many many many different ways to tell the computer to do the same thing. What is important is that you do one of the right ways  if someone else does something different than you, that doesnt mean one of you is right and one of you is wrong. The important thing is that the computer does what you want it to do. 2.7.2.3 Using the $ Operator Ok, so this whole [row,column] syntax seems a bit much, right? Like why do the columns all have names if we are just going to be typing in the numbers inside the brackets??? Well, good news! R makes it really easy to communicate with the computer about which columns you want to work with. For example, as we can see, in our data frame df we have four columns whose names are students, test1, test2, and test3. We can access these columns by using the $ operator. We simply type df$ followed by the name of our variable. What is really cool, since we are using RStudio, is that RStudio will usually show you a dropdown with all your variable names! As you type more letters, the dropdown will narrow. So, for example, we can access each column in our data frame as follows: df$students ## [1] Amy Bart Cathy Delilah Edwina ## Levels: Amy Bart Cathy Delilah Edwina df$test1 ## [1] 82 77 100 78 58 df$test2 ## [1] 99 100 98 64 61 df$test3 ## [1] 80 91 99 81 59 What is really cool is that each column can be treated just like a vector! So, since I am a teacher, I need to calculate the average score for each student in the class. We have done this a few times already! So, just like in our vectors example, we can get the average for each student as follows: (df$test1 + df$test2 + df$test3)/3 ## [1] 87.00000 89.33333 99.00000 74.33333 59.33333 But here is where working with R to talk to your computer is pretty cool. Instead of just calculating the average scores, I can actually add them to my data frame by creating a new column! All I have to do is: df$final_grade &lt;- (df$test1 + df$test2 + df$test3)/3 df ## students test1 test2 test3 final_grade ## 1 Amy 82 99 80 87.00000 ## 2 Bart 77 100 91 89.33333 ## 3 Cathy 100 98 99 99.00000 ## 4 Delilah 78 64 81 74.33333 ## 5 Edwina 58 61 59 59.33333 Here, we have created a new column called final_grade and we have assigned the average score of the three tests as its value! Thats pretty darn useful! 2.7.2.4 Creating Categorical Data (AKA Factors) However, as a teacher, I need to assign every student with a letter grade (A,B,C,D,F). I could do this by hand, but again, we want a really easy way to do this  imagine if our dataset had 1,000 students in it!!! It would be awful to do that by hand. In R, we refer to categorical data as factors. And we use the factor() syntax in order do this. These next few snippets of code might be confusing, dont worry at all! The best way to get familiar is to see it and then apply it! In fact, lets go over a little trick that can help with reading and writing code in R. We can actually write comments into our code. Comments are just plain text that the computer does not try to read  its just there to help you understand what is happening in the code. That is because sometimes code isnt easy to translate back into English. In order to write a comment, we simply write a # and then whatever comes after it on the same line is a comment. We can do that like so: ## This is a comment. # So is this ########################### # Here is a fancy comment # ########################### # Don&#39;t worry about the different colors ## R sometimes will try to color code things # Sometimes it helps a lot ## Sometimes it does not /\\(o0)/\\ Previously, we made a new column called final_grade. It is good practice to use comments to tell whoever is reading the code why you wrote that line of code. For example, we can update the code to read: ## Create a new variable called final_grade ## It is the average score of each of the three tests df$final_grade &lt;- (df$test1 + df$test2 + df$test3)/3 Ok ok! Now that we learned how to do that, lets get back to our goal of making a new column with the letter grade for each student. There are multiple steps - dont worry about memorizing them, you can always just come back to this code if you cannot remember. The first step is that we are going to create a new column. We just want to assign it a name (final_letter_grade) but we do not want to put any data in it yet. In R, when an object does not have data assigned to it, the value is called NA. NA pretty much means there is nothing here. So, we can create the new column like so: ## Create new variable called final_letter_grade df$final_letter_grade &lt;- NA ## Print out the df object so we can view our new variable df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 NA ## 2 Bart 77 100 91 89.33333 NA ## 3 Cathy 100 98 99 99.00000 NA ## 4 Delilah 78 64 81 74.33333 NA ## 5 Edwina 58 61 59 59.33333 NA So, as we can see, we have created a new column and it is empty. R uses NA to display that the entries are empty. Next, we are going to introduce some new syntax. We will use the comments to help understand what it is saying. It is totally okay if it is confusing at first. Essentially, we want to tell the computer that if the final grade is greater than or equal to 90, that the final grade should be an A. However, we also want to maintain that the grades are ordered (i.e., an A is higher than a B is higher than a C is). This is why factors are useful. We are first going to start by assigning final_letter_grade a number. In fact, the way I am going to do it, to maintain order, is to assign F to the number 0, D = 1, C = 2, B = 3, A = 4. This will help us tell the computer that not only do the grades have character names (A,B,), but that these letters have a numerical relationship with each other. So, let us just start with the people who got 90 or higher. We want to assign any student with a final_grade greater than or equal to 90 and final_letter_grade numerical value of 4 (because thats the value we are going to match with A later on). We can do this like so: ## Here we assign all students with a final grade &gt;= 90 ## With a final_letter_grade of 4 (which corresponds with an A) df$final_letter_grade[df$final_grade &gt;= 90] &lt;- 4 ## Print the df to see the result df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 NA ## 2 Bart 77 100 91 89.33333 NA ## 3 Cathy 100 98 99 99.00000 4 ## 4 Delilah 78 64 81 74.33333 NA ## 5 Edwina 58 61 59 59.33333 NA Ok ok okwhat is that all about? We can see from the table that that line of code only affected Cathy. We see that she was the only student whose final_grade was greater than or equal to 90. As such, we correctly assigned her a final_letter_grade of 4! So, how exactly did that weird line of code make that happen? The above line of code has three parts. The first is df$final_letter_grade. Here we are telling the computer that we want to assign a value to the final_letter_grade variable. The second part is [df$final_grade &gt;= 90]. Here we are telling the computer that we only want to change final_letter_grade for the students whose final_grade was 90% or higher. The final part is &lt;- 4 which is just telling the computer that for the students who had a final grade greater than or equal to 90, assign their final_letter_grade value to be 4. As we can see, since Cathy was the only student with a final_grade in the 90s, she was the only student assigned the value 4. We can then repeat this for each grade value. ## Here we assign all students with a final grade in the 80s ## With a final_letter_grade of 3 (which corresponds with a B) ## We use the &amp; operator to provide two restrictions ## The lower bound that the grade must be greater than or equal to 80 ## Adn the upper bound that the grade must be less than 90 df$final_letter_grade[df$final_grade &gt;= 80 &amp; df$final_grade &lt; 90] &lt;- 3 ## Here we assign all students with a final grade in the 70s ## With a final_letter_grade of 2 (which corresponds with a C) df$final_letter_grade[df$final_grade &gt;= 70 &amp; df$final_grade &lt; 80] &lt;- 2 ## Here we assign all students with a final grade in the 60s ## With a final_letter_grade of 1 (which corresponds with a D) df$final_letter_grade[df$final_grade &gt;= 60 &amp; df$final_grade &lt; 70] &lt;- 1 ## Here we assign all students with a final grade &lt; 60 ## With a final_letter_grade of 0 (which corresponds with an F) df$final_letter_grade[df$final_grade &lt; 60] &lt;- 0 ## Then lets print out the data frame to see if it worked df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 3 ## 2 Bart 77 100 91 89.33333 3 ## 3 Cathy 100 98 99 99.00000 4 ## 4 Delilah 78 64 81 74.33333 2 ## 5 Edwina 58 61 59 59.33333 0 As you can see we were able to correctly assign the right numerical value to each student! We used our \\(&amp;\\) logical operator to tell the computer we wanted to capture grades within two values. For example, we could tell the computer to find final_grades corresponding to a B by looking for final_grades greater than or equal to 80 AND less than 90. If we just said greater than 80 without including less than 90, we would have accidentally changed Cathys grade. Now there is one final step! We want to assign the numerical values (0,1,2,3,4) to the correct letter grades such that (F = 0, D = 1, C = 2, B = 3, D = 4). To do this we are going to use the factor() function. As we will discuss in more detail in the following chapter, a function takes arguments. An argument is just a piece of data that the function uses to complete its given task. Arguments are separated by commons and sometimes they have names that we have to type. In this case, the factor function receives 3 arguments: 1) a vector of data (in this case, the column of our data frame); 2) the numeric levels of the data (i.e., 0,1,2,3,4); and 3) the labels for each level (i.e., F, D, C, B, A). With this information, the factor function will tell the computer to convert our final_letter_grade column into an ordered, categorical variable. We do this like so: ## We will convert the final_letter_grade variable into a categorical variable ## By applying the factor function to it df$final_letter_grade &lt;- factor(df$final_letter_grade, levels = c(0,1,2,3,4), ## Here we include a vector with the numerical values labels = c(&quot;F&quot;,&quot;D&quot;,&quot;C&quot;,&quot;B&quot;,&quot;A&quot;)) ## Here we include a vector with the names of each level ## We can then print the data frame to confirm the code worked df ## students test1 test2 test3 final_grade final_letter_grade ## 1 Amy 82 99 80 87.00000 B ## 2 Bart 77 100 91 89.33333 B ## 3 Cathy 100 98 99 99.00000 A ## 4 Delilah 78 64 81 74.33333 C ## 5 Edwina 58 61 59 59.33333 F Tada! We have just made an ordered categorical variable! That is super cool! Dont worry if you didnt quite follow or if your head hurts - we are going to get lots of practice throughout this class working with categorical data! 2.7.2.5 Creating a Logical Variable So, we are just about done with this introduction. I want to show you a couple additional things we can now do with our factor. We might want to create a simple logical (TRUE/FALSE) variable indicating whether students passed or failed the class. We are going to call this variable passed, where a TRUE value means they passed and FALSE value means they did not. We can do this like so: ## First we create the variable df$passed &lt;- NA ## Next we assign anyone with an &quot;F&quot; grade a passed value of FALSE ## Be careful, you need the quotation marks around the F, otherwise the computer will think you mean FALSE df$passed[df$final_letter_grade == &quot;F&quot;] &lt;- FALSE ## Finally we assign anyone who did not get an &quot;F&quot; grade a passed value of TRUE df$passed[df$final_letter_grade != &quot;F&quot;] &lt;- TRUE ## We can look at our data frame to confirm that we got it correct df ## students test1 test2 test3 final_grade final_letter_grade passed ## 1 Amy 82 99 80 87.00000 B TRUE ## 2 Bart 77 100 91 89.33333 B TRUE ## 3 Cathy 100 98 99 99.00000 A TRUE ## 4 Delilah 78 64 81 74.33333 C TRUE ## 5 Edwina 58 61 59 59.33333 F FALSE As you can see, we now have a single variable which indicates if students passed or did not pass the class. We could certainly just use the final_letter_grade variable to figure out who passed and who didnt, but sometimes it is useful to take a categorical variable with many values and condense it down to a dichotomous variable. This may come up later when we learn to implement something like logistic regression! 2.7.2.6 Using the summary() function Before we wrap up, I just want to show you the summary() function. In R, you can pretty much put anything in the summary() function and the computer will tell you some information about that object. Sometimes this information is useful, sometimes it is not. For example, we can run summary() on our data frame like so: summary(df) ## students test1 test2 test3 final_grade ## Amy :1 Min. : 58 Min. : 61.0 Min. :59 Min. :59.33 ## Bart :1 1st Qu.: 77 1st Qu.: 64.0 1st Qu.:80 1st Qu.:74.33 ## Cathy :1 Median : 78 Median : 98.0 Median :81 Median :87.00 ## Delilah:1 Mean : 79 Mean : 84.4 Mean :82 Mean :81.80 ## Edwina :1 3rd Qu.: 82 3rd Qu.: 99.0 3rd Qu.:91 3rd Qu.:89.33 ## Max. :100 Max. :100.0 Max. :99 Max. :99.00 ## final_letter_grade passed ## F:1 Mode :logical ## D:0 FALSE:1 ## C:1 TRUE :4 ## B:2 ## A:1 ## As we can see, this gives us some useful information about each column in the data frame. We can see that the average (mean) score of the first test was 79, we can see that 2 students got a B in the class, and we can see that 4 students passed. Sometimes when data frames have a lot of columns, it is really messy to just run the summary function on the whole data frame. lets say you just want to see the final letter grades for the class. You can use the summary function just on this column by telling the computer: summary(df$final_letter_grade) ## F D C B A ## 1 0 1 2 1 As you can see, this tells us that 1 student got an F, 1 got a C, 2 got Bs, and 1 got an A. This can be particularly useful when you have a lot of observations and you want to summarize your data. For example, you might do a study and want to share the descriptive statistics about your sample. Maybe you want to know how many people in your study identified as Male, how many identified as Female, and how many identified as Gender Non-Conforming. The summary function can provide an easy way to access this information! 2.8 Nice Job Everyone Ok!!! That was quite literally so much information. Great job! :) If you are feeling overwhelmed or confused, do not worry at all. Part of learning to code is practicing. And that is why this book is also intended to give you lots of opportunities to practice! 2.9 Worksheet You can find the worksheet that corresponds to this chapter by clicking here. The file is an RMarkdown file which you can open in RStudio. You will be prompted to type in and run code to answer different questions! The goal of these worksheets is to give you the opportunity to practice using the skills we discussed in the chapter. 2.10 Copy Pasta At the end of each chapter in this book as well will be a copy pasta file. This file will contain generalized snippets of code that you can use to undertake the tasks described in this chapter. The more code you memorize, the faster you will be able to work in R - but, the thing is, there isnt a reason to memorize absolutely everything. In fact, it is good to know that when you forget how to do something that you know where to go to look to remember how to do it. The copy pasta file for this chapter can be found here "],["functions-and-libraries.html", "Chapter 3 Functions and Libraries 3.1 Okay, waitWhats a function?", " Chapter 3 Functions and Libraries In the first chapter, we got our first introduction to R. We learned how we can use R to communicate with our computer and ask it to do things that we would rather not do ourselves. Some these instructions are pretty intuitive to us - for example, telling the computer to add a few numbers together is pretty straightforward. However, sometimes we want the computer to do some pretty complicated tasks that require many steps. Many statistical techniques are quite involved and require many computational steps for the computer to undertake - and, yes, someone has to write out these computational steps (HINT: dont worry, that someone is usually not you!). Even just making a summary table of all your variables of interest can involve many steps. If a variable is continuous you need to take the mean and standard deviation, if it is categorical you need to calculate frequencies and proportions. And maybe, while we are at it, we want to stratify our descriptive statistics based on exposure group (control versus exposed), so we also would like to run the appropriate statistical tests (t-tests, ANOVA, chi-squared) and report the p-values. The task I have just described is quite important in public health research. When we write a paper, it is very important that the reader knows who our study sample is as well as key differences between groups of interest within our sample. This information is so important, in fact, that it is typically the first table (TABLE ONE!) of human subjects research papers. Even though descriptive statistics and t-tests arent the most complicated thing to do (if you so desired, you could run them by hand), the computer requires many many instructions to complete this task. For each variable you want descriptives for, the computer has to identify the type of variable (continuous, categorical, etc), then has to calculate means &amp; standard deviations or frequencies &amp; proportions, and, then, if applicable, run appropriate statistical tests. Then after the computer does all of this, it needs to format the information in a way that we can easily read and work with it. If youre thinking to yourself, that sounds like it takes a lot of code to do!!! I would tell you that you are quite correct! The good news is that R has a feature that makes communicating complicated tasks to the computer quite simple and that many people have dedicated a lot of time to writing the code to do these complicated tasks for you already! The important thing though is to learn how this works and how to use them effectively for yourself! In this chapter we will talk about functions, packages, and your library. A function is simply a collection of lines of code that are stored so that, instead of needing to rewrite all of the code every time you want to use it, you can just call the function with a single line of code and the computer will actually run all of the lines of code! A library is a collection of functionsin a way its a cookbook that you can load into R and then use the functions to complete given tasks. Dont worry if this is confusing, it will make sense soon! 3.1 Okay, waitWhats a function? Youve already seen a few functions from the first chapter. Whether it was running \\(c(1,2,3)\\) or \\(summary(vector)\\) or \\(data.frame(vector1,vector2,vector3)\\), each of these were functions. You can usually recognize functions in R because they have two main parts: the first is a name (such as c or summary) and the second part is a set of parentheses containing arguments. The name is actually associated with some code that isnt visible to you (unless you do some digging). When you run \\(summary(vector)\\), the computer doesnt just read summary and think to itself, well, I think my user wants the mean and median of the numbers in this vector. In fact, there are many lines of code associated with the summary function that the computer runs in the background. 3.1.1 A Real World Example of a Function: Buying a Cake To display what a function is, lets look at a real world example. It is my partners birthday in a few days and I am not a very good baker. I go to this bakery that I hear has the very best cakes. In this analogy, I am the coder and the bakery is the computer. So I walk into the bakery and I say to the baker: Please make me a cake, esteemed and talented baker! This would sort of like if there was a \\(bake\\_cake()\\) function in R and I told the computer-baker: bake_cake() And the baker responds: Can do! It will be ready in a few hours. You see, all I had to do was tell the baker to make the cake. It turns out the baker already has the recipe for making cake. That recipe is sort of like R code - it would be pretty frustrating to have to explain to the baker how to make the cake just like it would be frustrating to have to write every line of code for a complicated task. So, I go to the cafe next door and work on some other projects while the cake is being compiledI mean while the cake is being baked! When I return to the bakery, the baker hands me a beautiful, albeit incredibly plain cake - just a rectangular cake with white frosting. Now I look at the cake and my first instinct is to be frustrated because, well, this is a birthday cake and a plain white cake isnt going to cut it. But I realize this is my fault, I didnt tell the baker what kind of cake I wanted (remember, if the computer doesnt do what you wanted, then it means you didnt tell it the right thing!) So, I say to the baker: Wow that is a beautiful cake! Turns out I actually need another cake. This time could you make it a circle, with light pink frosting, dark pink roses around the rim, and in dark pink could you please write Happy Birthday, Angel! Now, in R I could communicate this to the computer-baker in R by saying: bake_cake(shape = &quot;Circle&quot;, frosting_color = &quot;Light Pink&quot;, roses = TRUE, rose_color = &quot;Dark Pink&quot;, message = &quot;Happy Birthday, Angel!&quot;, message_color = &quot;Dark Pink&quot;) In this example, we used arguments to provide more information to the computer to complete the function for us. Arguments are built into a function - so when someone makes a function, they decide what arguments a function can take in. Whoever made the \\(bake\\_cake\\) function made sure to include arguments (i.e., shape, roses, rose_color, etc) that would allow us to specify what kind of cake we want. So I go back to the cafe after making my order and when I return, lo and behold, before me is the cake that I wanted! First, I apologize if this example made it seem like you could use R to tell your computer to make a cake  that was misleading of me and I apologize. Your computer cannot make a cake. More importantly, though, this interaction displays the power of functions in R. Applying statistical concepts often involves running very complicated sequences of tasks - while it is important that we understand how they work, it is far too complicated (and also not a good use of time) to try to write code out for every single step of these procedures. As such, we can use specific functions to undertake complicated tasks with easy to write code. Each function has a unique set of arguments that we can also apply in order to provide information to the computer to complete the task as we desire it! 3.1.2 Picking Functions in R: The read.csv() function In order to follow along with this next section, please download the CSV file by clicking here. If you havent yet, create a folder for following along with this book and save the CSV file there. 3.1.2.1 Finding Functions When we want to complete a task using R, there are a couple of important questions to ask. The first is: Is there a function already built that can do this task? If you have completed this task before, then you just need to find the function from last time you used it. If not, then its time to use Dr. Google (alternatives include flipping through the index of an R coding book or asking your statistics teacher!). For example, maybe your PI gave you a .csv file containing the dataset you want to work with and you want to know if you could load the dataset using R so you can work with it! You type into Google: How do I load a csv file in R? Using this search you find many, many articles with tutorials on how to do so. After clicking a few, you discover that the \\(read.csv()\\) function appears to be the perfect function for undertaking this task. 3.1.2.2 Figuring Out How the Function Works So, a really cool thing about R is that every function has a documentation file. This is a file which tells you all about the function. It tells you what the function is for, it tells you what arguments the functions can take, it tells you which arguments are required and which are optional, and it tells you what information the computer will return to you at the end. You usually can find the documentation file by typing something like read.csv() documentation into Google. However! We are using RStudio so you can find this information without going online. In the bottom right corner of RStudio (unless you changed your settings), there is a pane that has 5 tabs: Files, Plots, Packages, Help, and Viewer. You can view documentation files in the Help tab. So, click on the Help button  there is a little icon of a house, click on that too!. The view in this pane should look like this: In the top right, there is a text entry with a magnifying glass. You can type in the name of the function you want to learn about here. In this case, read.csv. RStudio may try to create a drop down list based on what you type. Either click on the option for read.csv or finish typing read.csv and hit enter. The Help pane will take you the documentation file, which looks like: Hmm, this page has multiple functions on it, but we can see that the one we wanted is the second in the list. Lets just take a look at the function we want and try to decipher what it is saying: This provides us an overview of the arguments that the read.csv function can accept. First, we can see that there is only one REQUIRED argument, the one called file. How do we know that it is required? Because it does not have an \\(=\\) after it. The function is basically saying, I need you to tell me what file you want me to read, because I cant even guess what you want! All of the other arguments are optional. You can tell because the function assigns a default value for each one. For example, if you ran the function without specifying a value for the header argument, then the computer will assume that header = TRUE because that is how the function was written. We can see that all of the arguments besides file are optional. But, it isnt totally clear what these arguments actually mean! So, how do we find out? We just scroll down a little bit and see that we have descriptions of every single argument! How nice! For this function, there are three really important ones to know, pictured here: We can see the following important information: the file argument refers to the name of the file which the data are to be read from the header argument indicates to the computer whether or not the first row of the CSV has column names or not the computer will assume that the first row has column names if you do not specify because the default is header = TRUE the sep argument indicates to the computer what character separates data within the file a CSV file stands for Comma-Separated Values, so it makes sense that the sep argument defaults to a comma You might be seeing that really long description of the file argument and wondering what on earth all of that is about. It is trying to tell you that you need to make sure the file you want to read is somewhere that the computer can find. So, earlier I had you download a CSV file and save it in a place that makes sense. Your computer has a default location it assumes when you start working in R. You can find it by typing the following code: getwd() ## [1] &quot;C:/Users/Charl/OneDrive/Documents/Stats Textbook&quot; Basically, your computer is full of folders and different locations where files can be saved. You can think of each folder as a room  and your computer can only be in one room at a time (this sort of sounds like computers are consciousmaybe they are?). When we type in \\(getwd()\\), we are basically just finding out what room (i.e., folder) our computer is currently in. We call this room the Working Directory - its not super important to remember the name, but thats what the wd in getwd stands for. So now, we want to set the working directory of the computer to correspond to the location where you saved the csv file I had you download. In order to find the correct folder, we are going to look in the pane in the bottom right again. This time, instead of clicking the Help tab, we are going to click the Files tab, which will give you a display that looks like this: Now we are going to navigate by clicking on the appropriate folders to the folder (aka the room) where the CSV is. Just because our display is showing the folder we want does not mean that the computer knows this is the room it is supposed to be in. To tell the computer that this is the right place, click on the More button with the drop down arrow and select the option Set As Working Directory, like so: Now that we are in the correct directory, we can actually load the dataset by using the \\(read.csv()\\), like so: df &lt;- read.csv(&quot;dataset.csv&quot;) df We can take a look and we can see that we now have a data frame object called df and it contains the data from the CSV file! We did not modify any of the optional arguments because it was not necessary to do so! 3.1.3 Breather Time Ok. That was a lot of info. If youre just reading through, this is definitely a good spot to take a little breather, drink some water, stretch it out. We covered a bunch of stuff: what a function is, how to use functions, how to navigate the help pane in RStudio, and how to navigate folders and set the correct working directory! Especially if this is new for you, this might be pretty overwhelming. The good thing is that, when we use R for statistics work, loading data files is almost always the very first step so you will get lots of practice with this. Now, not every data file is going to be in CSV format. You might have an .xlsx file or a SAS file or an SPSS file or something else altogether. A quick Google search will usually tell you how to load these files, but, for simplicity, in the copy pasta file at the end of this chapter, I have included functions for loading common types of data files with R! 3.1.4 Looking Under the Hood: Writing Our Own Function Now that we have a grasp on what a function is and how it works, lets actually write our own function. Dont worry! Writing your own functions isnt common, but, you might just find as you get better at coding in R that writing your own functions will come in handy! As discussed before, functions allow us to tell the computer to do a series of complicated tasks with only one line of code. For example, the \\(read.csv()\\) function has a lot to it - the computer has to open the file, read each piece of data, convert it into a data frame, and then save it. All we had to do, though, was write just one line of code. Thats because somebody else wrote all those other lines of code  theyre just invisible to us. So, to understand better how this works, we are going to write our own function and then use it. We are going to write a function that converts Fahrenheit to Celsius. There is a simple equation for converting a Fahrenheit temperature into celsius: \\(Celsius = (Fahrenheit - 32)*(5/9)\\). Im going to call my temperature converter fahrenheit_celsius_converter. I can create the function like so: ## We will name our new function fahrenheit_celsius_converter ## here we are saying our function will have one required argument called temp ## temp is required because we did not assign it a value with the = sign fahrenheit_celsius_converter &lt;- function(temp){ ## we generate our new temperature value using the formula to convert fahrenheit to celsius new_temp &lt;- (temp - 32)*(5/9) ## We then &quot;return&quot; the new temperature we have calculated ## We use the return function to tell the computer this is the value we want it to spit out, so to speak return(new_temp) } So, now we can see from the above code that we have made a function and it converts a fahrenheit temperature to a celsius temperature. I have used lots of comments to help make sure that a reader can understand what the function does. Lets quickly test out the function to make sure it works. Some well known values are that freezing in Fahrenheit is 32 and freezing in Celsius is 0 and boiling in Fahrenheit is 212 and in Celsius is 100: fahrenheit_celsius_converter(32) ## [1] 0 fahrenheit_celsius_converter(212) ## [1] 100 Super cool! Now we no longer need to have that formula memorized (thank goodenss, right?!)! We can just use our function and the computer will go fetch the correct code! Now lets add a little bit more complexity to our code! Just for fun (isnt this fun :)?) I am even going to throw in a few functions that you havent seen before. Dont worry a bit! This is just to help get you a little more exposure to some cool tools at your disposal. I want to create an argument for this function that, instead of printing out a numeric value, will print out a sentence stating X degrees Fahrenheit is the same a Y degrees Celsius. Heres what that function looks like (there is more than one way to do this): ## Here we define our function ## We have two arguments ## The first, temp, is required and is the temperature (in Fahrenheit) being fed into the function ## The second, text, is optional. Its default value is false. ## When text is set equal to TRUE, the function will print out the text version of our conversion fahrenheit_celsius_converter &lt;- function(temp, text = FALSE){ ## we generate our new temperature value using the formula to convert fahrenheit to celsius new_temp &lt;- (temp - 32)*(5/9) ## Introducing the if statement ## We can use the if statement to tell the computer to do different things based on different parameters if(text == FALSE){ ## Here we are saying that if the text argument is FALSE, just return the converted value ## This is the same as our original function return(new_temp) } ## What is cool about an if statement is that you can follow it with an ELSE IF statement ## This just tells the computer that if the first if statement wasn&#39;t true, to try to do this one instead else if(text == TRUE){ ## So here, we want to print &quot;X degrees Fahrenheit is the same a Y degrees Celsius&quot; ## Where X = temp and Y = new_temp ## First, temp and new_temp are numeric, but we want them to be character types (because we are printing text!) ## We can use the as.character() function to make this conversion like so temp &lt;- as.character(temp) new_temp &lt;- as.character(new_temp) ## So, now I will introduce you to the paste function ## The paste function lets you stick together character objects to make one longer object output &lt;- paste(temp, &quot;degrees Fahrenheit is the same as&quot;, new_temp,&quot;degrees Celsius&quot;, sep = &quot; &quot;) ## notice the term sep = &quot; &quot; ## I am telling the computer to stick together these four pieces of text and to put a space (&quot; &quot;) in between each one return(output) } ## now, there is always the possibility that someone decided to do something silly like set text = 42 ## there are some other ways to catch this but one way is to use one more ELSE statement ## in this case we don&#39;t need to use an IF because there isn&#39;t anything specific that matters, logically ## if we have gotten to this else statement it means something is wrong else{ return(&quot;The text argument needs to equal TRUE or FALSE -- please try again!&quot;) } } Now, dont worry one bit if some of that just went straight over your head. I showed you this so you can see that even really simple tasks can get a bit complicated really easily. Youll also notice that I wrote a lot of comments alongside the code. I encourage you to always comment your code thoroughly. Its not just because someone else might read your code, but let me tell you, if you dont look at your code for a few weeks, when you come back it might look like gibberish. Commenting your code is a great way to make sure you dont forget what on earth you did! And isnt it nice that now we dont have to type all of that out ourselves. Check out how easy it is to use this function with just one line of code: fahrenheit_celsius_converter(32) ## [1] 0 fahrenheit_celsius_converter(32, text = TRUE) ## [1] &quot;32 degrees Fahrenheit is the same as 0 degrees Celsius&quot; fahrenheit_celsius_converter(32, text = 12) ## [1] &quot;The text argument needs to equal TRUE or FALSE -- please try again!&quot; Alright alright alright, thanks for bearing with me! I promise that my intentions are good and that knowing how to make a temperature converting function will help you as a statistician!! The main takeaway is that functions provide us with an easy way to apply very complicated tasks. In later chapters we will learn how to use various functions that allow us to run different methods like regression - you dont have to know how all the machinery on the inside of the function works, but it is important to understand how to use functions and to understand how they function so you can apply them correctly! 3.1.5 So Whats A Library? So far, we have already used some cool functions like \\(c()\\), \\(summary()\\), and \\(data.frame()\\). These functions are built in to R, so to speak. Our library is the set of functions that we have loaded into our R session! When you open R, certain functions are already pre-loaded into our library. These are functions that are generally considered so integral to using R that it made sense to have them ready to go when you start working. 3.1.5.1 How do I see what is in my library? Great question! In order to see what you have loaded in your library, you can start by running the \\(sessionInfo()\\) function like so: sessionInfo() ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.21 Rcpp_1.0.5 lattice_0.20-44 digest_0.6.27 ## [5] MASS_7.3-54 grid_3.6.1 nlme_3.1-152 magrittr_2.0.1 ## [9] evaluate_0.14 stringi_1.5.3 rlang_0.4.9 rstudioapi_0.13 ## [13] minqa_1.2.4 nloptr_1.2.2.2 Matrix_1.3-3 boot_1.3-28 ## [17] rmarkdown_2.11 splines_3.6.1 statmod_1.4.35 lme4_1.1-26 ## [21] tools_3.6.1 stringr_1.4.0 yaml_2.2.1 xfun_0.26 ## [25] compiler_3.6.1 htmltools_0.5.0 knitr_1.30 We have a few headers of interest: attached base package and other attached packages. We see here this word package. A package is simply a set of functions that can be loaded into your library. Packages are designed and written by people  each package usually has some set of specific, unifying functionality that can help you in some way. For example, the \\(stats\\) package contains functions that allow you to easily run regression models and the \\(ggplot2\\) package contains functions which can help you plot data. An easier way to see which packages you have in your R session is to click on the Packages tab in the bottom right pane like so: Here we see a list of A LOT of packages. These are all of the packages currently downloaded onto my computer. If the checkbox next to a package is marked, that means the package has been loaded into my library. If it isnt marked it means that the package is not loaded. In order to use a package, we must load it into our R session. We dont load all of the packages we have at once because that would overload our computer. I wont get into how R works, but the computer doesnt like it when we load lots of information in R (dont worry, for most of our statistics projects, this is not a concern!). There are two ways to load a package into our library. The most common is to use the \\(library()\\) function. We simply add the name of the package we want to load and it will load the package into our library. Lets say I want to load the \\(tidyverse\\) package. I would do that like so: library(tidyverse) Now, you see the computer gave us a bunch of warnings. That can look a little bit concerning, but I would like to point out two different kinds of message the computer will give you. A warning is a message that the computer provides that basically says Hey, I did what you asked me to do, but here is something you should be aware of! An error message is when the computer fails to do what youve asked - when the computer throws an error, that means it tried to do something and it crashed. Anyway! The other way to load a package into your library is to just scroll down to it in the packages pane and just click on the checkbox! 3.1.5.2 What if I want a new package that isnt on my computer? When you first start using R, you wont have very many packages on your computer. You have to download them. Packages are basically just expansion packs for R. Sometimes it can feel overwhelming figuring out what packages to use and, honestly, there are many many ways to complete the same task. Googling and asking other statisticians which packages to download is an important way to find new packages. Now, usually packages correspond with a task you are interested in. For example, if you want to run a mixed effects regression model, the \\(lme4\\) package was designed exactly for that purpose! If you want to run a mediation analysis, one option is the \\(mediation\\) package. How do I know this - a bit of Googling, a bit of asking around, reading academic manuscripts methods sections, reading the supplemental code attached to a manuscript, reading statistics textbooks, watching YouTube statistics tutorials, and, perhaps most importantly, by reading Q&amp;As on Stack Overflow. Stack Overflow is a forum where people can ask questions and the public can respond. You will likely find your answers to your R questions on Stack Overflow (or Reddit, actually), unless you are doing some real niche stuff! I actually would like to share with you one of my absolute favorite packages: \\(tableone\\). We are going to be using this package a lot! This package is for building descriptive tables of your study sample. It is called tableone because Table One in a human subjects research paper is typically the table describing the study sample. I would say this is the most important table in your entire study! If you dont know who is in your study, then, umm, how do you know what youre studying??? So, it turns out that the tabelone package isnt automatically built in to R. We have to download it. We have two options. The first is the click and point way. In the Packages pane, click on the Install button, like so: You can then start to type in the name of the package we want, tableone, in the appropriate box, like so. Notice how it will provide you a dropdown of available packages. These are packages that have been registered so that the computer knows to find them: Type in the full name of the package you want or click on it when you see it. Then you will click the Install button like so: Et voila! You have downloaded the package. You can also install the package by running the \\(install.packages()\\) function like so: install.packages(&quot;tableone&quot;) Usually, when you install, you will get a lot of messages. At the end, you want to see the message The downloaded source packages are in. If you get an ERROR message, that means your computer failed to download the package. Now that we have downloaded the package, we can simply add it to our library by typing in: library(tableone) So, thats pretty darn cool! The cool thing about R is that all the expansion packs are free!!! 3.1.5.2.1 Quick word of caution R is awesome, in part because it is free. It is an open source software which means that anyone can write packages for R and register them. That doesnt mean that they are inherently untrustworthy, but it does mean we need to develop a spidey-sense for when a package might be suspect. You know how on Wikipedia, sometimes there is a message that says something like This article doesnt appear to have sources - well, it is important to be able to always ask if a package is a good one for what you want to do. The nice thing about R is that there is a huuuuuggeee yuuuuuggeee community of users online that discuss these things. Some packages are so commonly applied that the statistics community has come to trust in them. Code was written by people and people make mistakesremember, the computer just does what you tell it to do, if you tell it the wrong thing then itll do the wrong thing. The takeaway of me telling you this is that when you use a new package, do a little vetting. Ask your stats professor, ask the internet, read up about it! One hint I will give you (and let me tell you this is not a rule by any means) is to look at the documentation files for the package you are considering to use! The more useful this file is, the more likely this is a good package. If you open a documentation file and it barely tells you anything  run! 3.1.5.3 Lets Use the TableOne Package real quick Ok ok ok, we are almost done for this chapter! Now that we have added the tableone package to our library, lets see it in action! Remember that earlier we downloaded a dataset and saved it as \\(df\\)? Well, it turns out that this dataset was taken from a study "],["mathematical-notation-probability-distributions.html", "Chapter 4 Mathematical Notation, Probability, &amp; Distributions 4.1 Statistics: Identifying the Signal From the Noise 4.2 Intro to Probability and the Normal Distribution 4.3 Conclusion", " Chapter 4 Mathematical Notation, Probability, &amp; Distributions Before we dive into learning and applying statistical tools, we need to learn about some important concepts. This chapter will likely be the most mathematically intensive in this book, but, I can promise that understanding the material here will make actually learning about different statistical methods throughout this course far far far more intuitive. In this chapter, we will discuss how statistics is all about assessing probability. Further, we will formally introduce some mathematical language we can use to discuss probability and we will introduce theoretical distributions and how we can use them to think about probability. In this chapter, we will specifically focus on introducing and understanding the normal distribution, one of the most important distributions in the field of statistics. 4.1 Statistics: Identifying the Signal From the Noise One of the primary goals in this book is to help get you comfortable with inferential statistics techniques. Inferential statistics are methods where we take data from a sample of \\(n\\) people and then try to learn something about the broader population of people we think they represent. When we talk about an arbitrary number of people, we will often use the the letter \\(n\\) when we dont know what that number actually is (or when the exact amount doesnt matter). A population represents a broad category of people we want to learn about and a sample represents a subset of that population that we recruited for a study. The reason inferential statistics are so important is because usually you cannot survey every person in a population of interest. Perhaps you want to do a study focusing on people who vape in the United States - well, that is quite literally millions of people. Surveying millions of people is impractical or impossible. However, it is more practical to recruit \\(n = 1,000\\) people who vape or \\(n = 10,000\\) people who vape. With inferential statistics methods, we can take information about our sample of \\(n\\) people who vape and try to make conclusions about the broader population of people who vape. This is our main mission when employing inferential statistics techniques - take information from a sample to learn about a corresponding population. These inferential conclusions are essentially probabalistic guesses! These methods allow us to ask, How probable do we think it is that the signal we have observed in our sample is representative of the broader population? Here, signal just refers to any effect or difference that we may observe. For example, in our (hypothetical) study of people who vape, we might find that twice as many men in the sample report using their vape while at work compared to women in the sample. This difference between men and women in the sample represents a signal - the objective of inferential statistical methods is to help us determine how probable we think it is that the signal within the sample represents a meaningful, non-random pattern within the overall population. In the case of our example, how probable do we think it is that men who vape are actually more likley than women who vape to vape while they are at work? One of the hardest parts about trying to identify these signals is wading through the noise in our data. We can think of noise as variability within our data that is simply the result of randomness. Even if there exists a signal in our overall population, when we run a study we intend to recruit a random sample - a sample is considered random when every person in the population has the same probability of being chosen. This randomness introduces variability into the data that may obscure signals we might observe. For example, lets say we want to know how likely it is that if we flip a coin we get heads. Intuitively, we imagine it is 50-50, that 50% of all coin flips should be heads and 50% should be tails. So, we flip a coin once and get heads. We flip it again and getheads?!?! In fact we flip 5 heads in a row before we get a tails! We flip the coin 100 hundred times and we end up flipping heads 72 times. Now, I promise you, the reader, that there is a 50% chance of getting heads (assuming this isnt some trick coin or some quantum thought experiment, you smart alec), however, because every coin flip is independent (i.e., not dependent on any other coin flip) and random, the reality is that we have flipped heads 72% of the time! This represents our signal. Because we have taken a random sample of coin flips, there is natural variation in the results we have observed - this variation is the noise. Noise, natural variation in our sample data as a result of randomness, obscures the signals that can tell us about our population. So, our goal in inferential statistics is to take a sample and try to learn something about the population we believe they represent. In order to do so, we must wade through noisy data in search of meaningful, probable signals. While by no means a rigorous equation, our ability to make conclusions about the population is dependent on the ratio of signal to noise, or: \\(\\frac{signal}{noise}\\). Often we assume that no signal exists at the population-level and then try to assess how probable our observed data is under this assumption. With a fraction, a big numerator (i.e., the top) means the overall number is bigger (i.e., \\(\\frac{4}{3} &gt; \\frac{2}{3}\\)). A bigger denominator (i.e., the bottom) means the overall number is smaller (i.e., \\(\\frac{2}{5} &lt; \\frac{2}{3}\\)). So, we can understand that the bigger (or stronger) our signal in our sample, the less probable we think it is that no signal exists at the population-level (or, the more probable we think it is that a signal does exist). Noise can be understood to obscure signals we may observe in our data (i.e., weaken signals) 4.1.1 Statistics and Falsification So, statistics represents a broad set of methods through which we can take data from a sample, search for signals amidst the random noise, and assess how probable we think it is that the signal we are observing is representative of the broader population! Importantly, statistics isnt about figuring out what is true or false. Statistical methods were developed because we really cannot know what is true  instead we can reflect on what is probable. If something is not probable, we could assume some alternate reality is true, a sort of logic of contradiction. We refer to the act of determining something is improbable as falsification. For example, we could never prove that men vape at work more than women do, but we could observe data and feel confident that it is probable that men and women dont vape the same amount at work - we can seek to falsify that possibility by examing our data. In statistics we do this by assuming a signal does not exist and then asking how probable our observed data is under this assumption - a confirmation by contradiction, of sorts. For example, we could assume that men and women who vape, vape the same amount at work. Then we could collect data and ask, if we assume that men and women vape the same amount at work, how likely is the data we have observed? Lets say we collect data about vaping at work and find that men vape at work 2% more than women - if we assume that men and women, generally, vape the same amount of work, does this signal in our sample seem probable? Sure! - 2% doesnt seem like a very big difference. Maybe we just happened to interview a couple men who vape at work a lot (e.g., noise). Or, what if in our sample, men smoke at work 300% more than women? This observation seems way more unlikely if we assume no true signal and we feel more confident rejecting our assumption that men and women vape the same amount at work! This is the logic we will be employing when we use inferential statistics techniques - we will assume that a signal doesnt exist and then observe data and assess how probable our observed data is under our assumption of no signal. This represents a logic of contradiction. We make an assumption and then ask how likely our observation are under that assumption. If the observations are unlikely, this provides evidence that our assumption may be wrong and then we reject our assumption. Almost every inferential statistical test involves three primary steps: the first is to assume that a signal does not exist in the overall population (e.g., assume that men do not vape at work more than women do); the second is then to measure and quantify the signal and the noise within the sample; and the final step is to ask how probable it is that we could observe the patterns in our data assuming that no such signal exists. To apply this to our coin flipping scenario: first, we assume that there is a 50% chance of flipping heads any flip; then we flip a coin \\(n\\) times and calculate how often we got heads; and, finally, based on the data, we ask how probable it is that we could have observed the sample (i.e., the coin flips) assuming that the chances of flipping a heads was 50-50. So, if we flip a coin 100 times and get heads 72 times, do we still feel confident that there is a 50% chance of getting heads? 4.1.2 Statistics: The Art of Making Educated Guesses Perhaps this was all a long-winded way of saying that statistics is the art of making educated guesses based on the data we have available to us. This is, in fact, a very human activity! We do it all the time! Every day we make probabalistic decisions based on information available to us. A classic example is which way should I drive home to avoid traffic? You usually cant know the best way, but from experience (your sample), you decide the route (usually dependent on the time of day and which routes are available). You choose the route you think has the highest probablity of being fastest. Now, statistics can feel scary because it is often presented as a bunch of mathematical equations and weird distributions and there are lots of Greek letters and tablesand lets be honest, does anyone even know what degrees of freedom even are? (Hint: You! You will know very soon, I promise!) I definitely dont whip out a calculator and do some mathematical calculations to decide which way to drive home (even Google Maps cannot predict a car crash before it happens). In this chapter, I want to go over some of this scary math stuff because its all just ways of presenting probability in formal and testable terms. So, as we dive in, I want to assure you that you are familiar with the logic behind probability - understanding how we represent probablity and probabilistic decision-making in statistics will actually make understanding the statistical methods way way way easier. 4.2 Intro to Probability and the Normal Distribution Statistics is all about assessing the probability of our observed data given some assumption. As such, we need ways to formally think through the concepts of probablity. 4.2.1 Defining Probablity Mathematically We need a way to express the following question mathematically: What is the probability that [insert phenomenon] will occur? For example, we might wish to ask, what is the probability that the result of our next coin flip will be heads? If we let \\(A\\) represent our next coin flip, we then want to ask, What is the probability that \\(A = heads\\)? We can use \\(P(x)\\) notation to achieve this statement mathematically. \\(P(x)\\) can simply be translated as The probablity of \\(x\\). So, if we were to write \\(P(A = heads)\\), we would read that as saying that The probability that our next coin flip will be heads is. Intuitively, we know that \\(P(A = heads) = .5 = 50\\%\\). We would read this as saying The probability that our next coin flip is heads equals 50%. We can also chain together multiple phenomena and ask how likely the combination of outcomes is. For example, we could ask \\(P(A = heads\\) \\(OR\\) \\(A = tails)\\). Here we are just asking what the probability is that the coin flip will be either heads or tails - we can see that since those are the only possibilities for a normal coin that \\(P(A = heads\\) \\(OR\\) \\(A = tails) = 1 = 100%\\). 4.2.1.1 Conditional Probablity At the heart of inferential statistics is the concept of conditional probablity. Conditional probability comes in handy when we want to ask, Assuming that \\(A\\) is true, what is the probablity of \\(B\\) occurring? Here \\(A\\) and \\(B\\) simply represent phenomenon or circumstances. We can write this mathematically, like so: \\(P(B|A)\\). We would read this as The probability of \\(B\\), given that \\(A\\) is true. Now, \\(A\\) does not actually have to be true, it can be entirely hypothetical. For example, someone could ask you if the freeway is the fastest route to get to your house - let \\(B\\) represent the freeway route to your home. Now, (hypothetically) you know that the freeway is the fastest way except during rush hour. During rush hour, you have found that the freeway is fastest only 1/3 of the time. So, let \\(A\\) represent whether or not it is rush hour. We could ask \\(P(B|A = not\\) \\(rush\\) \\(hour)\\). Well, from experience we have found that the freeway is always fastest when it is not rush hour so, \\(P(B|A = not\\) \\(rush\\) \\(hour) = 1 = 100%\\). Likewise we could ask \\(P(B|A = rush\\) \\(hour)\\). From experience, we have found that \\(P(B|A = rush\\) \\(hour) = 1/3 = 33.\\bar{3}\\%\\). Why is conditional probability so important in inferential statistics? In a quantitative study, we will observe some data (i.e., our study sample). We will also assume that a signal does not exist (e.g., men and women who vape, vape the same amount at work). So, we will try to ask \\(P(data|no\\) \\(signal)\\) - or, what is the probablity that we observed our data assuming that no signal exists? That is the foundation of every single inferential statistical method that we will employ, for example: If we want to know if smoking cigarettes leads to lung cancer, we would 1) assume that smoking cigarettes and lung cancer are not related, 2) observe data from a sample (perhaps ask people if they smoked and if they had lung cancer), and 3) then assess how probable our data is given our assumption that smoking cigarettes and lung cancer are not related. The probablity of data seems like a funny concept. Let us say we assume that men and women who vape do so the same amount at work. We recurit a sample and ask how often they vape at work and then we compare responses of men and women. If we assume that men and women vape the same amount at work, then we imagine it is quite probable that men and women report vaping at work at similar rates. However, its almost certain that men and women in our sample wont have identical vaping patterns at work - thus, it is important that we be able to capture probabilties of discrepancies between our observed data and our assumption. For example, if we think that men and women vape the same amount at work, it seems that it would be quite probable that in our sample we find that men, on average, vape at work 0.2 times more per workday than women - perhaps we randomly sampled a couple of men who vape more than others. Whereas, it might be quite improbable, assuming men and women vape the same amount, if we found that men vape 10 times more per workday than women - such an improbable finding might force us to question if our initial assumption was correct. So, we need a way to assess the probability of our data given some underlying assumption. 4.2.1.2 Introducing Theoretical Probability Distributions We do so by employing theoretical probability distributions. A probability distribution is a mathematical function that identifies the probability of a given outcome occurring. While distributions are sometimes a primary point of confusion in statistics, the reality is that distributions are just a way of capturing the way we think about probabilities - they first come from our intuition about the world, the math is just a way of making it rigorous. Essentially, a probability distribution is a tool by which we can make assumptions about the behavior of a given variable. For example, lets imagine we are playing a game where we are guessing the height of the next person to walk in the room - we dont have any information prior to making our guess. Well, our best guess is probably the average height of all peoplelets say we are pretty sure the average person is 5 foot 8 inches tall. We are also pretty sure that there are just as many people shorter than 58\" as there are taller than 58\" (i.e., the distribution of height is symmetrical around the mean). Further, we are quite positive that most peoples heights are around 58\" - we feel confident that most people are between 52\" and 62\". It is quite rare for someone to be shorter than or taller than that range. So, we have constructed a theory of the distribution of height in order to play this game. We think that if someone walks into the room (i.e., a random observation), that they are most likely to be of average height (58) or close to that height (whether taller or shorter). Further, we think that heights far away (way shorter or way taller) than 58 are the least likely to be observed. We can actually capture this probability by plotting it as a mathematical function like so. We will have the x-axis be height (in inches) and the y-axis will represent the hypothetical probability of observing that height if someone walked in the door: ## Let&#39;s create our x-axis, ranging from 4&#39;4&quot; (52 inches) to 7&#39;0&quot; (84 inches) x &lt;- seq(52, 84, by = .1) ## We will define probabilities using the dnorm function ## The dnorm function generates the points that correspond to a normal distribution ## We will set the average height to 5&#39;8&quot; (68 inches) with a standard deviation of 4 inches y &lt;- dnorm(x, mean = 68, sd = 4) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) As we can see, this mathematical function represents our idea of the distribution of height among a random sample (i.e., the people walking in the door). The height of the curve represents how likely we believe it is that that value will be observed. We can see that 68 inches is the most probable height to observe (it is the tallest point of the curve), that heights nearer to 58\" are relatively likely, and that heights further away from 58\" are rarer. Further, we dont have any reason to think there are more short people than tall, or vice versa, so the distribution is symmetrical around the mean. This curve is referred to as the normal distribution and it is the first and most important distribution we will encounter in our statistics work. We will often assume that an outcome follows the normal distribution. So, as we can see, this normal distribution sort of captures our intuition around how we believe height is distributed amongst the population. 4.2.1.3 Assessing Probabilities Using Theoretical Distributions One of the really cool parts of theoretical probability distributions is that we can ask how probable a range of outcomes are, assuming that the distribution is correct. We may wish to know how likely it is that someone 66\" inches or taller walks in the door! Importantly, we can agree that if someone walks through the door, there is a 100% that they will have a height! That might be silly to say, but since the height of the distribution curve represents the probability of observing that given value, that means that if you sum together the height of the curve at every point, that the sum will add up to exactly 1 (which corresponds to 100%). We can depict this using the concept of area under the curve. We can depict the area under the curve like so: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) polygon(x,y,col = &quot;slateblue1&quot;) The sum of the shaded region of the plot in this case is exactly 1. We can actually calculate this using the auc function in the MESS library. ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(x,y) ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 1 Now, this isnt super interesting. Of course there is a 100% chance that a person has a height. But, now we can start asking more interesting questions. If \\(X\\) is the height of the next person to walk in to the room, we could ask \\(P(X &gt;= 5&#39;8&quot; | height\\) \\(is\\) \\(normally\\) \\(distributed)\\) (or what is the probability someone is 58\" or taller, assuming that height is normally distributed around 58). We can use the same principle as above to make this calculation. We start by shading in the area under the curve corresponding to 58 and taller, like so: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(68,x[x&gt;=68]) index_val &lt;- which(x == 68) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) Intuitively, since the normal distribution is symmetrical, it appears that there is a 50% chance that the height of the next person to walk in will be average (58\") or greater. We can check with the auc function: ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.5 Perfect! Now lets do one moreat the beginning of the example, I asked how likely it is that someone 66\" or taller walks through the door next (or \\(P(X &gt;= 6&#39;6&quot;\\))). We can do that like so: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(78,x[x&gt;=78]) index_val &lt;- which(x == 78) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,4) ## [1] 0.0061 As we can see, we think it is quite unlikely that the next person to walk through the door will be 66\" or taller! In fact, rounding to 4 decimals, our distribution is suggesting to us that there is only a 0.61% chance that the next person to walk in will be 66\" or taller. I would say that matches up pretty well with how often I bump into folks that are that tall! 4.2.1.4 Cannot Prove That A Variable Follows a Distribution So, this is one of the tricky parts of statistics. We cannot prove that a variable \\(X\\) truly follows a given distribution. Generally, we assume that it does until evidence is presented to us that we should not. For example, we have displayed above why it makes sense to assume that height is normally distributed. The way we understand how height is distributed throughout the human population matches up with the structure of the normal distribution quite well. However, we cannot truly prove that height is normally distributed - we can only assume that it is. Now, if 100 people walked in the door and half were less than 50\" tall and half were over 70\", I might be a lot less confident that height follows a normal distribution. In other words, we can assume that a variable follows a given distribution and then we can look at data to reflect on this assumption. This is a central activity in undertaking inferential statistics. 4.2.1.5 Formally Defining the Normal Distribution This has been a rather descriptive introduction to the normal distribution. Given how important it is, lets go over how we define it and what its properties are. Remember, our job as statisticians is to identify the signal over the noise. Distributions provide us an ability to rigorously navigate the ratio between the two. There are two parameters which define a normal curve: the mean value (denoted \\(\\mu\\), the Greek letter mew) and the standard deviation (denoted \\(\\sigma\\), the Greek letter sigma). We can think of the mean as our signal and the standard deviation as a measure of how noisy the data is. The larger the standard deviation, the wider our normal curve is (i.e., the more spread out from the mean value we will expect values to be observed). We can visualize this by plotting multiple normal curves with different standard deviations: y1 &lt;- dnorm(x, mean = 68, sd = 1) y3 &lt;- dnorm(x, mean = 68, sd = 3) y6 &lt;- dnorm(x, mean = 68, sd = 6) y10 &lt;- dnorm(x, mean = 68, sd = 10) plot(x,y1,type=&quot;l&quot;,xlab = &quot;X&quot;, ylab = &quot;Density&quot;) lines(x,y3, col = &quot;blue&quot;) lines(x,y6, col = &quot;green&quot;) lines(x,y10, col = &quot;red&quot;) All of these curves are normal distributions with the same mean. They just have different standard deviations. We can understand that a distribution with a larger standard deviation is anticipated to represent noisier data than that with a smaller standard deviation. This is because, the larger the standard deviation, the more likley it is that we will observe values farther and farther away from the mean value. As we discussed in our sample, when we assume a variable is normally distributed, we understand that most of the values we will observe will fall close to the mean value. More extreme observations are understood the be rarer. We can quantify this more specifically - if we assume a variable is normally distributed, then we may understand that just over 2/3 (or around 68.2%) of all observations are expected to fall within 1 standard deviation of the mean. Further, that 95% of all observations will fall within 2 standard deviations of the mean. This can be depicted like so: In our example from before with height, we had defined a normal distribution with a mean height of 58\" and a standard deviation of 4. By definition, we would anticipate then that there is a 68.2% probability that the next person to walk through the door will be within 4 inches (1 standard deviation) of 58 (or, 54\" through 60) and that there is a 95% probability that the next person will be within 8 inches (2 standard deviations) of 58 (or 50\" through 64\"). We can confirm this by looking at the area under the curve again: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(64,x[x&gt;=64 &amp; x&lt;=72],72) index_low &lt;- which(x == 64) index_high &lt;- which(x == 72) poly_y &lt;- c(0,y[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.68 Here, we have filled in the area within 1 standard deviation of the mean and we calculated that this area under the curve sums to 0.68, corresponding to a 68% probability of observing a value within this range! We can do the same for within 2 standard deviations: ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(60,x[x&gt;=60 &amp; x&lt;=76],76) index_low &lt;- which(x == 60) index_high &lt;- which(x == 76) poly_y &lt;- c(0,y[index_low:index_high],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) ## First we take the integral of our curve using the AUC function in the MESS library ## AUC stands for &quot;Area Under the Curve&quot; AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## We will round the result to two decimal for ease of reading ## Taking integrals of lines is an imperfect art so R doesn&#39;t get exactly 1 round(AUC,2) ## [1] 0.95 Voila! We can see that 95% of observations fall between 2 standard deviations of the mean! You might be thinking that 95% (or 5%) is an important threshold in the statistics youve read or done before - we will get into that more in the next chapter! 4.2.1.6 The Standard Normal Distribution One of the most important distributions is the standard normal distribution, sometimes also called the \\(z\\)-distribution. It is a normal distribution whose mean value is 0 and whose standard deviation is 1. We can plot it like so: x&lt;- seq(-5,5,by=.1) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;) You can actually take any value from any normal distribution and standardize it so that it maps onto the z-distribution! This is really useful because it allows us to standardize many of our statistical tests. Standardization is simply the process of dividing the observed signal by the observed noise. Standardization was REALLY important before computers came along. I have been running the auc() function to calculate the area under the curve, but before computers this was actually an incredibly complicated task! Standardzing a curve means that instead of dealing with an infinite possible number of curves, we can just think about one. Old statistics books had tables you could use to check the area under the curve given a specific value on the \\(z\\)-distribution, because calculating the area under the curve is too challenging by hand. A value that is standardized to the \\(z\\)-distribution is called a \\(z\\)-score. A \\(z\\)-score is calculated by the following formula: \\(z = \\frac{x - \\mu}{\\sigma}\\), where \\(x\\) is a value observed from a normally distributed variable. In this case, the signal is the difference between the observed value \\(x\\) and the mean value \\(\\mu\\) and the noise is the standard deviation \\(\\sigma\\). For example, let us say that someone walked into the room and their height was 70 inches. Well, we know that the mean height \\(\\mu = 68\\) and that the standard deviation \\(\\sigma = 4\\). Therefore, the \\(z\\)-score is \\(\\frac{70 - 68}{4} = \\frac{2}{4} = 0.5\\). While, most of the time standardization is going to happen behind the scenes, it is important to reflect on how standardization doesnt change the expected results - it is just a way of making the data easier to work with. Our process indicates to us that the value of 70 in a normal distribution with mean \\(\\mu = 68\\) and standard deviation \\(\\sigma = 4\\) is equivalent to \\(z = 0.5\\). This would mean that the probability of someone being 70 inches or taller \\(P(X &gt;= 70|\\mu = 68, \\sigma = 4)\\) should have equivalent value to \\(P(z &gt;= 0.5| \\mu = 0, \\sigma = 1)\\). Lets check real quick. First, we can look at \\(P(X &gt;= 70|\\mu = 68, \\sigma = 4)\\) by shading in the region of the normal curve corresponding to heights greater than or equal to 70 inches and then we can calculate the area under the curve: x &lt;- seq(52, 84, by = .01) y &lt;- dnorm(x, mean = 68, sd = 4) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;Height in Inches&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(70,x[x&gt;=70]) index_val &lt;- which(x == 70) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values round(AUC,3) ## [1] 0.308 We get a value of 0.308. We can do the same process to check the value of \\(P(z &gt;= 0.5| \\mu = 0, \\sigma = 1)\\). We will plot the \\(z\\)-distribution and shade in all values greater than or equal to 0.5 and then take the area under the curve: x &lt;- seq(-5, 5, by = .01) y &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(.5,x[x&gt;=.5]) index_val &lt;- which(x == .5) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) AUC &lt;- MESS::auc(poly_x,poly_y) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values round(AUC,3) ## [1] 0.308 As we can see, we get the same exact value! I know that this process of standardizing values may seem a bit odd to do, but it is the foundation of the first inferential technique we will learn about in a few chapters - the \\(t\\)-test. In fact, when we get there, we will actually see that a \\(t\\)-test is actually just a slightly variation of standardizing a normally distributed variable! 4.2.1.7 Approximating a Normal Distribution From Sample Data We have covered a whole lot of information in this chapter! The last thing I want to discuss is how we approximate a normal distribution from available data. This is important because usually we do not know what the actual mean and standard deviation of a variable are. So, lets say I sat at my desk all day and I counted that \\(n\\) people walked through the door. I wrote down the height of every single person who came in, like so \\(\\{x_1,x_2,x_3,\\cdots,x_n\\}\\), where \\(x_i\\) represents the height of the \\(i\\)th person to walk through the door. In order to approximate the theoretical normal distribution we assume to describe height, we must attempt to estimate the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of the height of each person. It can be helpful to use \\(\\Sigma\\) (sigma) notation to make these calculations. \\(\\Sigma\\) is used in mathematics to say, take the sum of or add together. So, if I were to write: \\[\\sum_{i = 1}^{n}{x_i}\\] , what I am saying is, take the sum of every observation \\(x_i\\). The \\(i = 1\\) is saying that you should start with \\(x_1\\) and then keep adding the values together until you get to the value at the top, which is \\(n\\) and corresponds to \\(x_n\\). The mean value of a sampled variable with \\(n\\) observations is the sum of all the observations divided by the number of observations, \\(n\\). Or, this can be written as: \\[\\bar{x} = \\frac{\\sum_{i = 1}^{n}{x_i}}{n}\\] We denote this mean with \\(\\bar{x}\\) instead of \\(\\mu\\) because we have calculated this mean from the sample, not the entire population. We want to make this distinction because there are many applications where we will compare the sample mean \\(\\bar{x}\\) to an assumed population mean \\(\\mu\\). Next, we need to generate an estimation for the standard deviation \\(\\sigma\\). We will use the symbol \\(s\\) to denote the standard deviation when it is calculated from a sample. Our goal in measuring the standard deviation is to attempt to capture how far, on average, each observation \\(x_i\\) is from the mean value \\(\\bar{x}\\). The further observations are from the mean value, the larger the standard deviation is. Ideally, then we would just take the average of the distance from all the observed values from the mean, but, unfortunately, this actually always equals 0. For example, lets say our values of \\(x_i\\) were \\(\\{3,10,19,2,1\\}\\). Here the mean value is 7 (i.e., \\(\\frac{3 + 10 + 19 + 2 + 1}{5} = \\frac{35}{5} = 7\\)). We can take the distance of each observation from the mean \\(\\{3 -7,10-7,19-7,2-7,1-7\\} = \\{-4,3,12,-5,-6\\}\\) but the sum of all these distances actually equals 0: \\((-4) + 3 + 12 + (-5) + (-6) = 0\\). So, we are forced to do an intermediary step and calculate the variance, which we denote as \\(s^2\\). For the variance, we square the difference between each value and the mean before we sum them. By squaring the value first, we always end up with a positive number. But, we have also squared the values we actually wanted, hence why we consider standard deviation \\(s\\) the square root of variance \\(s^2\\). So, to calculate variance we use the following equation: \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}\\] We can then get the standard deviation because \\(s = \\sqrt{s^2}\\). All we need to do is take the square root of the variance and we have the standard deviation. At this point we have calculated a sample mean \\(\\bar{x}\\) and a standard deviation \\(s\\), which we can consider estimations of both the population-level values \\(\\mu\\) and \\(\\sigma\\). We can use our values of \\(\\bar{x}\\) and \\(s\\) to either generate the normal distribution or to standardize the values of our variable onto the \\(z\\)-distribution! In the worksheet for this chapter, you can practice taking a variable, calculating the mean \\(\\bar{x}\\) and the standard deviation \\(s\\), and then applying these values to both print a normal distribution and calculate z-scores! 4.3 Conclusion In this chapter, we have introduced a new language of probability. We have briefly discussed in this chapter how statistics is an art of falsification - we assume that something is true, typically by assuming that an outcome follows a specific theoretical distribution and then we try to assess how probable our sample data is given our assumption. If the data we observe seems improbable under our assumption, then we might need to question the validity of our assumption. "],["scientific-research-questions-and-null-hypothesis-significance-testing-framework.html", "Chapter 5 Scientific Research Questions and Null Hypothesis Significance Testing Framework 5.1 Introduction 5.2 Significance 5.3 An Important Distinction: Scientific Versus Statistical Hypotheses 5.4 A Look At History To Understand NHST 5.5 Falsifying the Null Hypothesis: Say Hello to the P-Value! 5.6 But, There Must Be an Alternative 5.7 Putting It All Together-ish: Null Hypothesis Significance Testing 5.8 In Conclusion", " Chapter 5 Scientific Research Questions and Null Hypothesis Significance Testing Framework 5.1 Introduction In the prior chapter, we discussed how statistics is an art of making probablistic guesses about the nature of phenomena that we observe. At its heart, inferential statistics techniques make an assumption that a signal does not exist and then ask how probable this assumption is once we observe some sample data. Before diving into learning these statistics techniques, it will be important that we discuss the null hypothesis significance testing (NHST) framework. While null hypothesis significance testing is not the only way to approach inferential statistics, it is the dominant framework and it is important that you understand how it operates. The NHST framework is not without criticism, including publicly by myself. A big part of this chapter is to introduce null hypothesis testing to you as a necessity, but also to make clear that you should not be basing the quality nor validity of your quantitative research on \\(p\\)-values alone. To approach this, in this chapter we will discuss some of the historical developments that have lead to NHST - specifically the works of Ronald Fisher versus that of Jerzy Neyman and Egon Pearson. Modern NHST is a sort of an amalgamation of the works of these two teams and, unfortunately, it is often applied in ways that have resulted in studies of poor scientific quality. You will not need to know the history of statistics in order be a successful applied statistician, BUT, understanding the principles of Fishers approach versus Neyman-Pearsons will provide you context into what NHST is and what some of its flaws are. 5.2 Significance Many of you (all of you?) are likely to recognize the importance of the word significance. Significance takes on a sacred quality - many of us have been taught to think of the term significant as indicating that your study has found something truly important worth sharing! Results that do not meet this standard are then considered inconsequential and thrown out, never to be looked at again. I really want to challenge you to not care too much about this word. As we will discuss, a result being significant does not inherently mean that it is meaningful, useful, or insightful. By the same token, a result being not significant does not mean the result is not meaningful, useful, or insightful. Part of the goal in this chapter is to understand what significance means and how to think about it in your own studies. 5.3 An Important Distinction: Scientific Versus Statistical Hypotheses Prior to discussing the null hypothesis framework, I first want to draw the distinction between scientific research questions and hypotheses and statistical hypotheses. While inter-related, we must understand the distinction, else we fail to adequately answer our primary research goals when employing statistical techniques. 5.3.1 Scientific Research Questions &amp; Scientific Hypotheses When we undertake a research study, we generally have a research question we are hoping to answer. The purpose of a scientific study is to answer your research question as best as possible. Research questions are presented in plain language, often at the end of the Introduction section of a manuscript. It is common to read a statement in research paper that reads something like: The primary objective of this research study is to address the following research question: among people who smoke cigarettes, is income level associated with likelihood of quitting cigarette use? Often times, a research question is followed by a hypothesis statement. To continue this example, the next sentence may read: We hypothesize that higher income levels will be associated with an elevated likelihood of reporting quitting cigarette use at 6-month follow-up. Assuming we are undertaking a quantitative study, we then take the data available to us and use statistical methods in an effort to best answer our research question. In this case, we need to identify a set of statistical methods that can be applied to answer our research question. The important thing in a research study is to answer our scientific research question and the results of our statistical tests must be understood as being in service to the mission to answer this question. 5.3.2 Statistical Hypotheses: The Null and the Alternate We choose our statistical methods in order to answer our scientific research question. However, each inferential statistical method has its own set of hypotheses. The hypotheses that correspond to a statistical method always follow the same form, regardless of the scientific research question being asked. Further, making a decision about your statistical hypotheses is not the same as making a decision about your scientific hypotheses. Within the NHST framework, an inferential statistical method has one inherent hypothesis (the null) and a second that can also be specified (the alternate). We will discuss the purpose and origins of these hypotheses later on in the chapter: \\(H_0\\): The null hypothesis, which states that any signal observed within a sample is the result of random chance (i.e., the signal does not exist at the population-level) \\(H_A\\): The alternate hypothesis, which states that the observed signal is not the result of random chance and that the signal does exist at the population-level. Within the NHST framework, this is often presented as the negation of the null hypothesis. Remember, when we take a random sample of a population, there is going to be variance (or noise) within the data. Much of the variation in our sample is simply the result of random chance - it wouldnt be weird to see variations in our sample data from what we may expect. For example, lets say we are running a study where we want to answer the scientific research question: Are people who smoke cigarettes more likely to experience depression than those who do not smoke cigarettes? We hypothesize that, Yes, people who smoke cigarettes are more likely to experience depression. In our study, we have measured CES-D scores, an index measuring severity of depressive symptomology, ranging from 0 - 60. So, we decide to compare CES-D depression scores of people who smoke cigarettes versus people who do not smoke cigarettes in an effort to address our scientific research question. A \\(t\\)-test is a statistical tool that can be used to compare the mean value (in this case, average CES-D score) of two groups  so that seems like a good place to start. The null hypothesis of a \\(t\\)-test is that the two groups have the same mean score. So if \\(\\mu_S\\) is the average score of smokers and \\(\\mu_{NS}\\) is the average score of non-smokers, then we can understand that our null hypothesis is: \\[H_0: \\mu_S = \\mu_{NS}\\] Lets immediately note that this null hypothesis doesnt correspond with our scientific hypothesis at all. In fact, we hypothesized that people who smoke would be more likely to experience depression but this hypothesis actually assumes that smokers and non-smokers have the same CES-D score. We can immediately see that we are going to have to translate the results of our statistical test in order to try to answer our scientific research question. Now, lets say we recruit 100 smokers and 100 non-smokers and we find that the average score of smokers is 14.2 and the average score of non-smokers is 14.1. Those do not equal each other! Is our null hypothesis wrong? Well, we dont really have enough information. You see, there are millions of people who smoke and even more who dont smokeif we assume that for these populations that the null hypothesis is true, then finding a small difference in CES-D scores among a relatively small sample is not strange at all! This is what we mean by observed by random chance - if we actually have a true random sample, then there is going to be some natural variation in our sample. Now, what if we recruited a different set of 100 smokers and 100 non-smokers and we found that smokers had a CES-D score of 22 and non-smokers had a CES-D score of 12. Well, as we have noted, it is not weird that values are different. That doesnt inherently mean that our null hypothesis is false BUT it is much stronger evidence that the null hypothesis may be false than the prior sample. Basically, each statistical method is trying to assess how improbable the null hypothesis is based on the data we have observed. If the difference in average CES-D scores is only 0.1, then the null hypothesis seems probable. If the difference is 5 or 10 or 20, then the null hypothesis seems less and less probable. The methods we use in null hypothesis testing are based around falsifying the null - we assume that it is true, observe data, and then ask how probable our data is assuming that the null is true. So, we could run our \\(t\\)-test and decide that we think that the null hypothesis is improbable, or that our result was significant. This indicates to us that our alternate hypothesis has merit: \\[H_A: \\mu_S \\neq \\mu_{NS}\\] Does this answer our scientific research question? We have shown that it is highly probable that the mean CES-D score of smokers does not equal the mean score of non-smokers. But, our research question was about likelihood of experiencing depression. Have we fully captured this idea? Or have we only captured one aspect? Can depression be so easily reduced to a single scale? Can it so readily be captured with one cross-sectional measurement? We might want more information before we answer our research question. 5.3.3 Answering Our Scientific Research Question with Our Statistical Results Often, when we write a quantitative research report, we string together multiple statistical tests, each with their own set of statistical hypotheses. We take the results of this set of statistical tests and we, as the authors, make an argument about how they answer our scientific research questions and reflect on the plausability of our initial scientific hypotheses. This was a rather long-winded way of saying that if you run a statistical test and find a significant result, your job as a researcher is not over. You must always seek to answer your primary research question and significance alone is not enough to do so. 5.4 A Look At History To Understand NHST Okay! Now that that is out of the way, we need to actually discuss the purpose of the null and alternate hypotheses and how we go about evaluating them. NHST is actually a weird mixture of the methods for statistical inference developed by Ronald Fisher and that developed by Jerzy Neyman and Egon Pearson. In fact, their approaches are not compatible and which approach is better has been a topic of debate for nearly a century now! Unfortunately, hypothesis testing is often taught as a series of steps and little time is spent reflecting on what NHST is and what its flaws are. In the following sections, we will discuss the system of statistical inference developed by Fisher and that by Neyman-Pearson. This will help us understand what NHST is and how to use it effectively. Unfortunately, as a social science researcher, you will often be expected to approach science in the normal and customary ways. For decades, NHST has been misapplied to the point that the normal use of NHST is quite often not scientifically rigorous. This has been argued to have resulted in the replication crisis in the social sciences. So, first we will discuss the system developed by Fisher, which focuses solely on falsification of the null hypothesis. Then we will discuss the system developed by Neyman-Pearson, which focuses on choosing between a main and alternate hypothesis. Finally, we will discuss how NHST has attempted to marry these two approaches and how this has resulted in a mishmash of ideas that are often misunderstood and misapplied. In a sense, we must know how to navigate NHST so that we can engage with and author quantitative research, but, I want to encourage you to be critical of these practices. 5.5 Falsifying the Null Hypothesis: Say Hello to the P-Value! Ronald Fisher (1890 - 1962) was a statistician who formally developed the modern idea of falsifying the null hypothesis. While his biography is not needed to understand the concepts, I do want to point out that Fisher (along with other statistical forefathers like Francis Galton and Karl Pearson) was a eugenecist who felt that statistics provided a powerful set of tools for studying racial differences. Fisher and other statisticians were faced with a challenging philosophical question: how do you prove that something is true? Well, it turned out that proving something is true, especially about human behavior, is basically impossible. There are too many people and too many sources of variability to ever really establish that some fact is true about large populations of people (nor is it clear that all people are ruled by some unifying set of constructs that explain humanness). Fishers approach was an elegant response to this conundrum. Instead of trying to prove something is true or false, we can actually assume that something is true and then make observations about the world through this assumption. Does what we are observing make sense with our assumption - or, how likely are our observations given our assumption? This is where the idea of the null hypothesis was born. If we want to try to show that some signal exists, Fishers method suggests that we assume that (\\(H_0\\)) a signal does not exist. Then we observe data and measure that signal within the data to get a test statistic \\(x\\). Finally, we calculate how probable it is that we observed a test statistic whose magnitude was at least \\(x\\) assuming the null hypothesis is true. In other words, we calculate the conditional probability: \\[P(signal \\geq x|H_0)\\] In other words, what is the probability of observing the signal (or a stronger signal) within our sample data, assuming that the null hypothesis is true that no signal exists. Since the null hypothesis is that no signal exists, we can see that \\(P(signal\\geq x|H_0)\\) is a way of assessing how likely the signal within our data is given this assumption (or, how likely the signal we observed is simply the result of random noise). If this value approaches 0, we are suggesting that there is a very low probability we could observe this signal in the sample if the null hypothesis were true. The closer to 0 this value becomes, the more confident we can feel that our assumption of \\(H_0\\) is wrong. In otherwords, the closer to 0 this value becomes, the more confident we can feel that a signal actually exists at the population level. We call this value the p-value. The p-value represents the probability of observing a signal of a given strength or stronger, assuming that (\\(H_0\\)) no signal actually exists. If the p-value is quite small, then that indicates to us that our assumption is probably wrong and that it is quite likely that a signal exists. Notice, this doesnt prove that \\(H_0\\) is false, it is just a way of reflecting that it is highly improbable. The term significant is then applied to p-values with values less than some pre-established threshold \\(\\alpha\\). \\(\\alpha\\) is typically set to 0.05 in most research practices. Fisher importantly argued that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon. In other words, study with significant findings should be viewed as sufficient alone to determine what is factual. 5.5.1 A P-Value is Not Cyril Pernet put together a nice list of things that a p-value is not: A p-value is not a measure of the strength or magnitude of a signal (i.e., a small p-value does not mean the signal is strong or meaningful) A p-value does not reflect the probability of replicating the signal observed (i.e., getting a small p-value doesnt mean replication studies are likely to get the same result) A small p-value is not evidence of any alternate hypothesis, it is only a reflection of the relationship between the data and the null The p-value is not the probability that the null hypothesis is true 5.5.2 So, How Do We Calculate the P-Value? The crux of this approach is that we need a formula by which we can actually calculate the p-value. The method by which we calculate a p-value is by comparing a test statistic to a corresponding theoretical distribution (which we discussed in the previous chapter!). Generally, a test statistic is a single number which is intended to capture the strength of the signal within the sample data. This test statistic can then be compared to its corresponding distribution in order to calculate the p-value. The theoretical distribution of a test statistic is dependent on how the test statistic is calculated, including the distributions of the data used to calculate it. Throughout this text, when we learn a new method, we will reflect on what the test statistic is and what the corresponding distribution is. Regardless, though, calculating the p-value always follows the same general steps. Let us use an example to show the logic of how p-values are calculated. 5.5.3 A P-Value Primer: Big Babies So, we are researching the birth weight of newborn babies in a small town. We had heard some rumors that babies born in this town are all really big! Doctors joke there must be something in the water. So, we want to know, are the babies born in this town actually bigger than normal babies. We happen to know that the weight of newborn babies is normally distributed, that the average weight of a newborn baby is \\(\\mu = 7.5\\) pounds, and that the standard deviation of birth weights is \\(\\sigma = 1.2\\) pounds. We start by setting our null hypothesis. Since the null is the assumption that no signal exists, the null would be that birth weights of babies in this small town are the same as babies generally. If we let \\(\\mu_{ST}\\) be the weight of newborn babies in this small town, then our null hypothesis would be: \\[H_0: \\mu_{ST} = \\mu\\] This is actually the same, mathematically, as saying that: \\[H_0: \\mu_{ST} - \\mu = 0\\] This is quite useful because the difference between the average baby weight in the small town and the average baby weight generally represents our signal. So, lets say we got birth records from a sample of babies in this small town and we calculated that the average birth weight of the sample of babies is \\(\\bar{x}\\). If we assume that \\(H_0\\) is true, then it makes sense that the value of \\(\\bar{x} - \\mu\\) would be normally distributed around the value 0. This is because if we take a random sample of babies from this small town and calculate their average weight \\(\\bar{x}\\) it is quite probable that \\(\\bar{x} \\neq \\mu\\) because there is always going to be random noise in the data, even if we are assuming that \\(\\mu_{ST} = \\mu\\). However, if \\(H_0\\) is actually true, then we can understand that values of \\(\\bar{x} - \\mu\\) close to 0 are more probable than values further from 0. Further, it seems likely that if \\(H_0\\) were true, that \\(\\bar{x}\\) has the same probability of being less than \\(\\mu\\) as being greater than \\(\\mu\\). In other words, it appears that if \\(H_0\\) is true, that \\(\\bar{x} - \\mu\\) is normally distributed around 0! In the prior chapter, we discussed the \\(z\\)-distribution (or the standard normal distribution). Further, we talked about how any normal distribution can be standardized by dividing the values by the standard deviation. We can actually transform our value \\(\\bar{x} - \\mu\\) so that it corresponds to the \\(z\\)-distribution. We use the following equation to do so: \\[z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] So, now we have this value \\(z\\). This is our test statistic. \\(z\\) has several cool properties: 1) it captures the strength of the signal in the dataset and 2) it corresponds to a \\(z\\)-distribution which captures the behavior of the signal assuming that \\(H_0\\) were true. Now, all we need to do is calculate \\(P(signal &gt;=z | H_0)\\). So, lets throw some numbers in. We got the weight of \\(n = 16\\) babies and found that their average weight \\(\\bar{x} = 8.1\\) pounds. So we can calculate \\(z\\) as follows: \\[z = \\frac{8.1 - 7.5}{\\frac{1.2}{\\sqrt{16}}} = \\frac{0.6}{0.3} = 2\\] So, now we can map the value \\(z = 2\\) onto the \\(z\\)-distribution like so: x&lt;- seq(-5,5,by=.1) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;) lines(c(2,2),c(0,1), col = &quot;red&quot;) We are almost there! To calculate the p-value, we need to calculate the probability that our test-statistic has a magnitude of 2 or greater. There are two approaches when using a two-tailed distribution. A two-tailed test or a one-tailed test. In a two-tailed test, we simply care about any observation more extreme than our calculated value \\(z\\). Since \\(z = 2\\), this means any value greater than or equal to 2 or less than or equal to -2. In a normally distributed variable centerred around 0, 2 and -2 have the same probability of occurring. So, for the two tail test, we fill in the tails starting at -2 and 2: x &lt;- seq(-5, 5, by = .01) y &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(2,x[x&gt;=2]) index_val &lt;- which(x == 2) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x&lt;=-2],-2) neg_index &lt;- which(x == -2) neg_y &lt;- c(y[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) Our p-value is the area of the filled in sections of the curve, which we learned how to calculate last chapter, by using the auc function like so: round(MESS::auc(poly_x,poly_y) + MESS::auc(neg_x,neg_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.045 We see that we got a p-value of 0.045. If we were using \\(\\alpha = 0.05\\) then we can see that \\(0.045 &lt; \\alpha\\) and we would describe this finding as significanct. In other words, we feel that it is quite improbable that we would observe babies of this average weight if there was no difference between babies in this small town and babies generally. However, we actually were more interested in whether or not babies in this small town were bigger than babies generally. Thus, values of \\(z\\) less than 0 are not of interest because they correspond to values of \\(\\bar{x}\\) where the babies in the small town have a lower birthweight. So, we can run a one-tailed test where we only look at the positive end of the distribution. If we plot that it looks like so: x &lt;- seq(-5, 5, by = .01) y &lt;- dnorm(x, mean = 0, sd = 1) ## We will now plot the normally distributed data as a line (type = &quot;l&quot;) plot(x,y, type=&quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;Density&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(2,x[x&gt;=2]) index_val &lt;- which(x == 2) poly_y &lt;- c(0,y[index_val:length(y)]) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) And the p-value can be calculated by the same method, which, since the normal distribution is symmetrical, we would anticipate is half the value of our two-tailed test: round(MESS::auc(poly_x,poly_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.023 And voila! We have done it! We have calculated our p-value. We would read this as saying that, assuming that our null hypothesis was true, there was a 2.3% chance we would have observed a mean baby weight of these 16 babies in this small town of 8.1 pounds or greater. If using an \\(\\alpha\\)-threshold of 0.05, we would describe this finding as significant. Now, our overall research question was about whether or not babies in this small town are, on average, larger than normal babies. Did we effectively answer that with this test? I would say we have found an important part of answering this question, but that our work is not complete. Yes, this provides evidence that it is probable that babies in this town dont have the same average birth weight as babies generally - this is evidence of the claims that babies born in this town are larger than average. If this were a real study though, we would need to also argue that our finding is not influenced by sampling or measurement bias! I raise this point just to indicate that a significant finding alone doesnt answer our research question, it just provides a piece of information we can use to answer our research question. 5.5.4 General Steps to Calculating P-Value So, the general steps to calculating a p-value are as follows: We assume \\(H_0\\) that no signal exists in the population The signal in the sample data is measured and identified This signal is converted into a test statistic which corresponds to a specific theoretical distribution The test statistic is compared to the distribution (area under the curve) in order to calculate the p-value Any time a p-value is calculated, these are the steps that are undertaken! Many of the statistical methods we employ were designed specifically because they allow this process to occurif you are ever learning about a method and you think why on earth are we doing this? the answer is likely that this is how we can convert our sample signal into a test statistic that can be compared to a theoretical distribution. 5.6 But, There Must Be an Alternative Fishers approach focuses on falsifying the null hypothesis via significance testing. The null hypothesis is assumed to be true - the goal is not to try to argue that the null hypothesis is actually true or actually false through Fishers approach. It is the researchers job to reflect on what the findings mean, how to interpret the results in relation to the over-arching scientific research question. Fishers approach takes a (rightfully) cautious approach to scientific discovery - we cant really know what is true and what is false, but we can get a sense for what is probable! In this way, scientific knowledge can be built up over time by replicating studies and by comparing results across settings and samples. It is then the job of researchers to reflect on these results to make conclusions about the nature of various phenomena. However, there are many cases where simply reflecting on the probability of the data observed falls short of intended purposes. This is especially true in industry and corporate settings - perhaps a company wants to figure out if one manufacturing approach is better than another or if their new measurement tool is accurately calibrated. In such circumstances, simply ruminating on and reflecting upon the results of a series of tests is not practical. There are certain cases where a decision has to be made based on the information at hand! Enter two of Fishers contemporaries, Jerzy Neyman and Egon Pearson. Neyman and Pearson developed a competing system for statistical inference, which was informed by involvement in manufacturing (i.e., they were interested in using statistics to figure out if a batch of a product on a production line was abnormal). Instead of simply reflecting on the probability of the data, assuming a null hypothesis, the Neyman-Pearson approach involves deciding between the main hypothesis (that no detectable signal exists) and an alternate hypothesis (that a detectable signal exists). Instead of significance testing, we refer to this process as hypothesis testing. In the Neyman-Pearson approach, two hypotheses are specified: \\(H_M\\): The main hypothesis, that no detectable signal exists \\(H_A\\): The alternate hypothesis, that a detectable signal exists The goal of the Neyman-Pearson approach is to pick one of these two hypotheses. To make this selection, they use the same significance criterion as Fisher. The \\(p\\)-value is calculated and if the \\(p\\)-value is less than \\(\\alpha\\), then the main hypothesis is rejected and the alternative hypothesis is accepted. If the \\(p\\)-value is greater than \\(\\alpha\\) then the main hypothesis is accepted and the alternative hypothesis is rejected. This is different from Fishers approach - while Fishers approach advocates for identifying significant results, Fisher was also a proponent that a conclusion about the null hypothesis cannot be made based on the results of a single study. Choosing between the main and alternate hypothesis introduces a new concern - is the determination correct? If you reject the null, should you have? If you accept the null, should you have? While Fisher (rightfully) treated the p-value as a probability, Neyman-Pearson treat it as a decision rule. Since it is a probability, however, that means that if a decision is made there is a chance that the decision is wrong. The Neyman-Pearson approach has a strong appeal to any person - Fishers approach requires cautious (and often ambiguous) interpretation, whereas the Neyman-Pearson approach has the finality of making a clear decision. Unfortunately, applying this logic to individual research studies has resulted in overly-conclusive scientific reporting across the social sciences. Choosing between the main and the alternate hypothesis also introduces an important concept: effect sizes. If the null hypothesis states that no signal exists and the alternate states that one exists, then the effect size represents the magnitude and the direction (positive or negative) of the signal. However, as we have discussed, in the Fisherian approach, we generally assume that there will be a signal in our sample data - we are just trying to figure out if we think that the observed signal is consistent with the null hypothesis. That means that relatively small observed effect sizes are likely to support the acceptance of the null even if that small effect size is the true value (i.e., the null is false) - not all deviations from the null hypothesis are detectable. 5.6.1 Visualizing the Null and the Alternate, an example So, to better understand why this notion of detectability is important, let us take our babies example and visualize it. Remember, we had compared the mean weight of babies in this small town (rumored to produce giant babies) versus the mean weight of average babies. We measured the signal by subtracting the mean weight of small town babies by the mean weight of average babies and then we standardized the value to a \\(z\\) value which is assumed (under the null hypothesis) to be described by the standard normal distribution (\\(z\\)-distribution). We depict the standard normal distribution below: x&lt;- seq(-5,5,by=.1) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) In the Fisherian approach, we used \\(\\alpha = 0.05\\) as a threshold for determining significance. In the Neyman-Pearson approach, \\(\\alpha\\) is used to define the critical region of a hypothesis test. As we recall from our definition of the normal distribution, 95% of observations are expected to occur within 2 standard deviations of the mean OR that only 5% of observations are expected to occur outside of 2 standard deviations of the mean. For a normally distributed variable (such as \\(z\\)), the critical regions defined by \\(\\alpha = 0.05\\) correspond to the two tails of the distribution outside of two standard deviations of the mean. Technically, for a \\(z\\)-distribution, 95% of expected observations fall between -1.96 and 1.96 (not -2 and 2, as seems intuitive), which we can visualize like so: x&lt;- seq(-5,5,by=.01) y&lt;- dnorm(x,mean = 0, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab=&quot;z&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) In the Neyman-Pearson approach, any calculated value of \\(z\\) that falls in the critical regions leads us to reject the null hypothesis and accept the alternate hypothesis. If \\(z\\) does not fall in the critical regions, then Neyman-Pearson indicates that you should accept the main/null hypothesis. What this means that, for a \\(z\\)-test, any value of \\(z\\) which falls between -1.96 and 1.96 is taken as in support of the main hypothesis. As such, the Neyman-Pearson null hypothesis is actually a little bit different than the Fisher null hypothesis. The Fisher null hypothesis would be that \\(H_0: z = 0\\) and we calculate a \\(p\\)-value to determine how likely our data is under that assumption. Whereas, the Neyman-Pearson null hypothesis actually corresponds to \\(H_0: z \\in 0 \\pm 1.96\\). The \\(\\in\\) symbol is just read as saying is in, so we can read that as \\(z\\) is within the range of 0 plus or minus 1.96. In the case of a \\(z\\)-test, 1.96 represents the minimum detectable effect size. For all values of \\(z\\) within this range (-1.96, 1.96) we will accept the main hypothesis. For all values of \\(z\\) outside of this range, we will reject the main hypothesis. We are using the same \\(p\\)-value criterion as Fisher to make our decision, but now instead of reflecting on the probability of the data, we are making a binary decision. What this then means is that the alternate hypothesis under Neyman-Pearson is that \\(H_A: z \\notin 0 \\pm 1.96\\). If we reject the main hypothesis and accept the alternative, what we are saying is that we think that our signal \\(z\\) is actually described by a different distribution, specifically, a normal distribution whose mean is not centered around 0. There are actually an infinite number of potential alternative hypothesis distributions that could be observed (i.e., because there are an infinite number of continuous values which could be the mean). Here, we visualize some of the possible alternative distributions. x&lt;- seq(-5,5,by=.01) y&lt;- dnorm(x,mean = 0, sd = 1) y_neg1&lt;- dnorm(x,mean = -1, sd = 1) y_neg2&lt;- dnorm(x,mean = -2, sd = 1) y1&lt;- dnorm(x,mean = 1, sd = 1) y2&lt;- dnorm(x,mean = 2, sd = 1) y3&lt;- dnorm(x,mean = 3, sd = 1) plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) #lines(x, y2, col = &quot;green&quot;) lines(x, y3, col = &quot;slateblue1&quot;) This might be a bit overwhelming to look at, but thats because there is literally an infinite number of potential alternative distributions. Lets look at the green distribution only real quick: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) The green distribution represents an alternative to the null distribution. The green distribution indicates that the true standardized difference between the small town babies and the average baby is \\(z = 1\\) (or \\(H_A: z \\in 1 \\pm 1.96\\). Just like with the standard distribution, we assume that if we calculated \\(z\\) for a sample, that the probable values of \\(z\\) would be normally distributed around 1. Here is the thing though, the probable values of this alternative overlap with a lot of the probable values of our null distribution. If the true alternate is an effect size of \\(z = 1\\), then we simply dont have a lot of power to actually identify it. Power refers to our ability to reject the null/main hypothesis when it should be rejected. In the case above, we will only reject the null hypothesis if \\(z\\) falls outside the -1.96 through 1.96 range. Which means that if the true signal is \\(z = 1\\), then we have little power to identify it. We can actually calculate the probability of correctly rejecting the null hypothesis when the alternate hypothesis is \\(H_A: z \\in 1 \\pm 1.96\\) by taking the area of the alternate curve within the critical regions, like so: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y1[index_val:length(y1)]) ## we need the negative values too #neg_x &lt;- c(x[x &lt;= -1.96],-1.96) #neg_index &lt;- which(x == -1.96) #neg_y &lt;- c(y1[1:neg_index],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;green&quot;) #polygon(neg_x,neg_y,col = &quot;green&quot;) round(MESS::auc(poly_x,poly_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.168 What this tells us is that, if the real effect is \\(z = 1\\), that we will only correctly reject the null 16.8% of the time - or that our power = 0.168. That is really not good at all! It can be useful to plot this a different way, like so: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## beta x beta_x &lt;- c(-1.96, x[x &gt;= -1.96 &amp; x &lt;= 1.96], 1.96) beta_y &lt;- c(0,y1[neg_index:index_val],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) polygon(beta_x,beta_y,col = &quot;green&quot;) This shaded green region represents the values of \\(z\\), assuming this is real distribution of the signal, represents the probability that we wrongly accept the null/main hypothesis. We refer to this region as \\(\\beta\\). Since the area under a curve equals 1, we can see that \\(\\beta = 1 - power\\). In this case, \\(\\beta = 1 - .168 = .832\\). Now, since there are infinitely many possible alternative hypotheses, Neyman and Pearson needed a way to establish what a detectable alternative hypothesis is. One way is to make \\(\\alpha\\) less restrictive. So, instead of \\(\\alpha = 0.05\\), we could set \\(\\alpha = 0.1\\). This would result in the following adjustment to our previous plot: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.65,x[x&gt;=1.65]) index_val &lt;- which(x == 1) + 65 poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.65],-1.65) neg_index &lt;- which(x == -1.65) neg_y &lt;- c(y[1:neg_index],0) ## beta x beta_x &lt;- c(-1.65, x[x &gt;= -1.65 &amp; x &lt;= 1.65], 1.65) beta_y &lt;- c(0,y1[neg_index:(index_val-1)],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) polygon(beta_x,beta_y,col = &quot;green&quot;) As we can see, adjusting \\(\\alpha\\) caused the region of \\(\\beta\\) to change as well. That is because \\(\\alpha\\) and \\(\\beta\\) are intertwined - if you increase \\(\\alpha\\), then \\(\\beta\\) will decrease and vice versa. BUT, this approach is not ideal. If you increase \\(\\alpha\\) then you are essentially making it more probable you will reject your null hypothesis. We set \\(\\alpha\\) to a low value of 0.05 so that we can be confident in our rejection. The higher \\(\\alpha\\) becomes, the riskier our rejections of the null hypothesis become (i.e., our chances of wrongly rejecting the null are now higher). So, Neyman-Pearson came up with a different idea. The issue is that their approach is not ideal for identifying relatively small signals. But, if we know the size of the signal we are looking for, then we can design our study to have enough power to identify that signal. So, Neyman-Pearson suggest defining an explicit alternate hypothesis in which the power to detect it is at least 80%, which is the equivalent to \\(\\beta = 0.2\\). In the case, of our example, this would mean shifting the alternate hypothesis further to the right so that the shaded green region has an area of 0.2. This would correspond with a \\(z\\)-value of approximately 2.2, which we can depict like so: plot(x,y,type=&quot;l&quot;,ylab=&quot;Density&quot;, xlab = &quot;z&quot;) #lines(x, y_neg2, col = &quot;red&quot;) #lines(x, y_neg1, col = &quot;red&quot;) y1 &lt;- dnorm(x, mean = 2.8, sd = 1) lines(x, y1, col = &quot;green&quot;) ## we want all values of x and y where x is greater than 68 ## The following three lines of code do this poly_x &lt;- c(1.96,x[x&gt;=1.96]) index_val &lt;- which(x == 1.96) poly_y &lt;- c(0,y[index_val:length(y)]) ## we need the negative values too neg_x &lt;- c(x[x &lt;= -1.96],-1.96) neg_index &lt;- which(x == -1.96) neg_y &lt;- c(y[1:neg_index],0) ## beta x beta_x &lt;- c(-1.96, x[x &gt;= -1.96 &amp; x &lt;= 1.96], 1.96) beta_y &lt;- c(0,y1[neg_index:index_val],0) ## Then we plot it polygon(poly_x,poly_y,col = &quot;slateblue1&quot;) polygon(neg_x,neg_y,col = &quot;slateblue1&quot;) polygon(beta_x,beta_y,col = &quot;green&quot;) round(MESS::auc(beta_x, beta_y),3) ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique ## &#39;x&#39; values ## [1] 0.2 So, in this case Neyman and Pearson would describe \\(z = 2.2\\) as the expected effect size. The logic is that if the true effect size (i.e., the alternate hypothesis) is that \\(z = 2.2\\), using their approach, they will correctly reject the null/main and accept the alternate hypothesis 80% of the time! If the true value of \\(z\\) is greater than 2.2, then this power will increase. What this means is, if the true signal you are trying to observe is closer to 0 than 2.2, you will have limited power to reject the null. For practical purposes, this would indicate that there isnt strong justification to use the test because the goal is to correctly pick the null/main or the alternate. If you cant successfully pick the alternate, then there is no point. We discussed before that you can improve power by increasing \\(\\alpha\\). This is not ideal because it makes our rejections less conservative. We only want to reject the null hypothesis if we feel really confident we should. The other way to improve power is to sample more people for the study. We can see that if we look at the formula for calculating \\(z\\) that it is built right in - the greater our sample size \\(n\\), the greater the magnitude of \\(z\\), and thus the greater power we have: \\[z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] \\(n\\) represents the number of people in our study. As \\(n\\) gets bigger, so does \\(z\\)! This may be confusing because we have a fraction nested inside a fraction. So, if \\(n\\) is bigger than the value of \\(\\frac{\\sigma}{\\sqrt{n}}\\) will get smaller. The bigger a denominator, the smaller the value becomes. So if the denominator of the whole fraction gets smaller, than the magnitude of \\(z\\) will go up! As we sample more people, the value of \\(z\\) will shift further and further away from 0! That means that \\(\\beta\\) will decrease and, by definition, the power of our study will increase. Power analyses are used to determine what size of \\(n\\) needs to be recruited so that we can detect a given expected effect size with a power of at least 0.8. The power analysis for a given method is dependent on how the test statistic is calculated. In the case of \\(z\\), we want to know how many people we need so that \\(z = 2.2\\), which we established above is associated with a power of 0.8 for a \\(z\\)-test. If we expect that babies in the small town have a mean weight \\(\\bar{x} = 8.1\\) pounds (our expected effect size is that babies in this small town are 0.6 pounds heavier at birth then the average baby, i.e., 8.1 - 7.5 = 0.6), recalling that the standard deviation of birth weight was \\(\\sigma = 1.2\\) and we want to calculate \\(n\\) for \\(z = 2.2\\), then we can do the following: \\[z = 2.2 = \\frac{8.1 - 7.5}{\\frac{1.2}{\\sqrt{n}}}\\] We can scramble the values around to find that: \\[\\sqrt{n} = \\frac{1.2*2.2}{8.1 - 7.5} = \\frac{2.64}{0.6} = 4.4\\] Since this is the square root, we square 4.4 to get \\(4.4^2 = 19.36\\). We always round up in power analyses because we cannot recruit 19.36 persons - this tells us if we expect that babies in this small town have a birth weight of 8.1 pounds, we need to recruit at least 20 babies to have adequate power to identify that this is true! 5.6.2 The Neyman-Pearson Decision Matrix What is immediately clear is that Neyman-Pearsons emphasis on making a decision has introduced a different type of complication to our idea of statistical inference. Fisher was not concerned with deciding if the null hypothesis was true or false - these statistical tests provide information on the probability of observations and it is the researchers job to contextualize the findings within the broader discourse of the research question. There is not a risk of being wrong in Fishers approach. In Neyman-Pearson, you must decide between the null and an alternative hypothesis. Even though we use expected effect size to make sure our study has enough power, the alternate hypothesis in Neyman-Pearson is typically just the opposite of the null. The Neyman-Pearson decision process is often depicted as a decision matrix, like so: Ideally, we correctly choose between the main/null and the alternative hypotheses. This is always the goal of this approach. Two types of errors can occur: a Type 1 Error, in which the null hypothesis is rejected when it should have been accepted; and a Type 2 Error, in which the null hypothesis is accepted when it should have been rejected. 5.6.2.1 Type 1 Error, Alpha, and Long-Term Success We can understand that the probability of making a Type 1 Error in the long run when the null is true is \\(\\alpha\\). As displayed above, if our \\(p\\)-value is less than 0.05, we reject the null - even in cases when the null should not be rejected. Our \\(p\\)-value, as defined by Fisher, is the probability of observing our data assuming the null is true. Our critical regions which are defined by \\(\\alpha\\) represent observed signal which, in total, have a 5% chance of occurring assuming the null hypothesis is true. That means, if we run 100 experiments with a true null hypothesis, we would expect to make a Type 1 Error 5 out of 100 (5%) times. This concept is often misunderstood within the social sciences. Neyman and Pearson were concerned with identifying abnormalities in production. If a factory makes thousands of products, then their tests can be run over and over and over again. In other words, Neyman and Pearson were working in an environment where there was constant experimental replication. They were concerned with success in the long run. They wanted to make sure if they ran their test 1,000 times, that they maximized the number of times that they were correct. However, in the endeavor to build scientific knowledge, we may be lucky to have the opportunity to replicate a study even one time, especially outside the context of randomized control trials. And, often, the replications undertaken are not true replications - they may have different sampling and measurement strategies that make them different. As a result, sometimes researchers use Neyman-Pearsons idea of a \\(p\\)-value and \\(\\alpha\\) to measure how likely it is that the null hypothesis is falsethis unfortunately is not an appropriate way to interpret the \\(p\\)-value. When we explicitly accept the null hypothesis under Neyman-Pearson, we are thinking about our long-term success running and re-running (and re-running and re-running) the same experiment. The misuse of the Neyman-Pearson decision matrix is often a result of researchers proclaiming that a single study can determine if the null or the alternate hypothesis is correct. 5.6.2.2 Type 2 Error, Beta, and Long-Term Success Likewise, the probability of making a Type 2 Error in the long run when the null is false is \\(\\beta\\). If our \\(p\\)-value is greater than \\(\\alpha\\), then we accept the null - even if we should reject the null. While the true value of \\(\\beta\\) is dependent on the true (and unknown) signal, we calculate \\(\\beta\\) such that we can at least identify an expected effect size correctly 80% of the time or more. The idea here is that, if the null hypothesis is false and the alternative hypothesis is true and the true signal is at least the magnitude of the expected effect size, then if we run 100 experiments, we should correctly reject the null and accept the alternative 80% of the time (or more). 5.6.2.3 Thinking Long-Term Under Neyman-Pearsons framework, it appears that one of the best ways to identify if the null or the alternate hypothesis is true, is to replicate our study design many, many times. If the null is true, we would expect to reject the null only 5% of the time. If the null is false, we would expect to reject the null at least 80% of the time. If you only run one iteration of an experiment, you simply do not have enough information to identify if the null is true or not. Imagine flipping a coin one time, getting a heads, and declaring there is a 100% chance of flipping heads - that would be a poor conclusion! Unfortunately, many researchers will use the Neyman-Pearson decision approach for one study and then use that to declare that the null hypothesis is false. Repeating the study design and observing the decision multiple times is the best way to truly capture which hypothesis should be accepted. 5.7 Putting It All Together-ish: Null Hypothesis Significance Testing Today, null hypothesis significance testing (NHST) is the primary framework used for statistical inference. NHST is often described as a mixture of the Fisher and Neyman-Pearson approaches, though it is not formally defined. To add to the potential confusion, for many statistical methods, applying Fisher and Neyman-Pearson lead to identical presentation of results. It is hard to explicitly define NHST because it isnt a well-defined framework. It has arisen over several generations of statistical practice, as researchers have applied (and mis-applied) the approaches of Fisher and Neyman-Pearson. Generally, on the surface, NHST looks very similar to Neyman-Pearson. A null and alternate hypothesis is specified. A statistical test is run. If the corresponding \\(p\\)-value is less than \\(\\alpha\\) then the researcher declares their results significant and rejects the null hypothesis. Otherwise, if the \\(p\\)-value is greater than \\(\\alpha\\), the result is deemed non-significant and the researcher opts to accept (or fail to reject) the null hypothesis. Often, non-signfiicance is misinterpreted as meaning that the result of a test has no value. As well, given concerns that a single study cannot be used as proof that the null is true, some researchers choose to fail to reject the null instead of accepting it based on non-significant results. However, in practice, most researchers tend to treat acceptance and failure to reject identically when it comes to answering their overall research question (i.e., they still treat the result as having no value in answering their scientific research question). As such, NHST has become a rather mechanical exercise. Data is fed into a statistical test. The result is either significant or not. If the result is significant then the null can be rejected. It is very common for researchers to then answer their over-arching scientific research question by asserting that they found a significant result. For example, lets say a study asked, Is there a relationship between childhood drug use and educational attainment? It would be fairly normal, under NHST practices, to read the following conclusion, We found a significant relationship between childhood drug use and educational attainment, without any reflection about the nature of this relationship. Significance, derived from the \\(p\\)-value, is a reflection of how likely our observed data is assuming the null hypothesis is true. Significance does not reflect on the likelihood of the null hypothesis nor does it reflect on the likelihood nor description of some alternative hypothesis. Therefore, it is not good statistical practice to use significance, alone, as an assertion that the null hypothesis is false or that some alternate hypothesis is true! 5.7.1 So, NHST Isnt a Good Idea? I realize I have introduced NHST in a pretty negative light. We have discussed the Fisher and Neyman-Pearson approaches because they both represent important frameworks for statistical inference. However, they are distinct and should be applied when it is appropriate to do so. The Fisherian approach is well-suited for research that is unlikely to be repeated (as is the case with most human subjects social science research). This is because Fishers approach does not attempt to decide whether the null hypothesis is true or false (as doing so with a single study is not well-founded), but instead focuses on how probable the null is given the observed data. Under Fishers approach, it is the researchers responsibility to take the results of statistical tests and their broad subject expertise to then answer their research question. Neyman-Pearson were more interested in decision-making in contexts where the same statistical test could be executed over and over again, such as in manufacturing (where every product produced may be subject to some test). Their notion of acceptance and rejection of the null hypothesis is based on long-term probabilities. If they run a test 100 times, their goal is maximize the number of times the test is correct. This often implies that there exists a need to decide if something is true or false. In much of scientific inquiry, there is no imperative to decide if some hypothesis is true or not - in fact, it is generally considered unwise to declare that anything is true, as there is always more data that may lead us to rethink our decision. In many applied settings, making a decision is important: should a patient receive treatment A or treatment B; is a batch of our product normal or abnormal? Neyman-Pearsons approach is more appropriate in settings where a decision must be made, whereas Fishers approach is often better suited for open-ended scientific inquiry. I bring this up to say that it is your responsibility as a researcher to decide the best approach. Often, the statistical tests used under Fisher and Neyman-Pearson are the same. The primary difference is in the presentation of the results. Under Fisher, it is your responsibility to reflect on the probability of the data (the \\(p\\)-value) in your effort to answer your scientific research question. There is no impetus to accept or reject the null hypothesis, the \\(p\\)-value and other results of statistical tests simply represent evidence that can be used in litigating your research question. Under Neyman-Pearson, significance is used to make a decision based on the data observed. This is often not congruent with the goals of scientific inquiry. For example, under Neyman-Pearson we would treat a \\(p\\)-value of 0.049 as significant but a \\(p\\)-value of 0.051 as not significant even though we understand that they indicate almost the same exact probability of the sample data assuming the null is true! However, Neyman-Pearson also importantly introduced the concepts of power and effect size, which provides a way to reflect on what the alternative to the null may actually be! 5.7.2 Misuse of NHST As I have alluded to, NHST is often poorly used. This is not intended to say that scientists are doing a bad job - instead, it is more a reflection of what kind of work is rewarded and how that has resulted in the proliferation of poor practices. The misapplication of NHST often is when researchers use NHST through the Neyman-Pearson framework when it is more appropriate to use Fishers. This occurs when researchers use a single study to declare a null hypothesis is false, when Neyman-Pearson fully intended their decision-making strategy to apply to long-run replications of the same test. This approach is far easier for both researchers and reviewers - the word significant becomes a signal that the research is important. As a result, researchers choose statistical tests with the goal of acheiving significance and reviewers and journals reward papers that find significance. Neyman-Pearsons strategy is intuitive and conclusive - unfortunately, social science research rarely warrants conclusive declarations. As a result, NHST is often taught as a hunt for significance. The Neyman-Pearson decision matrix is often taught, but the Fisherian approach is ignored. There is widespread misunderstanding of what a \\(p\\)-value is and, as a result, significant is often used as a word to justify the conclusion that the study has confirmed the authors scientific hypotheses. This is reinforced by the literature, where papers that approach significance testing in this way are rewarded and published. This indicates to other researchers and prospective researchers that this is best way to approach statistical inference. As such, we have discussed both Fisherian and Neyman-Pearson approaches. We have done so so that we are empowered to apply the correct one - often, a mixture of the two is warranted, but we need to be clear about why we are interpreting our results the way that we are. The goal is always to answer our scientific research questions - the results of our statistical tests represent pieces of evidence in the effort to answer our scientific research question. 5.7.3 What Should I Do? Which framework you apply will be dependent on the nature of your research question and your data. For every study, you will need to ask what the most appropriate framework to use is. Often, a mixture of both frameworks is warranted, but it is important to explicitly understand which parts of each are appropriate. If your research is focused on a decision-making process, then Neyman-Pearson should be applied in full. If your research is focused on building scientific knowledge, then the Fisherian approach makes sense. Since this book is intended for human subjects social science researchers (such as epidemiologists), I will say that it is unlikely that Neyman-Pearsons framework should be used alone. I generally recommend applying Fishers approach for interpreting \\(p\\)-values in relation to the null hypothesis, while also incorporating Neyman-Pearsons idea of measuring effect sizes. First, in studies aiming to build upon scientific knowledge, a single study should never be viewed as conclusive evidence of whether something is true or false. Fishers approach is well-suited for navigating the uncertainty of making conclusions, whereas the Neyman-Pearson approach often results in hard conclusions that are not fully supported by the data. Second, while Fisher was not concered with an alternate hypothesis, the alternate hypothesis is usually what is most interesting to us! If we want to know the relationship between two variables, the null hypothesis that there is no relationship is usually not interesting to us at all! So, it is important that we have a way to capture the potential alternative hypothesis. The best way to do this is by capturing the effect size of the relationship between two variables, as Neyman-Pearson do. An effect size is the magnitude and direction of a relationship between two variables within the sample - this effect size can be understood to be a best estimate of the population-level effect size if the null hypothesis is false. Just about every test, regardless of approaching it from the Fisher or Neyman-Pearson framework, measures effect size. Fisher is just concerned with whether or not the observed signal (effect size) is probable under the null hypothesis, whereas Neyman-Pearson were interested in asking if the measured signal is evidence of an alternate hypothesis. As such, Neyman-Pearson also introduced the concept of confidence intervals, which represent a range of probable values that this actual population-level effect may fall within, which usually is centered around the measured effect size. Now, these are my suggestions to you. You will likely encounter scientists who would not agree. You will likely encounter scientists who are not familiar with Fishers approach or Neyman-Pearsons. You may submit a paper and have reviewers confused why you have approached statistical inference from the Fisherian approach when they are accustomed to the Neyman-Pearson. The important thing is that, no matter what, you feel comfortable justifying the decisions you have made. 5.8 In Conclusion The goal of any study is to answer your scientific research question. Two frameworks - the Fisherian and the Neyman-Pearson - have been developed to guide statistical practices. For the social scientist, the Fisherian approach combined with Neyman-Pearson concepts of effect size and confidence intervals represent an important strategy for scientific inquiry. The Neyman-Pearson decision matrix is generally warranted where replication is undertaken and where the primary goal is to inform decision-making. The misapplication of Neyman-Pearsons decision approach has lead to many poor practices within the social sciences. By understanding both Fisher and Neyman-Pearson approaches, we can choose which option is best based on the goals and design of our study! "]]
